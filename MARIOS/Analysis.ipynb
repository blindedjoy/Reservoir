{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Analysis Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider today:\n",
    "\n",
    "It’s publication time. Execute all Marios related tasks.\n",
    "\n",
    "0) get Marios information on the new result.\n",
    "\n",
    "1) Identify the joint prediction paths, automate plot production, upload all these types of plots online\n",
    "\n",
    "2) Print out and read the article\n",
    "\n",
    "3) prepare for meeting with Marios, prepare pure prediction, multiple kinds.\n",
    "\n",
    "4) get ready for 3D loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mfinal_results\u001b[00m\r\n",
      "├── 150_500Hz_cyclic.pickle\r\n",
      "├── 150_500Hz_random.pickle\r\n",
      "├── 500_900Hz_cyclic.pickle\r\n",
      "├── 500_900Hz_random.pickle\r\n",
      "├── block_N_Targidx_40N_Obsidx_26.pickle\r\n",
      "└── combined_analysis.pickle\r\n",
      "\r\n",
      "0 directories, 6 files\r\n"
     ]
    }
   ],
   "source": [
    "! tree final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'PyFiles/imports.py'\n",
    "%run -i 'PyFiles/helpers.py'\n",
    "%run -i \"PyFiles/experiment.py\"\n",
    "%run -i \"PyFiles/analysis.py\"\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "import matplotlib\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import scipy.stats as stats\n",
    "import sys\n",
    "\n",
    "\n",
    "#def blockPrint():\n",
    "#    sys.stdout = open(os.devnull, 'w')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_freq(experiment):\n",
    "    freq_spec_lst = np.array(experiment[\"f\"])[experiment[\"resp_idx\"]]\n",
    "    target_freq_spec = np.round(np.mean(freq_spec_lst),1)\n",
    "    return(target_freq_spec)\n",
    "\n",
    "def IdxMatch(experiment, \n",
    "             resp_idx_range = range(272, 301), \n",
    "             n_obs = 28\n",
    "             ):\n",
    "    \"\"\"\n",
    "    obs3:\n",
    "    obs4: resp_idx_range = range(272, 301) n_obs = 28\n",
    "    obs5:\n",
    "    \"\"\"\n",
    "    #if obs_idx_lst == experiment[\"obs_idx\"]:\n",
    "    #    if resp_idx_lst == experiment[\"resp_idx\"]:\n",
    "    #        return True\n",
    "    resp_idx_list = list(resp_idx_range)\n",
    "    if resp_idx_list == experiment[\"resp_idx\"]:\n",
    "        if n_obs == len(experiment[\"obs_idx\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def freq_plot(analysis_obj, experiment_num = 0, title = \"Observer 4 experiment\",  save = None):\n",
    "    freq_rDF = analysis_obj.rDF_freq\n",
    "    freq_loss_df =  freq_rDF[freq_rDF[\"experiment #\"] == experiment_num]\n",
    "    \n",
    "    plt.figure(figsize = (12,5))\n",
    "    sns.scatterplot(x = \"freq\", y = \"L2_loss\", data = freq_loss_df, hue = \"model\", alpha = 0.3, legend = None)\n",
    "    sns.lineplot(x = \"freq\", y = \"L2_loss\", data = freq_loss_df, hue = \"model\", alpha = 0.9)\n",
    "    plt.title(title, fontsize = 16)\n",
    "    plt.xlabel(\"Frequency (Hz)\", fontsize = 15)\n",
    "    plt.ylabel(\"L2 Loss\", fontsize = 15)\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "\n",
    "def time_plot(analysis_obj, experiment_num = 0, title = \"Observer 4 experiment\", rolling = None, save = None):\n",
    "    time_rDF = analysis_obj.rDF_time\n",
    "    time_loss_df =  time_rDF[time_rDF[\"experiment #\"] == experiment_num]\n",
    "    #display(time_loss_df)\n",
    "    \n",
    "    plt.figure(figsize = (12,5))\n",
    "    \n",
    "    #sns.scatterplot(x = \"time\", y = \"L2_loss\", data = time_loss_df, hue = \"model\", alpha = 0.1)\n",
    "\n",
    "    rollings = {}\n",
    "    if rolling:\n",
    "        mean_ = []\n",
    "        #print( np.unique(time_loss_df.model))\n",
    "        for model in np.unique(time_loss_df.model):\n",
    "            sub_df = time_loss_df[time_loss_df.model == model]\n",
    "            rollings[model]= sub_df.L2_loss.rolling(rolling).mean()\n",
    "        \n",
    "        time_loss_df[\"rolling_L2\"] = -1\n",
    "        for model in np.unique(time_loss_df.model):\n",
    "            time_loss_df.rolling_L2[time_loss_df.model == model] = rollings[model]\n",
    "        sns.lineplot(x = \"time\", y = \"rolling_L2\", data = time_loss_df, hue = \"model\", alpha = 0.9)\n",
    "    else:\n",
    "        sns.lineplot(x = \"time\", y = \"L2_loss\", data = time_loss_df, hue = \"model\", alpha = 0.9)\n",
    "    lb = 0\n",
    "    ub = np.percentile(time_loss_df.L2_loss, 95)\n",
    "    #plt.ylim((lb,ub))\n",
    "    \n",
    "    title = title\n",
    "    plt.title(title, fontsize = 16)\n",
    "    plt.xlabel(\"Time (seconds)\", fontsize = 15)\n",
    "    plt.ylabel(\"L2 Loss\", fontsize = 15)\n",
    "    sns.despine()\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c361935791492d8fc83f3c81ae2786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='experiment list, loading data...', max=2.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f809831f362c4b73a85c4676df8c863f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='experiment list, fixing interpolation...', max=2.0, style…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fix_interpolation() missing 1 required positional argument: 'exper_spec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/DL_LAB/Reservoir/MARIOS/PyFiles/analysis.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"final_results/150_500Hz_cyclic.pickle\"]#\"experiment_results/publish/split_0.5/block_cyclicN_Targidx_40N_Obsidx_26.pickle\"]\n\u001b[1;32m      8\u001b[0m     dl_expers = EchoStateAnalysis(dl_path_list, model = \"cyclic\",\n\u001b[0;32m----> 9\u001b[0;31m                                   ip_use_observers = True, ip_method = \"linear\")\n\u001b[0m",
      "\u001b[0;32m~/Desktop/DL_LAB/Reservoir/MARIOS/PyFiles/analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_list, model, ip_method, bp, force, ip_use_observers, data_type)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_interpolation_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_interpolation_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exponential\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ip: linear\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL_LAB/Reservoir/MARIOS/PyFiles/analysis.py\u001b[0m in \u001b[0;36mchange_interpolation_pickle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mip_method_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mip_methods_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mtmp_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_interpolation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mip_method_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                 \u001b[0mexperiment_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nrmse\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ip: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mip_method_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nrmse\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interpolation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mexperiment_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ip: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mip_method_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interpolation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fix_interpolation() missing 1 required positional argument: 'exper_spec'"
     ]
    }
   ],
   "source": [
    "COMBINED_ANALYSIS_NOT_LOADED = True\n",
    "if COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    dl_path_list = glob.glob('final_results/*cyclic.pickle')\n",
    "\n",
    "    dl_path_list = [  #\"experiment_results/publish/split_0.5/block_cyclicN_Targidx_40N_Obsidx_50.pickle\",\n",
    "                     \"final_results/500_900Hz_cyclic.pickle\",\n",
    "    \"final_results/150_500Hz_cyclic.pickle\"]#\"experiment_results/publish/split_0.5/block_cyclicN_Targidx_40N_Obsidx_26.pickle\"]\n",
    "    dl_expers = EchoStateAnalysis(dl_path_list, model = \"cyclic\",\n",
    "                                  ip_use_observers = True, ip_method = \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pickle_A.experiment_lst[0] = pickle_zhizhuo.experiment_lst[0] \n",
    "def quick_plot(n, analysis, outfile = None ):\n",
    "    \n",
    "    if type(analysis) != list:\n",
    "        analysis = [analysis]\n",
    "    \n",
    "    #only save the plot if requested.\n",
    "    if outfile:\n",
    "        save_path = \"fig/\" + outfile \n",
    "        save_path_time = save_path + \"_freq\"\n",
    "        save_path_freq = save_path + \"_time\"\n",
    "    else:\n",
    "        save_path_time, save_path_freq  = None, None\n",
    "        outfile = \"\"\n",
    "    for i, analysis_ in enumerate(analysis):\n",
    "        models = list(analysis_.experiment_lst[0][\"nrmse\"].keys())\n",
    "        \n",
    "        analysis_.build_loss_df(group_by = \"freq\", models = models,  columnwise = False) #models = [\"cyclic\",\"ip: linear\"]\n",
    "        analysis_.build_loss_df(group_by = \"time\", models = models, columnwise = False)\n",
    "        freq_plot(analysis_, n, title = outfile + \" Avg. L2 Loss vs Frequency\", save = save_path_freq)\n",
    "        time_plot(analysis_, experiment_num = n, rolling = 150, save = save_path_time,\n",
    "              title = outfile + \" Avg. L2 Loss vs Time (rolling average)\")\n",
    "    return analysis[0].rDF_time\n",
    "if COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    quick_plot(0, dl_expers)\n",
    "    quick_plot(1, dl_expers)\n",
    "#quick_plot(2, dl_expers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    random_path_list = glob.glob('final_results/*random.pickle')\n",
    "    print(random_path_list)\n",
    "\n",
    "    random_expers = EchoStateAnalysis(random_path_list, model = \"uniform\",\n",
    "                                  ip_use_observers = True, ip_method = \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl_expers.hyper_parameter_plot()\n",
    "if COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    #check out whats what\n",
    "    n = 0\n",
    "    hi = [dl_expers, random_expers]\n",
    "    quick_plot(n, hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_dict_combine(exper1, exper2):\n",
    "    for key1, value1 in exper1.items():\n",
    "        if type(exper1[key1]) == dict:\n",
    "            #print(key1)\n",
    "            exper1[key1] = {**exper1[key1], **exper2[key1]}\n",
    "            #print(exper1[key1])\n",
    "    return exper1\n",
    "def analysis_combine(analysis1, analysis2, n):\n",
    "    for i, exper in enumerate(analysis1.experiment_lst):\n",
    "        print(i)\n",
    "        analysis1.experiment_lst[i] = recursive_dict_combine(dl_expers.experiment_lst[i], \n",
    "                                                             random_expers.experiment_lst[i])\n",
    "    return(analysis1)\n",
    "if COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    combined_analysis = analysis_combine(dl_expers, random_expers, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the combined analysis:\n",
    "#import pickle\n",
    "\n",
    "a = {'combined_analysis': combined_analysis}\n",
    "\n",
    "with open('final_results/combined_analysis.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "if not COMBINED_ANALYSIS_NOT_LOADED:\n",
    "    with open('final_results/combined_analysis.pickle', 'rb') as handle:\n",
    "        combined_analysis = pickle.load(handle)[\"combined_analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quick_plot(0, b[\"combined_analysis\"], outfile = \"500_900Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_plot(1, combined_analysis, outfile = \"150_500Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_analysis.hyper_parameter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delay Line: Reservoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.full((3,3), fill_value = 1)\n",
    "cyclic_weight = 1\n",
    "n_nodes = 10\n",
    "\n",
    "weights = np.zeros((n_nodes, n_nodes), dtype=np.int32) #np.float32)\n",
    "#weights[0, -1] = cyclic_weight <-- This is the only difference between the cyclic reservoir and the delay line.\n",
    "for i in range(n_nodes - 1):\n",
    "    #weights[i + 1, i] = cyclic_weight\n",
    "    weights[i+1, i] = cyclic_weight\n",
    "#weights[0, ]\n",
    "pd.DataFrame(weights)\n",
    "plt.imshow(weights)\n",
    "plt.xlabel(\"N: Number of Nodes\")\n",
    "plt.ylabel(\"N: Number of Nodes\")\n",
    "plt.title(\"Reservoir Weights\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.full((3,3), fill_value = 1)\n",
    "cyclic_weight = 1\n",
    "n_nodes = 10\n",
    "\n",
    "weights = np.zeros((n_nodes, n_nodes), dtype=np.int32) #np.float32)\n",
    "#weights[0, -1] = cyclic_weight <-- This is the only difference between the cyclic reservoir and the delay line.\n",
    "for i in range(n_nodes - 1):\n",
    "    #weights[i + 1, i] = cyclic_weight\n",
    "    weights[i+1, i] = cyclic_weight\n",
    "weights[0, -1] = cyclic_weight\n",
    "pd.DataFrame(weights)\n",
    "plt.imshow(weights)\n",
    "plt.xlabel(\"N: Number of Nodes\")\n",
    "plt.ylabel(\"N: Number of Nodes\")\n",
    "plt.title(\"Cylic Reservoir Weights\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "connectivity = 0.2\n",
    "spectral_radius = 0.99\n",
    "\n",
    "accept = np.random.uniform(size = (n, n)) < connectivity\n",
    "\n",
    "weights = np.random.uniform( -1., 1., size = (n, n))\n",
    "weights *= accept\n",
    "max_eigenvalue = np.abs( np.linalg.eigvals( weights)).max()\n",
    "\n",
    "\n",
    "# Set spectral radius of weight matrix\n",
    "weights *= spectral_radius / max_eigenvalue\n",
    "plt.imshow(weights)\n",
    "plt.xlabel(\"N: Number of Nodes\")\n",
    "plt.ylabel(\"N: Number of Nodes\")\n",
    "plt.title(\"Random Reservoir Weights\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 100\n",
    "alpha = 50\n",
    "n_nodes = 10\n",
    "n_inputs = 10\n",
    "orig_inputs = np.random.uniform(-1, 1, size = 4)\n",
    "\n",
    "input_bias = np.full((n_nodes,1), fill_value = alpha)\n",
    "\n",
    "input_weights = np.full((1,n_inputs), fill_value = beta)\n",
    "input_weights_zeroes = np.zeros((n_nodes -1, n_inputs))\n",
    "\n",
    "input_weights = np.vstack((input_weights, input_weights_zeroes))\n",
    "input_weights = np.hstack((input_bias, input_weights))\n",
    "plt.imshow(input_weights, aspect = 0.5)\n",
    "plt.xlabel(\"M: number of inputs\")\n",
    "plt.ylabel(\"N: number of Nodes\")\n",
    "plt.title(\"Delay Line Input Weights\")\n",
    "plt.colorbar()\n",
    "inputs = np.hstack((1, orig_inputs))\n",
    "print(inputs)\n",
    "print(alpha + np.sum(orig_inputs)*beta)\n",
    "input_weights @ inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_sq(arr):\n",
    "    temp = np.sin(arr)\n",
    "    return(temp**2)\n",
    "sin_sq(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legitimate Delay line. The only output is a single number for the first node of the Rc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 100\n",
    "alpha = 50\n",
    "n_nodes = 10\n",
    "n_inputs = 6\n",
    "input_bias = np.full((1,), fill_value = alpha)\n",
    "input_weights = np.full((n_inputs,), fill_value = beta)\n",
    "\n",
    "input_weights = np.hstack((input_bias, input_weights))\n",
    "input_weights_zeroes = np.zeros((n_nodes-1, n_inputs + 1))\n",
    "\n",
    "input_weights = np.vstack((input_weights, input_weights_zeroes))\n",
    "print(\"Input Weights (bias = 1, beta = 100)\")\n",
    "display(input_weights)\n",
    "plt.imshow(input_weights, aspect = 0.5)\n",
    "plt.xlabel(\"M: number of inputs\")\n",
    "plt.ylabel(\"N: number of Nodes\")\n",
    "plt.title(\"Delay Line Input Weights\")\n",
    "plt.colorbar()\n",
    "orig_inputs = np.random.uniform(-1, 1, size = n_inputs)\n",
    "inputs = np.hstack((1, orig_inputs))\n",
    "print(\"\")\n",
    "print(\"Inputs\")\n",
    "print(inputs)\n",
    "print(alpha + np.sum(orig_inputs)*beta)\n",
    "print(\"\")\n",
    "print(\"Output\")\n",
    "display(input_weights @ inputs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 100\n",
    "alpha = 50\n",
    "n_nodes = 10\n",
    "n_inputs = 6\n",
    "input_bias = np.full((1,), fill_value = alpha)\n",
    "input_weights = np.full((n_inputs,), fill_value = beta)\n",
    "\n",
    "input_weights = np.hstack((input_bias, input_weights))\n",
    "input_weights_zeroes = np.zeros((n_nodes-1, n_inputs + 1))\n",
    "\n",
    "input_weights = np.random.uniform(-1, 1, size = (n_nodes, n_inputs))\n",
    "#print(\"Input Weights (bias = 1, beta = 100)\")\n",
    "#display(input_weights)\n",
    "plt.imshow(input_weights, aspect = 0.5)\n",
    "plt.xlabel(\"M: number of inputs\")\n",
    "plt.ylabel(\"N: number of Nodes\")\n",
    "plt.title(\"Random Input Weights (cyclic, uniform, exponential)\")\n",
    "plt.colorbar()\n",
    "orig_inputs = np.random.uniform(-1, 1, size = n_inputs)\n",
    "inputs = np.hstack((1, orig_inputs))\n",
    "print(\"\")\n",
    "print(\"Inputs\")\n",
    "print(inputs)\n",
    "print(alpha + np.sum(orig_inputs)*beta)\n",
    "print(\"\")\n",
    "print(\"Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delay Line input weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls experiment_results/medium/split_0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Prediction 2.0\n",
    "The key to the pure prediction is to break it down. We are going to take in a parameter k, which is the maximum time-series to include in a block prediction. Then we will break the target matrix A into component sections.\n",
    "\n",
    "This will simply require an appropriate series. The next step is to develop an appropriate method for combining the different matrices.\n",
    "\n",
    "There can be two options: overlapping and not overlapping. If there are over-lapping predictions we take a simple average. Otherwise we simply combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls spectrogram_data/medium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_med = loadmat('spectrogram_data/medium/Intensity_1024.mat')[\"M\"]\n",
    "plt.imshow(A_med)\n",
    "plt.show()\n",
    "A_med = A_med.T\n",
    "plt.imshow(A_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zhizhuo experiment 1: finding the right indices ; 249, 289\n",
    "lst__ = [i for i in list(range(250 - 1, 289))]\n",
    "assert len(lst__) == 40\n",
    "assert lst__[0] + 1 == 250\n",
    "assert lst__[-1] + 1 == 289\n",
    "print(len(lst__))\n",
    "print(lst__[0] + 1)\n",
    "print(lst__[-1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#250-289 inclusive\n",
    "def get_series(matrixx, k, zhizhuo_endpoints = None, shape_assert = None, plott = False, start = 0):\n",
    "    matrixx = (matrixx - np.mean(matrixx)) / np.std(matrixx)\n",
    "    #start = 0\n",
    "    stop = matrixx.shape[1]\n",
    "    print(\"stop: \" + str(stop))\n",
    "    break_points = range(start, stop, k)\n",
    "    break_points = list(break_points)\n",
    "    if zhizhuo_endpoints:\n",
    "        zlb, zub = zhizhuo_endpoints[0], zhizhuo_endpoints[1]\n",
    "    \n",
    "    train_arrays  = []\n",
    "    target_arrays = []\n",
    "    \n",
    "    #### --> test area\n",
    "    if plott:\n",
    "        fig, ax = plt.subplots(1, len(break_points) - 1, figsize = (8, 12))\n",
    "    for i in range(len(break_points) - 1):\n",
    "        lb, ub = break_points[i], break_points[i+1]\n",
    "        if zhizhuo_endpoints:\n",
    "            sub_df = matrixx[ zlb:zub, lb:ub]\n",
    "            sub_df_train = matrixx[ (zlb - 100):zlb, lb:ub]\n",
    "            train_arrays.append(sub_df_train)\n",
    "            target_arrays.append(sub_df)\n",
    "            if plott:\n",
    "                ax[i].imshow(sub_df, aspect = 10)\n",
    "            if shape_assert:\n",
    "                assert( shape_assert == sub_df.shape[0])\n",
    "        else:\n",
    "            ax[i].imshow(matrixx[:,lb:ub], aspect = 1)\n",
    "            target_arrays.append(matrixx[:,lb:ub])\n",
    "    if plott:\n",
    "        plt.show()\n",
    "    print(len(train_arrays))\n",
    "    \n",
    "    dictt = {\"Train\" :[np.ones(arr.shape) for arr in train_arrays] , \n",
    "             \"xTr\" : train_arrays,\n",
    "             \"target\" :  [np.ones(arr.shape) for arr in target_arrays], #np.ones(self.xTr.shape), np.ones(self.xTe.shape)\n",
    "             \"xTe\" : target_arrays }\n",
    "    return dictt\n",
    "# The question is obvious,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_up_A_med = get_series(A_med, k = 1, \n",
    "                             zhizhuo_endpoints = [249, 289], \n",
    "                             shape_assert = 40, start = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(broken_up_A_med[\"xTr\"][0], aspect = 0.5)\n",
    "plt.show()\n",
    "plt.imshow(broken_up_A_med[\"xTe\"][0], aspect = 0.5)\n",
    "print(broken_up_A_med[\"xTr\"][0].shape)\n",
    "print(broken_up_A_med[\"xTe\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = broken_up_A_med[\"Train\"][0]\n",
    "print(Train.shape)\n",
    "xTr = broken_up_A_med[\"xTr\"][0]\n",
    "plt.imshow(xTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_up_A_med[\"xTe\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = { 'connectivity':    (-3, 0),\n",
    "            'n_nodes':   1000,\n",
    "            'spectral_radius': (0.001, 0.999),\n",
    "            'regularization':  (-6, 4),\n",
    "            \"leaking_rate\" :   (0.001, 1)}\n",
    "esn_cv_spec = EchoStateNetworkCV(bounds = bounds, \n",
    "                                 subsequence_length = 40, \n",
    "                                 esn_feedback = False, \n",
    "                                 eps = 10**(-5),\n",
    "                                 cv_samples = 3,\n",
    "                                 model_type = \"uniform\",\n",
    "                                 n_jobs = 2,\n",
    "                                 initial_samples = 150\n",
    "                                )\n",
    "assert broken_up_A_med[\"Train\"][0].shape == broken_up_A_med[\"xTr\"][0].shape\n",
    "best_args_ = esn_cv_spec.optimize(y = xTr, x = None)#np.ones(xTr.shape))\n",
    "#self.best_arguments =  self.esn_cv.optimize(x = self.Train, y = self.xTr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = \"\"\"best_args_ = {'cyclic_res_w': 0.001246639973578002,\n",
    "              'cyclic_input_w': 0.018582012103828045,\n",
    "              'cyclic_bias': 0.050674025611653906,\n",
    "              'leaking_rate': 0.001,\n",
    "              'n_nodes': 1000,\n",
    "              'random_seed': 123,\n",
    "              'feedback': False,\n",
    "              \"model_type\" : \"cyclic\"}\n",
    "best_args_ = {'connectivity': 1.0,\n",
    " 'spectral_radius': 0.5420020711421967,\n",
    " 'regularization': 1.367683798343926,\n",
    " 'leaking_rate': 1.0,\n",
    " 'n_nodes': 1000,\n",
    " 'random_seed': 123,\n",
    " 'feedback': False,\n",
    " 'model_type' : \"uniform\"}\"\"\"\n",
    "best_args_ = {'cyclic_res_w': 0.001246639973578002,\n",
    "              'cyclic_input_w': 0.018582012103828045,\n",
    "              'cyclic_bias': 0.050674025611653906,\n",
    "              'leaking_rate': 0.001,\n",
    "              'n_nodes': 1000,\n",
    "              'random_seed': 123,\n",
    "              'feedback': False,\n",
    "              \"model_type\" : \"cyclic\"}\n",
    "\n",
    "\n",
    "esn_spec =  EchoStateNetwork(**best_args_)#, model_type = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(broken_up_A_med[\"xTr\"][0].shape)\n",
    "print(np.ones(broken_up_A_med[\"xTr\"][0].shape).shape)\n",
    "broken_up_A_med.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_up_A_med[\"target\"][0].shape\n",
    "type(broken_up_A_med[\"Train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = broken_up_A_med[\"xTe\"][0]\n",
    "xTr, xTe = broken_up_A_med[\"xTr\"][0], broken_up_A_med[\"xTe\"][0]\n",
    "plt.imshow(test_)\n",
    "\n",
    "print(broken_up_A_med.keys())\n",
    "plt.imshow(broken_up_A_med[\"xTr\"][0], aspect = 0.2)\n",
    "esn_spec.train(y = broken_up_A_med[\"xTr\"][0],  x = None)\n",
    "pred_ = esn_spec.predict( n_steps = test_.shape[0], x = broken_up_A_med[\"target\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(broken_up_A_med[\"xTr\"][0])\n",
    "sns.heatmap(broken_up_A_med[\"Train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nrmse(test_, pred_))\n",
    "plt.imshow(pred_)\n",
    "plt.show()\n",
    "plt.imshow(test_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enablePrint()\n",
    "pickle_list = glob.glob('experiment_results/publish/*/*.pickle')\n",
    "pickle_list = pickle_list[1:]\n",
    "print(pickle_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check if path list has duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pickle_list) == len(list(np.unique(pickle_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_A = EchoStateAnalysis(pickle_list, model = \"uniform\", ip_use_observers = True, ip_method = \"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following helper functions are assisting me to accomplish getting the final figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_A = dl_expers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_freqs = []\n",
    "for i, experiment in enumerate(pickle_A.experiment_lst):\n",
    "    tf = get_target_freq(experiment)\n",
    "    target_freqs.append(tf)\n",
    "    #print(experiment[\"resp_idx\"])\n",
    "    if IdxMatch(experiment):\n",
    "        freq_plot(pickle_A, experiment_num = i)\n",
    "    if IdxMatch(experiment, n_obs = 96):\n",
    "        freq_plot(pickle_A, experiment_num = i)\n",
    "target_freqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Zhizhuo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_zhizhuo_data(analysis_obj_, experiment_num):\n",
    "    analysis_obj = copy.copy(analysis_obj_)#.copy()\n",
    "    obs4_yhat = loadmat('/Users/hayden/Desktop/ytesthat_ob4.mat')\n",
    "    zhizhuo_rez = obs4_yhat[\"ytesthat_ob4\"]\n",
    "    zhizhuo_rez = zhizhuo_rez.T\n",
    "    zhizhuo_rez = np.flip(zhizhuo_rez, axis = 1)\n",
    "    plt.imshow(zhizhuo_rez, aspect = 0.05)\n",
    "    #0 is zhizhuo_test\n",
    "    for n in [experiment_num]:\n",
    "        #hi = analysis_obj.get_experiment(analysis_obj.experiment_lst[n])\n",
    "        analysis_obj.model = \"uniform\"\n",
    "\n",
    "        exper_unif = analysis_obj.get_experiment(analysis_obj.experiment_lst[n])\n",
    "\n",
    "        unif_nrmse = nrmse(exper_unif.prediction, exper_unif.xTe)\n",
    "        analysis_obj.experiment_lst[n][\"prediction\"][\"uniform\"] = exper_unif.prediction\n",
    "        analysis_obj.experiment_lst[n][\"nrmse\"][\"uniform\"] = unif_nrmse\n",
    "\n",
    "        zhizhuo_nrmse = nrmse(zhizhuo_rez, exper_unif.xTe)\n",
    "        analysis_obj.experiment_lst[n][\"prediction\"][\"zhizhuo\"] = zhizhuo_rez\n",
    "        analysis_obj.experiment_lst[n][\"nrmse\"][\"zhizhuo\"] = zhizhuo_nrmse\n",
    "\n",
    "        #nrmse(exper_unif.prediction, exper_unif.xTe)\n",
    "        analysis_obj.model = \"exponential\"\n",
    "        exper_exp = analysis_obj.get_experiment(analysis_obj.experiment_lst[n])\n",
    "        print(n)\n",
    "        exper_exp.model = \"exponential\"\n",
    "        exp_nrmse = nrmse(exper_exp.prediction, exper_exp.xTe)\n",
    "\n",
    "        analysis_obj.experiment_lst[n][\"prediction\"][\"exponential\"] = exper_exp.prediction\n",
    "        analysis_obj.experiment_lst[n][\"nrmse\"][\"exponential\"] = exp_nrmse\n",
    "        analysis_obj.model = \"uniform\"\n",
    "    print(\"\")\n",
    "    return analysis_obj\n",
    "pickle_zhizhuo = add_zhizhuo_data(pickle_A, 0)\n",
    "pickle_zhizhuo.experiment_lst = [pickle_zhizhuo.experiment_lst[0]]\n",
    "pickle_zhizhuo.build_loss_df(group_by = \"freq\", models = [\"uniform\", \"exponential\", \"zhizhuo\", \"ip: linear\"], columnwise = False)\n",
    "pickle_zhizhuo.build_loss_df(group_by = \"time\", models = [\"uniform\", \"exponential\", \"zhizhuo\", \"ip: linear\"], columnwise = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_zhizhuo.build_loss_df(group_by = \"freq\", models = [\"uniform\", \"exponential\", \"zhizhuo\", \"ip: linear\"], columnwise = False)\n",
    "pickle_zhizhuo.build_loss_df(group_by = \"time\", models = [\"uniform\", \"exponential\", \"zhizhuo\", \"ip: linear\"], columnwise = False)\n",
    "\n",
    "pickle_A.experiment_lst[0] = pickle_zhizhuo.experiment_lst[0] \n",
    "freq_plot(pickle_zhizhuo, 0, title = \"Observer 4 experiment: Avg. L2 Loss vs Frequency\", save = \"obs_4_freq\")\n",
    "time_plot(pickle_zhizhuo, experiment_num = 0, rolling = 150, save = \"obs_4_time\",\n",
    "          title = \"Observer 4 experiment: Avg. L2 Loss vs Time (rolling average)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots(experiment_num):\n",
    "    print(\"experiment number: \" + str(experiment_num))\n",
    "    experiment = pickle_A.experiment_lst[experiment_num]\n",
    "    print(\"target_freq: \" + str(get_target_freq(experiment)))\n",
    "    print(\"n observers: \" + str(len(experiment[\"obs_idx\"])))\n",
    "    #print(experiment[\"resp_idx\"])\n",
    "    freq_plot(pickle_A, experiment_num = experiment_num, \n",
    "              title = \"low Frequency experiment: \" + \"loss vs freq\")\n",
    "    time_plot(pickle_A, experiment_num = experiment_num, rolling = 150, \n",
    "              title = \"low Frequency experiment: \" + \"time vs freq\")\n",
    "get_plots(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_plot(pickle_A, 1, title = \"Low Frequency experiment: Avg. L2 Loss vs Frequency\", save = \"low_freq_freq\")\n",
    "time_plot(pickle_A, experiment_num = 1, rolling = 150, save = \"low_freq_time\",\n",
    "          title = \"Low Frequency experiment: Avg. L2 Loss vs Time (rolling average)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_idx = pickle_A.experiment_lst[1][\"resp_idx\"]\n",
    "print(resp_idx)\n",
    "f = np.array(pickle_A.experiment_lst[1][\"f\"])\n",
    "f[resp_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_plot(pickle_A, 2, title = \"Rena experiment: Avg. L2 Loss vs Frequency\", save = \"Rena_freq\")\n",
    "time_plot(pickle_A, experiment_num = 2, rolling = 150, save = \"Rena_time\",\n",
    "          title = \"Rena experiment: Avg. L2 Loss vs Time (rolling average)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, len(pickle_A.experiment_lst)):\n",
    "    get_plots(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_A.hyper_parameter_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pickle_A.experiment_lst)):\n",
    "    freq_plot(pickle_A, i)\n",
    "    time_plot(pickle_A, experiment_num = i, rolling = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_A.make_R_barplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_rDF = pickle_A.rDF_time\n",
    "plt.figure(figsize = (16,8))\n",
    "pickle_loss_df_50_50_split =  pickle_rDF[pickle_rDF.split == 0.5]\n",
    "#pickle_loss_df_50_50_split = pickle_loss_df_50_50_split[pickle_loss_df_50_50_split.model != \"exponential\"]\n",
    "\n",
    "mean_ =  pickle_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = pickle_loss_df_50_50_split[\"L2_loss\"], data = pickle_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = pickle_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)\n",
    "plt.title(\"block_N_Targidx_40N_Obsidx_26\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle_rDF = pickle_A.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "pickle_loss_df_50_50_split =  pickle_rDF[pickle_rDF.split == 0.7]\n",
    "#pickle_loss_df_50_50_split = pickle_loss_df_50_50_split[pickle_loss_df_50_50_split.model != \"exponential\"]\n",
    "\n",
    "mean_ =  pickle_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = pickle_loss_df_50_50_split[\"R\"], data = pickle_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = pickle_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)\n",
    "plt.title(\"block_N_Targidx_40N_Obsidx_26\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_rDF = pickle_A.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "pickle_loss_df_50_50_split =  pickle_rDF[pickle_rDF.split == 0.9]\n",
    "#pickle_loss_df_50_50_split = pickle_loss_df_50_50_split[pickle_loss_df_50_50_split.model != \"exponential\"]\n",
    "\n",
    "mean_ =  pickle_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = pickle_loss_df_50_50_split[\"R\"], data = pickle_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = pickle_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)\n",
    "plt.title(\"block_N_Targidx_40N_Obsidx_26\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_pretty_pics(experiment_number = 0, \n",
    "                     modelz = [\"ip: linear\", \"uniform\",  \"exponential\"], #\"zhizhuo\",\n",
    "                    show_images = False, show_residuals = False\n",
    "                    ):\n",
    "    #blockPrint()\n",
    "    if experiment_number == 0:\n",
    "        zhizhuo_label = \"obs4\"\n",
    "    elif experiment_number == 4:\n",
    "        zhizhuo_label = \"obs5\"\n",
    "    else:\n",
    "        zhizhuo_label = \"experiment \" + str(experiment_number)\n",
    "        \n",
    "    if experiment_number == 0:\n",
    "        zhizhuo_label = \"low frequency\"\n",
    "    \n",
    "    spec = pickle_A.experiment_lst[experiment_number]\n",
    "    f =  spec[\"f\"]\n",
    "    freqs_dict = { idx : f[idx] for idx in spec[\"obs_idx\"]}\n",
    "    freqs_ = [f[idx] for idx in spec[\"resp_idx\"]]\n",
    "    \n",
    "    truth = spec[\"xTe\"]\n",
    "    if show_images:\n",
    "        plt.imshow(truth, aspect = 0.01)\n",
    "        plt.title(\"Ground truth\")\n",
    "        plt.show()\n",
    "        fig, ax = plt.subplots(2,2, figsize = (12, 6))\n",
    "        ax = ax.flatten()\n",
    "        for i, model in enumerate(modelz):\n",
    "\n",
    "            ax[i].imshow(spec[\"prediction\"][model], aspect = 0.01)\n",
    "            ax[i].set_title(model)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    nrmses = []\n",
    "    \n",
    "    #for i, model in enumerate(modelz):\n",
    "        \n",
    "    residuals = []\n",
    "    for i, model in enumerate(modelz):\n",
    "        pred_ = spec[\"prediction\"][model]\n",
    "        #if model == \"zhizhuo\":\n",
    "        #    pred_ = np.flip(pred_, axis = 1)\n",
    "        nrmse_spec = nrmse(pred_, truth)\n",
    "        nrmses.append({model : nrmse_spec})\n",
    "        residuals.append(np.abs(truth - pred_))\n",
    "        \n",
    "    if show_residuals:\n",
    "        fig, ax = plt.subplots(2,2, figsize = (12, 6))\n",
    "        ax = ax.flatten()\n",
    "        for i, model in enumerate(modelz):\n",
    "            sns.heatmap(residuals[i], ax = ax[i])\n",
    "            ax[i].set_title(model + \" residuals^2, R: \" + str(round(nrmse_spec, 5)))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    palette_ = dict(zip(modelz, sns.color_palette(\"tab10\")[0:4]))\n",
    "    # = {\"uniform\": \"C0\", \"best interpolation\": \"C1\", \"zhizhuo\": \"C2\", \"expoenential\": \"k\"}\n",
    "    \n",
    "    nrmse_df = pd.DataFrame(nrmses)\n",
    "    nrmse_df = nrmse_df.melt()\n",
    "    nrmse_df.columns = [\"model\", \"R\"]\n",
    "    nrmse_df = nrmse_df.sort_values(by='R', ascending=True)\n",
    "    \n",
    "    modelz_ord = list(nrmse_df.model.values)\n",
    "    \n",
    "    #barplot\n",
    "    #fig, ax = plt.subplots(1,1, figsize = (12, 6.5))\n",
    "    display(nrmse_df)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    barplot = sns.barplot(x = \"model\", y = \"R\", data = nrmse_df, palette = palette_)\n",
    "    #pal.as_hex()\n",
    "    plt.title(\"RMSE for \" + zhizhuo_label)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.savefig('obs5_R.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "make_pretty_pics(0, show_residuals = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# serious problem: experiments are getting duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pickle_A.experiment_lst:\n",
    "    spec = i[\"resp_idx\"]\n",
    "    \n",
    "    if len(spec) > 1:\n",
    "        exp_resp_lst = (spec)\n",
    "        exp_f = i.keys()\n",
    "        print(exp_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "new_f = sio.loadmat(\"/Users/hayden/Desktop/f_new.mat\")\n",
    "new_f = new_f[\"f\"]\n",
    "\n",
    "freq_imp = [list(new_f[idx])[0] for idx in exp_resp_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f[exp_resp_lst[-1] +13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_results_df = pickle_A.R_results_df\n",
    "R_results_df_rel = pickle_A.R_results_df_rel\n",
    "R_results_df[\"experiment\"] = [0,1,2] * 2\n",
    "R_results_df_rel = R_results_df_rel[R_results_df_rel[\"model\"] != \"interpolation\"]\n",
    "R_results_df_rel[\"experiment\"] = [0,1,2] * 2\n",
    "display(R_results_df_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = \"model\", y = \"R\", data = R_results_df_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize = (14, 15))\n",
    "ax = ax.flatten()\n",
    "print(ax)\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    R_df_spec = R_results_df[R_results_df.experiment == i]\n",
    "    R_df_rel_spec = R_results_df_rel[R_results_df_rel.experiment == i]\n",
    "    \n",
    "    #sns.barplot(x = \"model\", y = \"R\", data = R_df_spec)#, ax=ax[0])\n",
    "    R_df_spec =R_df_spec.drop(columns = \"experiment\")\n",
    "    R_df_rel_spec =R_df_rel_spec.drop(columns = \"experiment\")\n",
    "    \n",
    "    #sns.violinplot(x = \"model\", y = \"R\", data = self.R_results_df_rel, ax=ax[1])\n",
    "    sns.barplot(x = \"model\", y = \"R\", data = R_df_spec, ci = None, ax=ax[2*i])\n",
    "    \n",
    "    sns.barplot(x = \"model\", y = \"R\", data = R_df_rel_spec, ci = None, ax=ax[2*i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_rDF = pickle_A.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "pickle_loss_df_50_50_split =  pickle_rDF[pickle_rDF.split == 0.9]\n",
    "#pickle_loss_df_50_50_split = pickle_loss_df_50_50_split[pickle_loss_df_50_50_split.model != \"exponential\"]\n",
    "\n",
    "mean_ =  pickle_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = pickle_loss_df_50_50_split[\"R\"], data = pickle_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = pickle_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)\n",
    "plt.title(\"block_N_Targidx_40N_Obsidx_26\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with asymmetric experiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'PyFiles/imports.py'\n",
    "%run -i 'PyFiles/helpers.py'\n",
    "%run -i \"PyFiles/experiment.py\"\n",
    "%run -i \"PyFiles/analysis.py\"\n",
    "import scipy.stats as stats\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_path_list = glob.glob('experiment_results/medium/*/*.txt')\n",
    "test_analysis = EchoStateAnalysis([medium_path_list[0]], \n",
    "                                  model = \"uniform\", \n",
    "                                  ip_use_observers = True, \n",
    "                                  ip_method = \"linear\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj_test = test_analysis.experiment_lst[0]\n",
    "test_experiment = test_analysis.get_experiment(json_obj_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj_test[\"best arguments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_esn = test_experiment.esn_spec\n",
    "test2_esn = EchoStateNetwork(**json_obj_test[\"best arguments\"][\"exponential\"],\n",
    "                             resp_idx = json_obj_test[\"resp_idx\"],\n",
    "                             obs_idx = json_obj_test[\"obs_idx\"],\n",
    "                             exponential = False, plot = True, \n",
    "                             llambda2 = 10**(-2))\n",
    "test2_esn.noise = 0.5\n",
    "test2_esn.get_exp_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = np.array([[16,16,17,18], [15,15,15,15]])\n",
    "np.hstack((hi, np.array([[2],[2]])))\n",
    "# {'llambda': 0.00938595717962852, 'llambda2': 0.002908498759116776, 'connectivity': 1.0, 'spectral_radius': 0.48154601180553436, 'regularization': 0.3676013152573216, 'leaking_rate': 0.7179883186221123, 'noise': 1.2589254117941673, 'n_nodes': 1000, 'random_seed': 123}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_esn.exp_weights.shape\n",
    "test2_esn.obs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_esn = test_experiment.esn_spec\n",
    "test2_esn = EchoStateNetwork(**json_obj_test[\"best arguments\"][\"exponential\"],\n",
    "                             resp_idx = json_obj_test[\"resp_idx\"],\n",
    "                             obs_idx = json_obj_test[\"obs_idx\"],\n",
    "                             exponential = False, plot = True, dual_lambda = True, \n",
    "                             llambda2 = 0.0001)\n",
    "test2_esn.noise = 0.1\n",
    "test2_esn.get_exp_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_esn = test_experiment.esn_spec\n",
    "test2_esn = EchoStateNetwork(**json_obj_test[\"best arguments\"][\"exponential\"],\n",
    "                             resp_idx = json_obj_test[\"resp_idx\"],\n",
    "                             obs_idx = json_obj_test[\"obs_idx\"],\n",
    "                             exponential = False, plot = True, dual_lambda = True, \n",
    "                             llambda2 = 10)\n",
    "test2_esn.noise = 0.5\n",
    "test2_esn.get_exp_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(-1, 1, size=(10, 3))\n",
    "np.random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_error = np.random.normal(loc = 0, scale = 0.01, size = (10,3))\n",
    "normal_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_weights1to3 = test2_esn.exp_weights[:3]\n",
    "print(exp_weights1to3 )\n",
    "exp_weights1to3 + normal_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice([-1, 1], (3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining new pickle results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\".pickle\" in 'experiment_results/publish/split_0.5/block_N_Targidx_1N_Obsidx_4.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_A.experiment_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages, glob the path lists from experiment results\n",
    "\n",
    "Why not try something asymmetric? Asymmetric exponential weights? Otherwise we will totally collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "\n",
    "%run -i 'PyFiles/imports.py'\n",
    "%run -i 'PyFiles/helpers.py'\n",
    "%run -i \"PyFiles/experiment.py\"\n",
    "%run -i \"PyFiles/analysis.py\"\n",
    "medium_path_list = glob.glob('experiment_results/medium/*/*.txt')\n",
    "publish_path_list = glob.glob('experiment_results/publish/*/*.txt')\n",
    "publish_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## September 18th Task List:\n",
    "\n",
    "\n",
    "0) Continue to clean up analysis notebook\n",
    "1) Work to finish grading\n",
    "2) code asymmetric exponential weights\n",
    "3) Work more on biological kaggle problem\n",
    "4) Work on the paper (Rena parts)\n",
    "\n",
    "## Monday Tasks\n",
    "1) fix figure (Cycles 4 and 5) <br> \n",
    "2) check out the new biological kaggle problem (Cycle 3) <br> \n",
    "3) Select and extract (adjusted) indexes for block tests for Zhizhuo (Cycle 2) <br>\n",
    "4) Do the parts of the paper which Rena requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"PyFiles/experiment.py\"\n",
    "publish_sightIp = EchoStateAnalysis(publish_path_list, model = \"uniform\", ip_use_observers = True, ip_method = \"linear\")\n",
    "publish_sightIp.build_loss_df(models = [\"uniform\", \"exponential\", \"ip: linear\"])\n",
    "publish_sightIp.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.build_loss_df(models = [\"uniform\", \"exponential\", \"ip: linear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = publish_sightIp.get_experiment(publish_sightIp.experiment_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_f(path = '/Users/hayden/Downloads/f_3000.mat'):\n",
    "    zhiF = loadmat(path)\n",
    "    \n",
    "    \n",
    "    ff = list(zhiF[\"f\"].reshape(-1,))\n",
    "    print(ff[272])\n",
    "#\"/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/spectrogram_data/publish/f_new.mat\")\n",
    "get_f()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(publish_sightIp.rDF.model) # you need to expand this to include \"ip: nearest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment session: relative R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rolling_rel_plot(n, rolling = 100, difference = False):\n",
    "    dictLst = []\n",
    "    hi = publish_sightIp.rDF\n",
    "    #display(set(hi.model))\n",
    "    #print(\"hi\")\n",
    "\n",
    "    sub_hi = hi[hi[\"experiment #\"] == n]\n",
    "    sub_hi_unif = sub_hi.R[sub_hi.model == \"uniform\"].values\n",
    "    sub_hi_ip = sub_hi.R[sub_hi.model == \"ip: linear\"].values\n",
    "    #we want to normalize, for the sake of comparison, the r values across the different models.\n",
    "    # divide by the sum of the ip\n",
    "    lenn = len(sub_hi_ip)\n",
    "\n",
    "    denominator = np.sum(sub_hi_ip) / lenn\n",
    "\n",
    "    sub_hi_unif = pd.Series(sub_hi_unif / denominator)\n",
    "    sub_hi_ip   = pd.Series(sub_hi_ip / denominator)\n",
    "    diff = sub_hi_unif - sub_hi_ip \n",
    "\n",
    "    dict_ = {\"uniform\" :  sub_hi_unif, \"ip\" : sub_hi_ip}\n",
    "\n",
    "    dictLst.append(dict_)\n",
    "    rolling_ip = sub_hi_ip.rolling(rolling).mean()\n",
    "    rolling_unif = sub_hi_unif.rolling(rolling).mean()\n",
    "    #print(np.mean(diff))\n",
    "    diff_roll = diff.rolling(rolling).mean()\n",
    "    xx = range(len(rolling_ip))\n",
    "    if difference:\n",
    "        color_ = \"green\" if np.mean(diff)<0 else \"red\"\n",
    "        sns.scatterplot(x = xx, y = diff,  color = color_, alpha = 0.01)\n",
    "        sns.lineplot(x = xx, y = diff_roll,  color = color_, alpha = 0.3)\n",
    "        plt.ylim(-5,5)\n",
    "        plt.title(\"Difference: rel unif - interpolation: > 0 -> rc doing better\")\n",
    "        return((np.mean(diff) < 0), len(diff))\n",
    "    else:\n",
    "        sns.lineplot(x = xx, y = rolling_ip,  color = \"red\", alpha = 0.3) #label = \"interpolation\",\n",
    "        sns.lineplot(x = xx, y = rolling_unif,  color = \"blue\", alpha = 0.3) #label = \"Uniform Random RC\",\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 5))\n",
    "better_90 = []\n",
    "better_50 = []\n",
    "for i in range(29):\n",
    "    try:\n",
    "        bet = rolling_rel_plot(i, rolling = 150, difference = True)\n",
    "        if bet[1] > 400:\n",
    "            better_50.append(bet[0])\n",
    "        else:\n",
    "            better_90.append(bet[0])\n",
    "    except:\n",
    "        print(i)\n",
    "def quality(better):\n",
    "    return(str(np.sum(better)/len(better)))\n",
    "print(quality(better_50))\n",
    "print(quality(better_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the nan interpolation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_where_linear_fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#indices_where_linear_fails = []\n",
    "for i, experiment in enumerate(publish_sightIp.experiment_lst): #gets indices of nan interpolation methods.\n",
    "    spec_ip = experiment[\"nrmse\"][\"ip: linear\"]\n",
    "    if math.isnan(spec_ip):\n",
    "        hi = publish_sightIp.get_experiment(experiment)\n",
    "        hi.interpolation_method = \"griddata-nearest\"\n",
    "        hi.runInterpolation()\n",
    "        print(hi.ip_res[\"nrmse\"])\n",
    "        print(i)\n",
    "        #indices_where_linear_fails.append(i)\n",
    "        \n",
    "        publish_sightIp.experiment_lst[i][\"nrmse\"][\"ip: linear\"] = hi.ip_res[\"nrmse\"]\n",
    "        publish_sightIp.experiment_lst[i][\"prediction\"][\"ip: linear\"] = hi.ip_res[\"prediction\"]\n",
    "        #later uncomment these lines. For now I am simply overwriting ip:linear for the loss_df.\n",
    "        #publish_sightIp.experiment_lst[i][\"nrmse\"][\"best interpolation\"] = hi.ip_res[\"nrmse\"]\n",
    "        #publish_sightIp.experiment_lst[i][\"prediction\"][\"best interpolation\"] = hi.ip_res[\"prediction\"]\n",
    "    else:\n",
    "        print(\"\")\n",
    "        #publish_sightIp.experiment_lst[i][\"nrmse\"][\"best interpolation\"] = spec_ip\n",
    "        #publish_sightIp.experiment_lst[i][\"prediction\"][\"best interpolation\"] = experiment[\"prediction\"][\"ip: linear\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in publish_sightIp.experiment_lst[8][\"prediction\"].items():\n",
    "    print(np.array(value).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "for n in range(len(publish_sightIp.experiment_lst)):\n",
    "    if not n:\n",
    "        new, old = [], []\n",
    "    exper_unif = publish_sightIp.get_experiment(publish_sightIp.experiment_lst[n])\n",
    "    \n",
    "    #nrmse(exper_unif.prediction, exper_unif.xTe)\n",
    "    exper_exp = publish_sightIp.get_experiment(publish_sightIp.experiment_lst[n])\n",
    "    print(n)\n",
    "    exper_exp.model = \"exponential\"\n",
    "    new_ = nrmse(exper_exp.prediction, exper_exp.xTe)\n",
    "    publish_sightIp.experiment_lst[n][\"prediction\"][\"exponential\"] = exper_exp.prediction\n",
    "    publish_sightIp.experiment_lst[n][\"nrmse\"][\"exponential\"] = new_\n",
    "    #new.append(new_)\n",
    "    #old.append(old_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse_lst = []\n",
    "for i, exp_ in enumerate(publish_sightIp.experiment_lst[1:26]):\n",
    "    nrmse_lst.append(exp_[\"nrmse\"])\n",
    "hi = pd.DataFrame(nrmse_lst)\n",
    "hi = hi.drop(columns = [\"ip: linear\", \"exponential\"])\n",
    "hi = hi.melt()\n",
    "\n",
    "hi.columns = [\"model\", \"nrmse\"]\n",
    "sns.violinplot(x = \"model\", y = \"nrmse\" , data = hi)\n",
    "print(np.mean(hi.nrmse[hi.model == \"exponential\"]))\n",
    "print(np.mean(hi.nrmse[hi.model == \"uniform\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.experiment_lst = publish_sightIp.experiment_lst[:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_best_args = publish_sightIp.experiment_lst[n][\"best arguments\"][\"exponential\"]\n",
    "\n",
    "#test_esn.get_observers(publish_sightIp.experiment_lst[1][\"get observer inputs\"])\n",
    "test_esn = EchoStateNetwork(**test_best_args, exponential = True)\n",
    "test_esn.train(x = exper_.Train, y = exper_.xTr) #self.Train, y = self.xTr\n",
    "pred_ = test_esn.predict(exper_.xTe.shape[0], exper_.xTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.make_R_barplots() #TODO fix compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(publish_sightIp.experiment_lst[i][\"prediction\"][\"ip: linear\"].shape)\n",
    "    #sub_df_nrows = blindIP_loss_df[blindIP_loss_df[\"experiment #\"] == i].shape[0]\n",
    "    #print(sub_df_nrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average R across frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle_A.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "blindIP_loss_df_50_50_split = df[df == 0.5]\n",
    "#blindIP_loss_df_50_50_split = df[df.model != \"exponential\"]\n",
    "\n",
    "mean_ =  blindIP_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = blindIP_loss_df_50_50_split[\"R\"], data = blindIP_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = blindIP_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blindIP_loss_df = publish_sightIp.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "blindIP_loss_df_50_50_split = publish_sightIp.rDF[publish_sightIp.rDF.split == 0.5]\n",
    "blindIP_loss_df_50_50_split = blindIP_loss_df_50_50_split[blindIP_loss_df_50_50_split.model != \"uniform\"]\n",
    "\n",
    "mean_ =  blindIP_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = blindIP_loss_df_50_50_split[\"R\"], data = blindIP_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = blindIP_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blindIP_loss_df = publish_sightIp.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "blindIP_loss_df_50_50_split = publish_sightIp.rDF[publish_sightIp.rDF.split == 0.5]\n",
    "blindIP_loss_df_50_50_split = blindIP_loss_df_50_50_split[blindIP_loss_df_50_50_split.model != \"ip: linear\"]\n",
    "\n",
    "mean_ =  blindIP_loss_df_50_50_split.R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = blindIP_loss_df_50_50_split[\"R\"], data = blindIP_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = blindIP_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log R. These plots are bad because they don't care about the average loss per frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blindIP_loss_df = publish_sightIp.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "blindIP_loss_df_50_50_split = publish_sightIp.rDF[publish_sightIp.rDF.split == 0.5]\n",
    "blindIP_loss_df_50_50_split = blindIP_loss_df_50_50_split[blindIP_loss_df_50_50_split.model != \"exponential\"]\n",
    "\n",
    "blindIP_loss_df_50_50_split[\"log_R\"] = np.log(blindIP_loss_df_50_50_split.R)\n",
    "\n",
    "mean_ =  blindIP_loss_df_50_50_split.log_R.rolling(50).mean()\n",
    "colors = [\"cyan\", \"red\"]\n",
    "\n",
    "sns.scatterplot(x = \"time\", y = np.log(blindIP_loss_df_50_50_split[\"R\"]), data = blindIP_loss_df_50_50_split, \n",
    "                hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = mean_, data = blindIP_loss_df_50_50_split, \n",
    "             hue = \"model\", alpha = 0.9)\n",
    "plt.ylim(-10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blindIP_loss_df = publish_sightIp.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "blindIP_loss_df_50_50_split = publish_sightIp.rDF[publish_sightIp.rDF.split == 0.5]\n",
    "blindIP_loss_df_50_50_split = blindIP_loss_df_50_50_split[blindIP_loss_df_50_50_split.model != \"exponential\"]\n",
    "#blindIP_loss_df_50_50_split[blindIP_loss_df_50_50_split.model != \"exponential\"] = blindIP_loss_df_50_50_split\n",
    "sns.scatterplot(x = \"time\", y = \"R\", data = blindIP_loss_df_50_50_split, hue = \"model\", alpha = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "#mean_ =  blindIP_loss_df_50_50_split.R.rolling(50).mean()\n",
    "#std_ = blindIP_loss_df_50_50_split.R.rolling(50).std()\n",
    "#ub = mean_ - std_\n",
    "#lb = mean_ + std_\n",
    "\n",
    "sns.lineplot(x = \"time\", y = blindIP_loss_df_50_50_split.R.rolling(50).mean(),\n",
    "             data = blindIP_loss_df_50_50_split, hue = \"model\", alpha = 0.9)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = blindIP_loss_df_50_50_split.R.rolling(50).quantile(.95),\n",
    "             data = blindIP_loss_df_50_50_split, hue = \"model\", alpha = 0.4)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = blindIP_loss_df_50_50_split.R.rolling(50).quantile(0.05), data = blindIP_loss_df_50_50_split, hue = \"model\", alpha = 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.kde_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#publish_sightIp.build_loss_df(models = [\"uniform\", \"ip: linear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.experiment_lst[1].keys()\n",
    "\n",
    "hi = publish_sightIp.get_experiment(publish_sightIp.experiment_lst[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(15):\n",
    "    \n",
    "    #print(publish_sightIp.experiment_lst[n][\"nrmse\"])\n",
    "    hi = publish_sightIp.experiment_lst[n][\"get_observer_inputs\"][\"split\"]\n",
    "    if hi == 0.5:\n",
    "        nrmse_dict = publish_sightIp.experiment_lst[n][\"nrmse\"]\n",
    "        for key in nrmse_dict.keys():\n",
    "            nrmse_dict[key] = np.round(nrmse_dict[key], 3)\n",
    "        \n",
    "        print(n)\n",
    "        print(nrmse_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_div(exper_number, col_wise = False, plot = True, col_wise_method = \"freq\"):\n",
    "    \"\"\" Calculates KL Divergence #https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810\n",
    "    Assuming experiment_lst[0]\n",
    "    \"\"\"\n",
    "    xTe = np.array(publish_sightIp.experiment_lst[exper_number][\"xTe\"])\n",
    "    pred_test = publish_sightIp.experiment_lst[exper_number][\"prediction\"]\n",
    "   \n",
    "    #print(publish_sightIp.experiment_lst[0][\"nrmse\"])\n",
    "    def get_empirical_pdf_data(obj, plot = plot):\n",
    "        obj = obj.flatten()\n",
    "        nparam_density = stats.kde.gaussian_kde(obj)\n",
    "        x = np.linspace(-4, 3, 200)\n",
    "        nparam_density = nparam_density(x)\n",
    "        #ax.plot(x, nparam_density, 'r-', label='non-parametric density (smoothed by Gaussian kernel)')\n",
    "        if plot:\n",
    "            plt.hist(np.array(pred_test[\"uniform\"]).ravel(),  normed=True)\n",
    "            plt.plot(x, nparam_density, 'k--', label='non-parametric density')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        return(nparam_density)\n",
    "\n",
    "    def kl_divergence(p, q):\n",
    "        return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "    if not col_wise: #col_number is related to frequency.\n",
    "        uniform_rc_epdf = get_empirical_pdf_data(obj = np.array(pred_test[\"uniform\"]))\n",
    "        linear_ip_epdf = get_empirical_pdf_data(obj = pred_test[\"ip: linear\"])\n",
    "        ground_truth_epdf = get_empirical_pdf_data(obj = xTe)\n",
    "        \n",
    "        kl_divergence_dict = {}\n",
    "        for key in pred_test.keys():\n",
    "            epdf_spec = get_empirical_pdf_data(np.array(pred_test[key]))\n",
    "            kl_divergence_dict[key] = kl_divergence(epdf_spec, ground_truth_epdf)\n",
    "        return(kl_divergence_dict)\n",
    "    else:\n",
    "        which_axis = 1 if col_wise_method == \"freq\" else 0\n",
    "        \n",
    "        kl_divergence_dict = {}\n",
    "        for key in pred_test.keys():\n",
    "            kl_divs_spec = []\n",
    "            for i in range(xTe.shape[which_axis]):\n",
    "                if col_wise_method == \"freq\":\n",
    "                    epdf_spec_i = get_empirical_pdf_data(np.array(pred_test[key])[:, i])\n",
    "                    ground_truth_epdf_i = get_empirical_pdf_data(obj = xTe[:, i])\n",
    "                else:\n",
    "                    epdf_spec_i = get_empirical_pdf_data(np.array(pred_test[key])[i, :])\n",
    "                    ground_truth_epdf_i = get_empirical_pdf_data(obj = xTe[i, :])\n",
    "                kl_spec_i = kl_divergence(epdf_spec_i, ground_truth_epdf_i)\n",
    "                kl_divs_spec.append(kl_spec_i)\n",
    "            kl_divergence_dict[key]=kl_divs_spec\n",
    "            \n",
    "        kl_divergence_df = pd.DataFrame(kl_divergence_dict)\n",
    "        kl_divergence_df = kl_divergence_df.drop(columns = 'ip: linear')\n",
    "        \n",
    "        #rolling\n",
    "        if col_wise_method == \"time\":\n",
    "            print(\"rolling\")\n",
    "            for col in list(kl_divergence_df.columns):\n",
    "                print(col)\n",
    "                print(kl_divergence_df[col])\n",
    "                mean_ = kl_divergence_df[col].rolling(5).median()\n",
    "                kl_divergence_df[col] = mean_\n",
    "        \n",
    "        len_df = len(kl_divergence_df)\n",
    "        kl_divergence_df = kl_divergence_df.melt()\n",
    "        kl_divergence_df.columns = [\"model\", \"kl divergence\"]\n",
    "        kl_divergence_df[\"freq_idx\"] = list(range(len_df))*len(kl_divergence_df.model.unique())\n",
    "        sns.lineplot(x = \"freq_idx\", y = \"kl divergence\", data = kl_divergence_df, hue = \"model\")\n",
    "        plt.show()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    kl_spec = get_kl_div(i, col_wise = True, plot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broad kl-divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_lst= []\n",
    "for i in trange(25):\n",
    "    kl_spec = get_kl_div(i, plot = False)\n",
    "    print(kl_spec)\n",
    "    kl_div_lst.append(kl_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_pd = pd.DataFrame(kl_div_lst)\n",
    "kl_div_pd = kl_div_pd.drop(columns = [\"best interpolation\", \"zhizhuo\"])\n",
    "kl_div_pd = kl_div_pd.melt()\n",
    "kl_div_pd.columns = [\"model\", \"kl divergence\"]\n",
    "sns.swarmplot(x = \"model\", y = \"kl divergence\", data = kl_div_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hi.f))\n",
    "\n",
    "\n",
    "new_T = np.arange(min(hi.T),max(hi.T), step = 1/2751.5)\n",
    "assert len(new_T) == len(hi.f)\n",
    "T_dict = {\"T\": new_T}\n",
    "savemat(\"new_T.mat\", T_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sns.barplot(data = pd.DataFrame(exp0[\"nrmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette(\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = sns.color_palette().as_hex()\n",
    "list(np.array(o)[[2,3,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs4 = publish_sightIp.experiment_lst[0]\n",
    "obs4_dictt = {\"experiment\": obs4}\n",
    "\n",
    "with open('obs4.pickle', 'wb') as handle:\n",
    "    pickle.dump(obs4_dictt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pretty_pics(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array(publish_sightIp.experiment_lst[0][\"prediction\"][\"uniform\"])\n",
    "arr1.shape\n",
    "arr2 = np.array(publish_sightIp.experiment_lst[4][\"prediction\"][\"uniform\"])\n",
    "\n",
    "rc1dict = {\"rc_pred\" : arr1,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[0][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[0][\"xTe\"]) }\n",
    "rc2dict = {\"rc_pred\" : arr2,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[4][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[4][\"xTe\"]) }\n",
    "savemat(\"rc1.mat\", rc1dict)\n",
    "savemat(\"rc2.mat\", rc2dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zhizhuo_freqs(nn = 1):\n",
    "    for n in range(nn):\n",
    "    \n",
    "        #print(publish_sightIp.experiment_lst[n][\"nrmse\"])\n",
    "        split_ = publish_sightIp.experiment_lst[n][\"get_observer_inputs\"][\"split\"]\n",
    "        if split_ == 0.5:\n",
    "            nrmse_dict = publish_sightIp.experiment_lst[n][\"nrmse\"]\n",
    "            for key in nrmse_dict.keys():\n",
    "                nrmse_dict[key] = np.round(nrmse_dict[key], 3)\n",
    "\n",
    "            print(n)\n",
    "            print(nrmse_dict)\n",
    "\n",
    "\n",
    "            resp_idx_spec = publish_sightIp.experiment_lst[n][\"resp_idx\"]\n",
    "            xTe = publish_sightIp.experiment_lst[n][\"xTe\"]\n",
    "            missing_frequencies = [np.round(hi.f[idx],1) for idx in resp_idx_spec]\n",
    "            print(missing_frequencies[0])\n",
    "            print(missing_frequencies[-1])\n",
    "\n",
    "            matlab_resp_idxs = [idx + 1 for idx in resp_idx_spec]\n",
    "            \n",
    "            print(\"response indices: \" + str(matlab_resp_idxs[0]) + \" \" + str(matlab_resp_idxs[-1]))\n",
    "            print(\"number of f missing lines: \" + str(len(matlab_resp_idxs)))\n",
    "\n",
    "            print(\"T's: (\" + str(hi.xTe.shape[0]) + \", \" + str(hi.A.shape[0]) + \")\")\n",
    "\n",
    "            obs_idx_spec = publish_sightIp.experiment_lst[n][\"obs_idx\"] \n",
    "            matlab_obs_idxs = [idx + 1 for idx in obs_idx_spec]\n",
    "            print(\"total observers: \" + str(len(matlab_obs_idxs)))\n",
    "    \n",
    "    #return(matlab_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_zhizhuo_freqs()\n",
    "get_zhizhuo_freqs(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array(publish_sightIp.experiment_lst[0][\"prediction\"][\"uniform\"])\n",
    "arr1.shape\n",
    "arr2 = np.array(publish_sightIp.experiment_lst[13][\"prediction\"][\"uniform\"])\n",
    "arr2.shape\n",
    "publish_sightIp.experiment_lst[13][\"nrmse\"]\n",
    "\n",
    "rc1dict = {\"rc_pred\" : arr1,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[0][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[0][\"xTe\"]) }\n",
    "rc2dict = {\"rc_pred\" : arr2,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[13][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[13][\"xTe\"]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "arr1 = np.array(publish_sightIp.experiment_lst[0][\"prediction\"][\"uniform\"])\n",
    "arr1.shape\n",
    "arr2 = np.array(publish_sightIp.experiment_lst[13][\"prediction\"][\"uniform\"])\n",
    "arr2.shape\n",
    "publish_sightIp.experiment_lst[13][\"nrmse\"]\n",
    "\n",
    "rc1dict = {\"rc_pred\" : arr1,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[0][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[0][\"xTe\"]) }\n",
    "rc2dict = {\"rc_pred\" : arr2,\n",
    "           \"interpolation\" : publish_sightIp.experiment_lst[13][\"prediction\"][\"ip: linear\"],\n",
    "           \"ground_truth\" : np.array(publish_sightIp.experiment_lst[13][\"xTe\"]) }\n",
    "savemat(\"rc1.mat\", rc1dict)\n",
    "savemat(\"rc2.mat\", rc2dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(publish_sightIp.experiment_lst[0][\"xTe\"]), aspect = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.hyper_parameter_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#publish_sightIp.loss_plot(split = 0.5, rolling = 40)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = blindIP_loss_df_50_50_split.R.rolling(50).mean(),\n",
    "             data = blindIP_loss_df_50_50_split, hue = \"model\", alpha = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blindIP_loss_df = publish_sightIp.rDF\n",
    "plt.figure(figsize = (16,8))\n",
    "blindIP_loss_df_90_10_split = publish_sightIp.rDF[publish_sightIp.rDF.split == 0.9]\n",
    "#blindIP_loss_df_50_50_split[blindIP_loss_df_50_50_split.model != \"exponential\"] = blindIP_loss_df_50_50_split\n",
    "sns.scatterplot(x = \"time\", y = \"R\", data = blindIP_loss_df_90_10_split, hue = \"model\", alpha = 0.02)\n",
    "\n",
    "sns.lineplot(x = \"time\", y = blindIP_loss_df_90_10_split.R.rolling(50).mean(),\n",
    "             data = blindIP_loss_df_90_10_split, hue = \"model\", alpha = 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium\n",
    "#### Blind Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_blindIp = EchoStateAnalysis(medium_path_list, ip_use_observers = False)\n",
    "#medium_blindIp.make_R_barplots() \n",
    "medium_blindIp.build_loss_df()\n",
    "medium_blindIp.loss_plot(rolling = 7, split = 0.5)\n",
    "medium_blindIp.loss_plot(rolling = 7, split = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_blindIp.hyper_parameter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "medium_sightIp = EchoStateAnalysis(medium_path_list, ip_use_observers = True)\n",
    "medium_sightIp.build_loss_df(models = [\"uniform\", \"exponential\", \"ip: linear\"])\n",
    "medium_sightIp.loss_plot(rolling = 7, split = 0.5)\n",
    "medium_sightIp.loss_plot(rolling = 7, split = 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish Size Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_blindIp = EchoStateAnalysis(publish_path_list, ip_use_observers = False)\n",
    "publish_blindIp.hyper_parameter_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_blindIp.make_R_barplots() \n",
    "publish_blindIp.build_loss_df()\n",
    "publish_blindIp.loss_plot(rolling = 7, split = 0.5, loss = \"R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_blindIp.loss_plot(rolling = 7, split = 0.9, loss = \"R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### publish sight ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.loss_plot(rolling = 7, split = 0.9, loss = \"R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"PyFiles/analysis.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_sightIp.df.info() #is there  something I'm not seeing? What will I have to present tomorrow? What can I deliver?\n",
    "np.unique(publish_sightIp.df[\"target hz\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhizhuo results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'PyFiles/imports.py'\n",
    "%run -i 'PyFiles/helpers.py'\n",
    "%run -i \"PyFiles/experiment.py\"\n",
    "%run -i \"PyFiles/analysis.py\"\n",
    "files2import = glob.glob('/Users/hayden/Desktop/zhizhuo_block_results/*.mat')\n",
    "files2import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lst = []\n",
    "for i in files2import:\n",
    "    data_lst.append(loadmat(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real51, y_hat101, y_real101, y_hat51 = data_lst\n",
    "truth_51 = y_real51[list(y_real51.keys())[3]]\n",
    "pred_51  = y_hat51[list(y_hat51.keys())[3]]\n",
    "print(nrmse(pred_51, truth_51))\n",
    "plt.imshow(truth_51, aspect = 4)\n",
    "plt.show()\n",
    "plt.imshow(pred_51, aspect = 4)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_freqs_51 = list(range(1740, 2240 + 10, 10)) \n",
    "obs_freqs51a = list(range(1740  - (26*10), 1740, 10))\n",
    "obs_freqs51b = list(range(2240 + 10, 2240 + (26*10) + 10, 10))\n",
    "assert targ_freqs_51[-1] + 10 == obs_freqs51b[0]\n",
    "assert targ_freqs_51[0] - 10 == obs_freqs51a[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "truth_101 = y_real101[list(y_real101.keys())[3]]\n",
    "pred_101  = y_hat101[list(y_hat101.keys())[3]]\n",
    "print(nrmse(pred_101, truth_101))\n",
    "plt.imshow(truth_101, aspect = 4)\n",
    "plt.show()\n",
    "plt.imshow(pred_101, aspect = 4)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "zhizhuo51 = EchoStateExperiment(\"medium\", obs_freqs = obs_freqs51a + obs_freqs51b, target_freqs = targ_freqs_51 )\n",
    "zhizhuo51.f[zhizhuo51.resp_idx[0]]\n",
    "#print(zhizhuo51.A.shape)\n",
    "#for i in zhizhuo51.A.shape[1]\n",
    "hi = \"\"\"\n",
    "for i in range(0,1000):\n",
    "    his = truth_51.T[0:]\n",
    "    mine = zhizhuo51.A[:, i ] #511:\n",
    "    assert his.shape == mine.shape, str(his.shape) + str(mine.shape)\n",
    "    if np.array_equal(his,mine):\n",
    "        print(i)\n",
    "\"\"\"\n",
    "\n",
    "def retrieve_zhizhuo_series(series_idx):\n",
    "    mine = zhizhuo51.A[ 511:, 153 ]\n",
    "    his = truth_51[series_idx,:]\n",
    "    nrmse_lst = []\n",
    "    for i in range(0, 1000):\n",
    "        mine = zhizhuo51.A.T[ i , 511: ]\n",
    "        nrmse_spec = nrmse(his,mine)\n",
    "\n",
    "        nrmse_lst.append(nrmse_spec)\n",
    "    nrmse_series = pd.Series(nrmse_lst)\n",
    "    candidate_idx = nrmse_series.idxmin()\n",
    "    candidate = zhizhuo51.A.T[candidate_idx , 511: ]\n",
    "    plt.plot(candidate, \"--\", linewidth = 5, alpha = 0.6, label = \"actual data\") \n",
    "    plt.plot(his, \":r\", label = \"zhizhuo testset\", linewidth = 3, alpha = 0.5)\n",
    "    titl = str(candidate_idx) + \" idx: \" +  str(zhizhuo51.f[candidate_idx]) + \" Hz\"\n",
    "    plt.title(titl)\n",
    "    plt.legend()\n",
    "    print(truth_51.shape)\n",
    "retrieve_zhizhuo_series(0)\n",
    "plt.show()\n",
    "retrieve_zhizhuo_series(-1)\n",
    "plt.show()\n",
    "plt.plot(zhizhuo51.A[511:, 249])\n",
    "\n",
    "#plt.plot(zhizhuo51.A[174, :])\n",
    "#zhizhuo51.get_observers(split = 0.4995, method = \"exact\", plot_split = True)\n",
    "#zhizhuo51.runInterpolation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert zhizhuo51.xTe.T.shape == truth_51.shape, str(zhizhuo51.xTe.T.shape) + \" != \" + str(truth_51.shape)\n",
    "sns.heatmap(truth_51)\n",
    "plt.show()\n",
    "sns.heatmap(zhizhuo51.xTe.T)\n",
    "plt.show() \n",
    "plt.imshow(pred_51, aspect = 4)\n",
    "plt.show() \n",
    "\n",
    "plt.imshow(zhizhuo51.ip_res[\"prediction\"].T, aspect = 4)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(zhizhuo51.xTe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Zhizhuo block results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Step 2: store hyper-parameter-results: Let's get some nice hyper-parameter plots.\n",
    "#TODO: Step 1: check if observers are correct:\n",
    "#TODO: fix\n",
    "\n",
    "\n",
    "def check_shape_obs(file = \"default\"):\n",
    "    \"\"\"\n",
    "    Check the shape\n",
    "    \"\"\"\n",
    "    if file == \"default\":\n",
    "        nf = get_new_filename(exp = exp, current = True)\n",
    "    else:\n",
    "        nf = file\n",
    "    with open(nf) as json_file: # 'non_exp_w.txt'\n",
    "        datt = json.load(json_file)\n",
    "    #datt = non_exp_best_args[\"dat\"]\n",
    "    #datt[\"obs_tr\"], datt[\"obs_te\"]   = np.array(datt[\"obs_tr\"]), np.array(datt[\"obs_te\"])\n",
    "    #datt[\"resp_tr\"], datt[\"resp_te\"] = np.array(datt[\"resp_tr\"]), np.array(datt[\"resp_te\"])\n",
    "    return(datt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#experiment.save_json(exp = False)\n",
    "#fp = bp + 'targetKhz:_0.01__obskHz:_0.01.txt'\n",
    "#fp = bp + 'targetKhz:_0.02__obskHz:_0.01.txt'\n",
    "def topline(spec_path, \n",
    "            base_path = \"/Users/hayden/Desktop/experiment_results/2k/medium/\",\n",
    "            #base_path = #\"./experiment_results/...\"\n",
    "            verbose = False,\n",
    "            print_filestructure = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(base_path)\n",
    "    fp = base_path + spec_path\n",
    "    \n",
    "    hi = load_data(file = fp)\n",
    "    if print_filestructure == True:\n",
    "        for i in hi.keys():\n",
    "            print(i + \"/\")\n",
    "\n",
    "            if type(hi[i]) == dict:\n",
    "\n",
    "                for j in hi[i].keys():\n",
    "                    print(\"    \" + j)\n",
    "                    \n",
    "    if verbose == True:\n",
    "        print(\"DATA STRUCTURE: (it's a dict)\")\n",
    "        print(\"/n inputs:\")\n",
    "        print(hi[\"experiment_inputs\"])\n",
    "        print(hi[\"get_observer_inputs\"])\n",
    "\n",
    "        print(\"/n key saved values:\")\n",
    "        print(hi[\"best arguments\"])\n",
    "        print(hi[\"nrmse\"])\n",
    "    return(hi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifdel(dictt, key):\n",
    "    \"\"\" If a key is in a dictionary delete it. Return [modified] dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del dictt[key]\n",
    "        return(dictt)\n",
    "    except:\n",
    "        return(dictt)\n",
    "ifdel({\"a\":1, \"b\" : 2}, \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lst = []\n",
    "for exper in experiment_lst:\n",
    "    print()\n",
    "    pd_lst.append(exper[\"nrmse\"]) #, index = [0]))\n",
    "hi = pd.melt(pd.DataFrame(pd_lst))\n",
    "hi.columns = [\"model\" ,\"nrmse\"]\n",
    "hi = hi.iloc[:11,:]\n",
    "sns.catplot(x = \"model\", y = \"nrmse\", data = hi, kind = \"bar\")\n",
    "plt.title(\"R for block prediction: large dataset\")\n",
    "plt.ylabel(\"R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_lst_unq = check_for_duplicates(path_lst, verbose = False)\n",
    "dict_lst_unq = check_for_duplicates(dict_lst, verbose = False)\n",
    "\n",
    "complete_experiment_path_lst = path_lst_unq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete_experiment_path_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(check_for_duplicates(complete_experiment_path_lst) != True), \"duplicates found\"\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "bp_ = \"\"# \"./experiment_results/\"\n",
    "\n",
    "\n",
    "\n",
    "# fix nrmse calculation AND interpolation\n",
    "for i in trange(len(experiment_lst), desc='experiment list, fixing interpolation...'): \n",
    "    exper_ = experiment_lst[i]\n",
    "    exper_obj = get_experiment(exper_)\n",
    "    \n",
    "    train_set, test_set = exper_obj.xTr, exper_obj.xTe\n",
    "    models_spec = list(exper_[\"prediction\"].keys())\n",
    "    \n",
    "    for model_ in models_spec:\n",
    "        pred_ = exper_[\"prediction\"][model_]\n",
    "        corrected_nrmse = nrmse(pred_, test_set)\n",
    "        exper_[\"nrmse\"][model_] = corrected_nrmse\n",
    "        \n",
    "    experiment_lst[i] = fix_interpolation(exper_, method = \"linear\") \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_combination(ip, exp, Test, n = 10, optimize = True):\n",
    "    \n",
    "    if optimize == True:\n",
    "        nrmses = []\n",
    "        predictions = []\n",
    "        #(np.array(Test) + np.array(predictions[\"exponential\"])) / 2\n",
    "        vals =[]\n",
    "        for i in range(n):\n",
    "            a = i / n\n",
    "            b = 1 - a\n",
    "            hybrid_pred = ((1-a) * ip + a * exp)\n",
    "            predictions += [hybrid_pred]\n",
    "            nrmses += [nrmse(hybrid_pred , Test) ]\n",
    "            vals += [a]\n",
    "            #nrmse(predictions[\"hybrid\"], Test) \n",
    "        idx = np.argmin(nrmses) \n",
    "        print(nrmses)\n",
    "        print(\"A!! \" + str(vals[idx]))\n",
    "        best_prediction = predictions[idx]\n",
    "        best_nrmse      = nrmses[idx]\n",
    "        return(best_prediction, best_nrmse)\n",
    "    else:\n",
    "        hybrid_pred  = (0.5 * ip) + (0.5 * exp)\n",
    "        hybrid_nrmse = nrmse(hybrid_pred , Test)\n",
    "        return(hybrid_pred, hybrid_nrmse)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add hybrids\n",
    "for i in list(range(len(experiment_lst))):\n",
    "    experiment_lst[i]\n",
    "    predictions_= experiment_lst[i][\"prediction\"]\n",
    "    Train, Test = recover_test_set(experiment_lst[i])\n",
    "    #hybrid_pred_, hybrid_R = optimize_combination(np.array(predictions_[\"interpolation\"]),\n",
    "    #                                                                  np.array(predictions_[\"exponential\"]),\n",
    "    #                                                                  Test, optimize = False)\n",
    "    experiment_lst[i][\"nrmse\"][\"hybrid\"]      = hybrid_R\n",
    "    experiment_lst[i][\"prediction\"][\"hybrid\"] = hybrid_pred_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_lst[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df():\n",
    "    IGNORE_IP = False\n",
    "\n",
    "    def quick_dirty_convert(lst):\n",
    "        if IGNORE_IP == True:\n",
    "            lst *= 2\n",
    "        else:\n",
    "            lst *= 4\n",
    "        pd_ = pd.DataFrame(np.array(lst).reshape(-1,1))\n",
    "        return(pd_)\n",
    "\n",
    "\n",
    "    idx_lst = list(range(len(experiment_lst)))\n",
    "    #idx_lst *= 3\n",
    "    #idx_lst = pd.DataFrame(np.array(idx_lst).reshape(-1,1))\n",
    "\n",
    "    idx_lst = quick_dirty_convert(idx_lst)\n",
    "\n",
    "    obs_hz_lst, targ_hz_lst, targ_freq_lst = [], [], []\n",
    "\n",
    "    for i, experiment in enumerate(experiment_lst):\n",
    "        #print(experiment['experiment_inputs'].keys())\n",
    "        targ_hz = experiment[\"experiment_inputs\"][\"target_hz\"]\n",
    "        obs_hz  = experiment[\"experiment_inputs\"][\"obs_hz\"]\n",
    "        targ_freq = experiment[\"experiment_inputs\"]['target_frequency']\n",
    "\n",
    "        if experiment[\"experiment_inputs\"][\"target_hz\"] < 1:\n",
    "            targ_hz *= 1000*1000\n",
    "            obs_hz  *= 1000*1000\n",
    "        obs_hz_lst  += [obs_hz]\n",
    "        targ_hz_lst += [targ_hz]\n",
    "        targ_freq_lst += [targ_freq]\n",
    "\n",
    "\n",
    "        hz_line = {\"target hz\" : targ_hz }\n",
    "        hz_line = Merge(hz_line , {\"obs hz\" : obs_hz })\n",
    "\n",
    "        #print(hz_line)\n",
    "        df_spec= experiment[\"nrmse\"]\n",
    "\n",
    "        #df_spec = Merge(experiment[\"nrmse\"], {\"target hz\": targ_hz})\n",
    "        df_spec = pd.DataFrame(df_spec, index = [0])\n",
    "\n",
    "        df_spec_rel = df_spec.copy()\n",
    "        #/df_spec_diff[\"uniform\"]\n",
    "        #df_spec_diff[\"rc_diff\"]\n",
    "\n",
    "        if IGNORE_IP == True:\n",
    "            df_spec_rel = df_spec_rel / experiment[\"nrmse\"][\"uniform\"]#\n",
    "        else:\n",
    "            df_spec_rel = df_spec_rel / experiment[\"nrmse\"][\"interpolation\"]\n",
    "\n",
    "\n",
    "\n",
    "        #print( df_spec_rel)\n",
    "        #print(experiment[\"experiment_inputs\"].keys())\n",
    "        if i == 0:\n",
    "            df      = df_spec\n",
    "            df_rel  = df_spec_rel\n",
    "\n",
    "\n",
    "        else:\n",
    "            df = pd.concat([df, df_spec])\n",
    "            df_rel = pd.concat([df_rel, df_spec_rel])\n",
    "\n",
    "\n",
    "    df_net = df_rel.copy()\n",
    "\n",
    "    obs_hz_lst, targ_hz_lst = quick_dirty_convert(obs_hz_lst), quick_dirty_convert(targ_hz_lst)\n",
    "    targ_freq_lst = quick_dirty_convert(targ_freq_lst)\n",
    "    #display(df)\n",
    "    if IGNORE_IP == True:\n",
    "        df_rel = df_rel.drop(columns = [\"interpolation\"])\n",
    "        df  = df.drop(columns = [\"interpolation\"])\n",
    "    #df_rel  = df_rel.drop(columns = [\"hybrid\"])\n",
    "    #df      = df.drop(    columns = [\"hybrid\"])\n",
    "\n",
    "    df, df_rel = pd.melt(df), pd.melt(df_rel)\n",
    "    df  = pd.concat( [idx_lst, df,  obs_hz_lst, targ_hz_lst, targ_freq_lst] ,axis = 1)\n",
    "\n",
    "    df_rel = pd.concat( [idx_lst, df_rel,  obs_hz_lst, targ_hz_lst, targ_freq_lst], axis = 1)\n",
    "\n",
    "    #df_diff = pd.concat( [idx_lst, df_diff,  obs_hz_lst, targ_hz_lst, targ_freq_lst], axis = 1)\n",
    "\n",
    "    col_names = [\"experiment\", \"model\", \"nrmse\", \"obs hz\", \"target hz\", \"target freq\" ]\n",
    "    df.columns, df_rel.columns    = col_names, col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df_diff = df[df[\"model\"] == \"uniform\"]\n",
    "df_diff.model = \"diff\"\n",
    "nrmse_ = (df[df[\"model\"] == \"exponential\"][\"nrmse\"].values - df_diff[\"nrmse\"].values) * 100\n",
    "df_diff.nrmse = nrmse_\n",
    "def plot_loss_reduction():\n",
    "\n",
    "    df_diff = df[df[\"model\"] == \"uniform\"]\n",
    "    df_diff.model = \"diff\"\n",
    "    #df_diff[\"nrmse\"] = df_diff[\"nrmse\"] - df[df[\"model\"] == \"exponential\"][\"nrmse\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    #df[df[\"model\"] == \"exponential\"] \n",
    "\n",
    "    nrmse_ = (df[df[\"model\"] == \"exponential\"][\"nrmse\"].values - df_diff[\"nrmse\"].values) * 100\n",
    "    df_diff.nrmse = nrmse_\n",
    "    pct = round(np.mean(nrmse_ < 0) * 100,2)\n",
    "    print(\"odds of loss reduction with exponential weights vs uniform weights: \" + str(pct) + \"%\")\n",
    "    print(\"mean % loss change: \" + str(round(np.mean(nrmse_))) + \"%\")\n",
    "\n",
    "    #sns.catplot(x = \"model\", y = \"nrmse\", data = df_diff)\n",
    "    fig, ax = plt.subplots(1,1,figsize = (10, 6))\n",
    "    plt.xlabel(\"%change in loss\")\n",
    "    plt.ylabel(\"density\")\n",
    "    sns.kdeplot(df_diff[\"nrmse\"], shade = True)\n",
    "    plt.axvline(x=0, color = \"black\", label = \"zero\")\n",
    "    plt.axvline(x=np.mean(df_diff[\"nrmse\"]), color = \"red\", label = \"mean loss reduction\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_loss_reduction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_reduction2d(xx = \"target hz\"):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (6,6))\n",
    "    sns.kdeplot(df_diff[xx], df_diff[\"nrmse\"],\n",
    "                     cmap=\"Blues\", shade=True, shade_lowest=False, ax = ax)#, alpha = 0.5)\n",
    "    #plt.ayvline(y=0, color = \"black\", label = \"zero\")\n",
    "    sns.scatterplot(x = xx, y = \"nrmse\", data = df_diff, ax = ax,  linewidth=0, color = \"black\", alpha = 0.4)\n",
    "    plt.title(\"2d kde plot: nrmse vs target hz\")\n",
    "    plt.axhline(y=0.5, color='black', linestyle='-')\n",
    "    ax.set_ylabel(\"pct loss exp vs unif RC\")\n",
    "plot_loss_reduction2d()\n",
    "plot_loss_reduction2d(xx = \"obs hz\")\n",
    "plot_loss_reduction2d(xx = \"target freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(experiment_lst[0][\"prediction\"][\"uniform\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_lst[0][\"nrmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in experiment_lst:\n",
    "    print(i.keys())\n",
    "    print(i[\"best arguments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args= {\"relative\": False,\n",
    "       \"columnwise\" : True,\n",
    "       \"rolling\" : 9,\n",
    "        \"models\" : [\"uniform\", \"exponential\", \"ip: linear\"]#, \"hybrid\"]\n",
    "}\n",
    "Loss_df = build_loss_df(**args)\n",
    "#L2_loss_df = loss_df(**args, loss_ = \"L2\")\n",
    "#L1_loss_df = loss_df(relative = False, columnwise = False, rolling = 10, loss_ = \"L1\", models = [\"uniform\", \"exponential\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_plot(Loss_df, rolling = 1, split = 0.9, loss = \"R\", include_ip = True)\n",
    "loss_plot(Loss_df, rolling = 30, split = 0.5, loss = \"R\", include_ip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_plot(Loss_df, rolling = 1, split = 0.5, loss = \"R\", include_ip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nrmse_kde_2d(xx = \"target hz\", \n",
    "                      log = True, \n",
    "                      alph = 1, \n",
    "                      black_pnts = True, \n",
    "                      models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"},\n",
    "                      enforce_bounds = False,\n",
    "                      target_freq = None):\n",
    "    \"\"\"\n",
    "    #todo description\n",
    "    \"\"\"\n",
    "    if target_freq != None:\n",
    "        df_spec = df[df[\"target freq\"] == target_freq]\n",
    "    else:\n",
    "        df_spec = df.copy()\n",
    "            \n",
    "    \n",
    "    def plot_(model_, colorr, alph = alph,  black_pnts =  black_pnts):\n",
    "        if colorr == \"Blues\":\n",
    "            color_ = \"blue\"\n",
    "        elif colorr == \"Reds\":\n",
    "            color_ = \"red\"\n",
    "        elif colorr == \"Greens\":\n",
    "            color_ = \"green\"\n",
    "            \n",
    "        df_ = df_spec[df_spec.model == model_] #df_ip  = df[df.model == \"interpolation\"]\n",
    "        \n",
    "        #display(df_)\n",
    "            \n",
    "        \n",
    "        hi = df_[\"nrmse\"]\n",
    "        cap = 1\n",
    "        if log == True:\n",
    "            hi = np.log(hi)/ np.log(10)\n",
    "            cap = np.log(cap) / np.log(10)\n",
    "        \n",
    "        \n",
    "        sns.kdeplot(df_[xx], hi, cmap= colorr, \n",
    "                    shade=True, shade_lowest=False, ax = ax, label = model_, alpha = alph)#, alpha = 0.5)\n",
    "        \n",
    "        if  black_pnts == True:\n",
    "            col_scatter = \"black\"\n",
    "        else:\n",
    "            col_scatter = color_\n",
    "        \n",
    "        sns.scatterplot(x = xx, y = hi, data = df_,  linewidth=0, \n",
    "                        color = col_scatter, alpha = 0.4, ax = ax)\n",
    "        \n",
    "        plt.title(\"2d kde plot: nrmse vs \" + xx)\n",
    "        \n",
    "        plt.axhline(y=cap, color=color_, linestyle='-', label = \"mean \" + str(model_), alpha = 0.5)\n",
    "        sns.lineplot(y = hi, x = xx, data = df_ , color = color_)#, alpha = 0.2)\n",
    "        if enforce_bounds == True:\n",
    "            ax.set_ylim(0,1)\n",
    "        if log == True:\n",
    "            ax.set_ylabel(\"log( NRMSE) \")\n",
    "        else: \n",
    "            ax.set_ylabel(\"NRMSE\")\n",
    "            \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (12,6))\n",
    "    for model in list(models.keys()):\n",
    "        print(model)\n",
    "        plot_(model, models[model], alph = alph)\n",
    "    #plot_(\"interpolation\", \"Blues\")\n",
    "    #plot_(\"exponential\", \"Reds\", alph = alph)\n",
    "    \n",
    "def kde_plots( target_freq = None, \n",
    "               log = False, \n",
    "               model = \"uniform\", \n",
    "               models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"},\n",
    "               enforce_bounds = True,\n",
    "               split = None):\n",
    "    \"\"\"\n",
    "    HEATMAP EXAMPLE:\n",
    "                     enforce_bounds = True)\n",
    "    flights = flights.pivot(\"month\", \"year\", \"passengers\") #y, x, z\n",
    "    ax = sns.heatmap(flights)\n",
    "    plot_nrmse_kde_2d(**additional_arguments, \n",
    "                      models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"})\n",
    "    \n",
    "    plot_nrmse_kde_2d(xx = \"obs hz\", **additional_arguments, \n",
    "                      models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"})\n",
    "    \"\"\"\n",
    "    \n",
    "    additional_arguments ={ \"black_pnts\" : False, \n",
    "                           \"alph\" : 0.3, \n",
    "                           \"target_freq\" : target_freq}    \n",
    "    \n",
    "    cmap = \"coolwarm\"\n",
    "    \n",
    "   \n",
    "    def add_noise(np_array, log = log):\n",
    "        sizee = len(np_array)\n",
    "        x =  np.random.randint(100, size = sizee) + np_array \n",
    "        \n",
    "        return(x)\n",
    "    \n",
    "    nrmse_dict = {}\n",
    "    \n",
    "    for i, model in enumerate([\"uniform\", \"exponential\", \"interpolation\"]):\n",
    "        df_ = df[df.model == model ]\n",
    "        \n",
    "        xx, yy = add_noise(df_[\"target hz\"]), add_noise(df_[\"obs hz\"])\n",
    "\n",
    "        nrmse= df_[\"nrmse\"]\n",
    "        if log == True:\n",
    "            print(\"hawabunga\")\n",
    "            nrmse = np.log(nrmse)\n",
    "        nrmse_dict[model] = nrmse\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    nrmse_diff = nrmse_dict[\"exponential\"].values.reshape(-1,)  - nrmse_dict[\"uniform\"].values.reshape(-1,) \n",
    "    print(\"(+): \" + str(np.sum((nrmse_diff > 0)*1)))\n",
    "    \n",
    "    print(\"(-): \" + str(np.sum((nrmse_diff < 0)*1)))\n",
    "    \n",
    "    \n",
    "    display(nrmse_diff)\n",
    "    xx, yy = add_noise(df_[\"target hz\"]), add_noise(df_[\"obs hz\"])\n",
    "    #sns.distplot(nrmse_diff, ax = ax[2])\n",
    "    sns.scatterplot(x = xx, y = yy, data = df_, ax = ax[2], palette=cmap, alpha = 0.9, s = 50, hue = nrmse_diff) #size = nrmse,\n",
    "    ax[2].set_title(\" diff: exponential - uniform\" )\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_nrmse_kde_2d(**additional_arguments, log = False, \n",
    "                      models = models, #{\"exponential\" : \"Reds\", \"uniform\" : \"Blues\", \"interpolation\" : \"Greens\"},\n",
    "                     enforce_bounds = True)\n",
    "    \n",
    "    \n",
    "    plot_nrmse_kde_2d(xx = \"obs hz\", **additional_arguments, log = False, \n",
    "                       models = models, #{\"exponential\" : \"Reds\", \"uniform\" : \"Blues\", \"interpolation\" : \"Greens\"},\n",
    "                       enforce_bounds = True)\n",
    "    \n",
    "               \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "kde_plots(models = {\"interpolation\" : \"Greens\", \"hybrid\" : \"Reds\"})#, \"uniform\" : \"Blues\"},)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_plots(models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Blues\", \"uniform\" : \"Reds\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: move the legend to the lower right corner\n",
    "kde_plots(target_freq = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exper_ in experiment_lst:\n",
    "    get_experiment(exper_,  compare_ = True, plot_split = False  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_plots(target_freq = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    " \n",
    "def optimize_combination(ip, exp, Test, n = 10, optimize = True):\n",
    "    \n",
    "    if optimize == True:\n",
    "        nrmses = []\n",
    "        predictions = []\n",
    "        #(np.array(Test) + np.array(predictions[\"exponential\"])) / 2\n",
    "        vals =[]\n",
    "        for i in range(n):\n",
    "            a = i / n\n",
    "            b = 1 - a\n",
    "            hybrid_pred = ((1-a) * ip + a * exp)\n",
    "            predictions += [hybrid_pred]\n",
    "            nrmses += [nrmse(hybrid_pred , Test) ]\n",
    "            vals += [a]\n",
    "            #nrmse(predictions[\"hybrid\"], Test) \n",
    "        idx = np.argmin(nrmses) \n",
    "        print(nrmses)\n",
    "        print(\"A!! \" + str(vals[idx]))\n",
    "        best_prediction = predictions[idx]\n",
    "        best_nrmse      = nrmses[idx]\n",
    "        return(best_prediction, best_nrmse)\n",
    "    else:\n",
    "        hybrid_pred  = (0.5 * ip) + (0.5 * exp)\n",
    "        hybrid_nrmse = nrmse(hybrid_pred , Test)\n",
    "        return(hybrid_pred, hybrid_nrmse)\n",
    "    \n",
    "\n",
    "def show_images(exper_, aspect = 10, sigma = 1, method = \"heatmap\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Train, Test = recover_test_set(exper_)\n",
    "    \n",
    "    exper_ = fix_interpolation(exper_)\n",
    "    \n",
    "    predictions = exper_[\"prediction\"]\n",
    "    #print(predictions)\n",
    "    for i in predictions.values():\n",
    "        a = np.array(i)\n",
    "        #print(a.shape)\n",
    "        \n",
    "    nrmses      = exper_[\"nrmse\"]\n",
    "    predictions[\"ground_Truth\"]  = Test\n",
    "    nrmses[\"ground_Truth\"]       = 0\n",
    "    #predictions[\"ground_Truth_smooth\"]  = gaussian_filter(Test, sigma=sigma)\n",
    "    #nrmses[\"ground_Truth_smooth\"]       = 0\n",
    "    \n",
    "    fig, ax = plt.subplots(2,3, figsize = (16, 10))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, (key, value) in enumerate(predictions.items()):\n",
    "        print(i)\n",
    "        print(key)\n",
    "        arr = np.array(value)\n",
    "        full_arr = np.concatenate((Train, arr), axis = 0)\n",
    "        #arr = full_arr\n",
    "        \n",
    "        if method != \"heatmap\":\n",
    "            plt.imshow(arr.T, aspect = aspect)\n",
    "            plt.title(key +\" R: \" + str(nrmses[key]))\n",
    "            plt.subplot(2,3,i)\n",
    "            plt.show()       \n",
    "        else:\n",
    "            sns.heatmap(full_arr.T, ax = ax[i])\n",
    "            ax[i].set_title(key +\" R: \" + str(nrmses[key]))\n",
    "            #plt.show()\n",
    "    #plt.show()\n",
    "show_images(experiment_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[1])\n",
    "ax[0].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "ax[1].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "ax[0].set_ylabel(\"NRMSE\"); ax[1].set_ylabel(\"NRMSE\")\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[1])\n",
    "ax[0].set_title(\"Relative NRMSE vs Interpolation model across different RC's\")\n",
    "ax[1].set_title(\"Relavite NRMSE vs Interpolation model across different RC's\")\n",
    "ax[0].set_ylabel(\"Relative NRMSE\"); ax[1].set_ylabel(\"Relative NRMSE\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (7,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df_diff, ax = ax)\n",
    "#sns.boxplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[1])\n",
    "ax.set_title(\"Relative NRMSE: ([exp nrmse] -  [unif nrmse])/[unif_nrmse] * 100\")\n",
    "ax.set_ylabel(\"Relative NRMSE\"); ax.set_ylabel(\"Relative NRMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = df[\"nrmse\"] #df[\"nrmse\"]np.log(df[\"nrmse\"])\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5, legend = False)\n",
    "sns.lineplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "cap = np.log(1.0)/np.log(10)\n",
    "plt.axhline(y=cap, color=\"black\", linestyle='-', label = \"mean \" + str(model_), alpha = 0.5)\n",
    "plt.ylabel(\"log( NRMSE)\")\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5, legend = False)\n",
    "sns.lineplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "cap = np.log(1.0)/np.log(10)\n",
    "plt.axhline(y=cap, color=\"black\", linestyle='-', label = \"mean \" + str(model_), alpha = 0.5)\n",
    "plt.ylabel(\"log( NRMSE)\")\n",
    "plt.ylim((-1.5,cap))\n",
    "\n",
    "\n",
    "\n",
    "hi = df[\"nrmse\"]\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5, legend = False)\n",
    "sns.lineplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "plt.ylabel(\"NRMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = np.log(df[\"nrmse\"])\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"obs hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "sns.lineplot( y = hi, x = \"obs hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "ax.set_ylabel(\"Log ( NRMSE )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target freq\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "plt.ylim((0,1.5))\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target freq\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_plot(experiment_lst):\n",
    "    \"\"\"\n",
    "    Let's visualize the hyper-parameter plots.\n",
    "    \"\"\"\n",
    "    log_vars = [\"noise\", \"connectivity\", \"regularization\", \"llambda\"]\n",
    "    \n",
    "    for i, experiment in enumerate(experiment_lst):\n",
    "        df_spec_unif = pd.DataFrame(experiment[\"best arguments\"][\"uniform\"], index = [0])\n",
    "        df_spec_exp  = pd.DataFrame(experiment[\"best arguments\"][\"exponential\"], index = [0])\n",
    "        \n",
    "        if not i:\n",
    "            df_unif = df_spec_unif\n",
    "            df_exp = df_spec_exp\n",
    "        else:\n",
    "            df_unif = pd.concat([df_unif, df_spec_unif])\n",
    "            df_exp = pd.concat([df_exp, df_spec_exp])\n",
    "\n",
    "    unif_vars = [\"connectivity\", \"regularization\", \"leaking_rate\", \"spectral_radius\"]\n",
    "    exp_vars  = [\"llambda\", \"noise\"]\n",
    "    df_unif = df_unif[unif_vars]\n",
    "    df_exp = df_exp[unif_vars + exp_vars]\n",
    "    \n",
    "    for i in list(df_unif.columns):\n",
    "        if i in log_vars:\n",
    "            df_unif[i] = np.log(df_unif[i])/np.log(10)\n",
    "            \n",
    "    for i in list(df_exp.columns):\n",
    "        if i in log_vars:\n",
    "            df_exp[i] = np.log(df_exp[i])/np.log(10)\n",
    "    \n",
    "    \n",
    "    #display(df_unif)\n",
    "    \n",
    "    sns.catplot(data = df_unif)\n",
    "    plt.title(\"uniform RC hyper-parameters\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    sns.catplot(data = df_exp)\n",
    "    plt.title(\"exponential RC hyper-parameters\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    #display(df_exp)\n",
    "   \n",
    "hyper_parameter_plot(experiment_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, experiment in enumerate(experiment_lst):\n",
    "    #print(experiment['get_observer_inputs'].keys())\n",
    "    split = experiment['get_observer_inputs'][\"split\"]\n",
    "    targ_hz = experiment['experiment_inputs'][\"target_hz\"]\n",
    "    targ_idx_LB, targ_idx_UB = experiment[\"resp_idx\"][0], experiment[\"resp_idx\"][-1]\n",
    "    obs_hz = experiment['experiment_inputs'][\"obs_hz\"]\n",
    "    f = np.array(experiment_8_obj.f)\n",
    "    obs_idx = experiment[\"obs_idx\"] \n",
    "\n",
    "    obs_idx  = [int(j) for j in experiment[\"obs_idx\"] ]\n",
    "    obs_freq = [max(f) - f[j] for j in obs_idx]\n",
    "    \n",
    "    \n",
    "    print(\"\\nexperiment: \" + str(i) + \", target hz: \" + str(targ_hz) + \", obs hz: \" + str(obs_hz) +\n",
    "         \", split: \" + str(split))\n",
    "\n",
    "    \n",
    "    print(\"target idx: [\" + str(targ_idx_LB) + \", \" + str(targ_idx_UB) + \"]\")\n",
    "    print(\"target freq: [\" + str(max(f) - f[targ_idx_LB]) + \", \" + str(max(f) - f[targ_idx_UB]) + \"]\")\n",
    "    print(\"obs idx: \" + str(obs_idx))\n",
    "    print(\"obs freq: \" + str(obs_freq))\n",
    "    print(experiment_8_obj.A.shape[0] - np.array(experiment[\"prediction\"][\"interpolation\"]).shape[0])\n",
    "    print(experiment_8_obj.A.shape[0])\n",
    "    #print(experiment[\"resp_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_exp_weights(json_obj, llambda = None):\n",
    "    print(json_obj.keys())\n",
    "    esn_ = EchoStateNetwork(**json_obj[\"best arguments\"][\"exponential\"], plot = True)\n",
    "    esn_.obs_idx  = json_obj[\"obs_idx\"]\n",
    "    esn_.resp_idx = json_obj[\"resp_idx\"]\n",
    "    if llambda != None:\n",
    "        esn_.llambda = llambda\n",
    "    esn_.get_exp_weights()\n",
    "\n",
    "\n",
    "for i in experiment_lst:\n",
    "    show_exp_weights(i)  \n",
    "#show_exp_weights(experiment_2)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_exp_weights(i, llambda = 10**-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**-2 \n",
    "np.log(10**-4)/np.log(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment_lst = [experiment_1, experiment_2, experiment_3, experiment_4, \n",
    "                  experiment_5, experiment_6, experiment_7, experiment_8]\n",
    "\n",
    "def quick_dirty_convert(lst):\n",
    "    lst *= 3\n",
    "    print(lst)\n",
    "    pd_ = pd.DataFrame(np.array(lst).reshape(-1,1))\n",
    "    return(pd_)\n",
    "    \n",
    "\n",
    "idx_lst = list(range(len(experiment_lst)))\n",
    "#idx_lst *= 3\n",
    "#idx_lst = pd.DataFrame(np.array(idx_lst).reshape(-1,1))\n",
    "\n",
    "idx_lst = quick_dirty_convert(idx_lst)\n",
    "\n",
    "obs_hz_lst, targ_hz_lst = [], []\n",
    "\n",
    "for i, experiment in enumerate(experiment_lst):\n",
    "    targ_hz = experiment[\"experiment_inputs\"][\"target_hz\"]\n",
    "    obs_hz  = experiment[\"experiment_inputs\"][\"obs_hz\"]\n",
    "    \n",
    "    \n",
    "    if experiment[\"experiment_inputs\"][\"target_hz\"] < 1:\n",
    "        targ_hz *= 1000*1000\n",
    "        obs_hz  *= 1000*1000\n",
    "    obs_hz_lst  += [obs_hz]\n",
    "    targ_hz_lst += [targ_hz]\n",
    "    \n",
    "        \n",
    "    hz_line = {\"target hz\" : targ_hz }\n",
    "    hz_line = Merge(hz_line , {\"obs hz\" : obs_hz })\n",
    "    \n",
    "    #print(hz_line)\n",
    "    df_spec = experiment[\"nrmse\"]\n",
    "    #df_spec = Merge(experiment[\"nrmse\"], {\"target hz\": targ_hz})\n",
    "    df_spec = pd.DataFrame(df_spec, index = [0])\n",
    "    \n",
    "    df_spec_rel = df_spec.copy()\n",
    "    df_spec_rel = df_spec_rel / experiment[\"nrmse\"][\"interpolation\"]\n",
    "    \n",
    "    #print( df_spec_rel)\n",
    "    #print(experiment[\"experiment_inputs\"].keys())\n",
    "    if i == 0:\n",
    "        df = df_spec\n",
    "        df_rel = df_spec_rel\n",
    "        \n",
    "    else:\n",
    "        df = pd.concat([df, df_spec])\n",
    "        df_rel = pd.concat([df_rel, df_spec_rel])\n",
    "\n",
    "#obs_hz_lst  *= 3\n",
    "#targ_hz_lst *= 3\n",
    "\n",
    "obs_hz_lst, targ_hz_lst = quick_dirty_convert(obs_hz_lst), quick_dirty_convert(targ_hz_lst)\n",
    "        \n",
    "df, df_rel = pd.melt(df), pd.melt(df_rel)\n",
    "df  = pd.concat( [idx_lst, df,  obs_hz_lst, targ_hz_lst] ,axis = 1)\n",
    "\n",
    "df_rel = pd.concat( [idx_lst, df_rel,  obs_hz_lst, targ_hz_lst], axis = 1)\n",
    "\n",
    "df.columns     = [\"experiment\", \"model\", \"nrmse\", \"obs hz\", \"target hz\" ]\n",
    "df_rel.columns = [\"experiment\", \"model\", \"nrmse\", \"obs hz\", \"target hz\"] \n",
    "display(df)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[1])\n",
    "ax[0].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "ax[1].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[1])\n",
    "ax[0].set_title(\"Relative NRMSE vs Interpolation model across different RC's\")\n",
    "ax[1].set_title(\"Relavite NRMSE vs Interpolation model across different RC's\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def merge_unif_exp(fp_unif, fp_exp):\n",
    "    exp_dat = load_data(fp_exp)\n",
    "    unif_dat = load_data(fp_unif)\n",
    "    assert exp_dat[\"prediction\"][\"interpolation\"] == unif_dat[\"prediction\"][\"interpolation\"], \"something is wrong!\"\n",
    "    joint_dat = unif_dat.copy()\n",
    "    for i in [\"prediction\", \"nrmse\", \"best arguments\"]:\n",
    "        exp_dict = {\"exponential\" : exp_dat[i][\"exponential\"]}\n",
    "        joint_dat[i] = Merge(joint_dat[i], exp_dict)\n",
    "    print(joint_dat[\"best arguments\"])\n",
    "        \n",
    "     \n",
    "    return(joint_dat)\n",
    "#0.5_1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.1.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.25.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_5 = load_data('/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt')\n",
    "                         #bp = '/Users/hayden/Desktop/')\n",
    "experiment_5_obj = get_experiment(experiment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "uniform_ = df_rel[df_rel.model == \"uniform\"]\n",
    "exp_ = df_rel[df_rel.model == \"exponential\"]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,6))\n",
    "sns.kdeplot(uniform_[\"target hz\"], uniform_[\"nrmse\"],\n",
    "                 cmap=\"Reds\", shade=True, shade_lowest=False, ax = ax[0])#, alpha = 0.5)\n",
    "sns.kdeplot(exp_[\"target hz\"], exp_[\"nrmse\"],\n",
    "                 cmap=\"Blues\", shade=True, shade_lowest=False, ax = ax[1])#, alpha = 0.5)\n",
    "ax[0].set_ylim(0,0.3)\n",
    "ax[1].set_ylim(0,0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOOTH\n",
    "#experiment_5 = load_data('/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/2k/medium/split_0.5/targetKhz:_0.02__obskHz:_0.01.txt')\n",
    "#                         #bp = '/Users/hayden/Desktop/')\n",
    "#experiment_5_obj = get_experiment(experiment_5)\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "experiment_ = experiment_5.copy()\n",
    "for key, prediction in experiment_[\"prediction\"].items():\n",
    "    prediction = np.array(prediction)\n",
    "    Train, Test = recover_test_set(experiment_)\n",
    "    \n",
    "    experiment_[\"nrmse\"][key] = nrmse(prediction, Test)\n",
    "get_experiment(experiment_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOOTH\n",
    "experiment_5 = load_data('/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/2k/medium/split_0.5/targetKhz:_0.02__obskHz:_0.01.txt')\n",
    "                         #bp = '/Users/hayden/Desktop/')\n",
    "experiment_ = experiment_5.copy()\n",
    "for key, prediction in experiment_[\"prediction\"].items():\n",
    "    prediction = np.array(prediction)\n",
    "    Train, Test = recover_test_set(experiment_)\n",
    "    \n",
    "    experiment_[\"nrmse\"][key] = nrmse(prediction, Test)\n",
    "experiment_5_obj = get_experiment(experiment_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liang_idx_convert(250, 259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "def liang_idx_convert(lb, ub):\n",
    "    idx_list = list(range(lb, ub + 1))\n",
    "    return idx_list\n",
    "\n",
    "\n",
    "\n",
    "experiment_ = EchoStateExperiment( size = \"medium\", \n",
    "                                   test_time_idx = liang_idx_convert(250, 259), \n",
    "                                   target_frequency = 2000,\n",
    "                                   train_time_idx = liang_idx_convert(220, 249),\n",
    "                                   prediction_type = \"column\",\n",
    "                                   model = \"uniform\")\n",
    "experiment_.get_observers(method = \"exact\", plot_split = True, aspect = 1)\n",
    "\n",
    "bounds = {\n",
    "          #'noise' : (-2, -4),\n",
    "          'llambda' : (-3, -1), \n",
    "          'connectivity': (-3, 0), # 0.5888436553555889, \n",
    "          'n_nodes': 1000,#(100, 1500),\n",
    "          'spectral_radius': (0.05, 0.99),\n",
    "          'regularization': (-10,-2)}\n",
    "\n",
    "\n",
    "default_presets = {\n",
    "                  'scoring_method' : 'tanh',\n",
    "                  \"cv_samples\" : 5,\n",
    "                  \"max_iterations\" : 4000,\n",
    "                  \"eps\" : 1e-5,\n",
    "                  'subsequence_length' : 250,\n",
    "                  \"initial_samples\" : 100}\n",
    "\n",
    "cv_args = {\n",
    "  'bounds' : bounds,\n",
    "  \"n_jobs\" : 4,\n",
    "  \"verbose\" : True,\n",
    "  \"plot\" : False, \n",
    "  **default_presets\n",
    "}\n",
    "\n",
    "experiment_.RC_CV(cv_args = cv_args, model = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(experiment_.dat[\"resp_tr\"].T, aspect = 0.1)\n",
    "plt.show()\n",
    "plt.imshow(experiment_.dat[\"resp_te\"].T, aspect = 0.1)\n",
    "plt.show()\n",
    "plt.imshow(experiment_.dat[\"obs_tr\"].T, aspect = 10)\n",
    "plt.show()\n",
    "plt.imshow(experiment_.dat[\"obs_te\"].T, aspect = 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwave_data = np.load(\"/Users/hayden/Desktop/GW.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwave_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwave_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3,figsize = (16,8))\n",
    "ax = ax.flatten()\n",
    "n = 5\n",
    "for i, j in enumerate(range(n, n+6)): \n",
    "    ax[i].imshow(gwave_data[j,:,:,1], aspect = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3,figsize = (16,8))\n",
    "ax = ax.flatten()\n",
    "for i, j in enumerate([5, 6, 7, 8, 12,13]): #[5, 6, 7, 8, 12,13]\n",
    "    ax[i].imshow(gwave_data[j,:,:,1], aspect = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/msripooja/steps-to-convert-audio-clip-to-spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import pickle\n",
    "import IPython.display as ipd\n",
    "def Audio(transform):\n",
    "    obj = ipd.Audio(data = transform[\"signal\"], rate = transform[\"sr\"])\n",
    "    return(obj)\n",
    "\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "\n",
    "pth_high = \"/Users/hayden/Desktop/18th_century_high.m4a\"\n",
    "\n",
    "#pth = \"/Users/hayden/Desktop/19th_century_high.wav\"#\"/Users/hayden/Desktop/get_free.wav\"\n",
    "\n",
    "def partition_song(array, x_start, x_len,  y_start, y_stop):\n",
    "    print(array.shape)\n",
    "    x_stop = x_start + 3000\n",
    "    partitioned_array = array[y_start:y_stop, x_start:x_stop].copy()\n",
    "    print(partitioned_array.shape)\n",
    "    return(partitioned_array)\n",
    "\n",
    "def get_transform(trans_type,\n",
    "                  hop_length_mult = 1, pth = \"/Users/hayden/Desktop/18th_century_dense.m4a\",\n",
    "                  n_bins_mult = 2, bins_per_octave =12, sr = None, y_axis = \"hz\", filter_scale = 1,\n",
    "                  norm = 1, method = \"db\", label = None, partition = False, save_path = None\n",
    "                  ):\n",
    "    fmin   = librosa.note_to_hz('C0')\n",
    "    if trans_type == 'cqt':\n",
    "        \n",
    "        y_axis = \"cqt_note\"\n",
    "    \n",
    "    assert label, \"enter a label so that you can save a file\"\n",
    "    assert trans_type in [\"stft\", \"cqt\", \"hybrid_cqt\", \"pseudo_cqt\", \"vqt\"]\n",
    "    \n",
    "    x, sr = librosa.load(pth, sr=None)\n",
    "    #C = np.abs(librosa.cqt(x, sr=None))\n",
    "    \n",
    "    #print(\"transform_type: \" + trans_type)\n",
    "    # Librosa transform functions dictionary:\n",
    "    trans_dict = { \"stft\" : librosa.stft,\n",
    "                   \"cqt\" : librosa.cqt,\n",
    "                   \"hybrid_cqt\": librosa.hybrid_cqt,\n",
    "                   \"pseudo_cqt\": librosa.pseudo_cqt,\n",
    "                   \"vqt\" : librosa.vqt\n",
    "    }\n",
    "    #n_bins = int(84*n_bins_mult)\n",
    "    n_bins = n_bins_mult\n",
    "    default_args = {\n",
    "         \"stft\" : {\n",
    "              \"n_fft\" : 512\n",
    "              #\"sr\" : sr\n",
    "         },\n",
    "         \"cqt\" : {\n",
    "                  \"fmin\" : fmin,\n",
    "                  'pad_mode': 'wrap',\n",
    "                  #\"fmin\": FMIN, , \n",
    "                  \"sr\" : sr,\n",
    "                  #\"n_bins\": 84,\n",
    "                  #\"hop_length\" : 2**6 * hop_length_mult\n",
    "                  #\"n_bins\" : n_bins #\"norm\": norm,\n",
    "                  }, #, \"hop_length\" : 64**hop_length_exp \"fmin\"  #\"filter_scale\": filter_scale : 1 #\"res_type\": \"fft\", #\"sparsity\": 0.1\n",
    "         \"hybrid_cqt\": {},\n",
    "         \"pseudo_cqt\": {},\n",
    "         \"vqt\" : {}\n",
    "        \n",
    "    }\n",
    "    \n",
    "    #x = librosa.resample(x, orig_sr = sr, target_sr = sr*5) #NOPE\n",
    "\n",
    "    N_FFT = len(x)\n",
    "    N_FFT_exp = 4\n",
    "    f = trans_dict[trans_type]\n",
    "    X = f(x, **default_args[trans_type]) #stft #, n_fft =int(N_FFT/np.exp(N_FFT_exp)\n",
    "\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    Xpow = np.log10( librosa.db_to_power(Xdb))\n",
    "    matrix2plot = Xpow if method == \"pow\" else Xdb\n",
    "    \n",
    "    if partition:\n",
    "        matrix2plot = partition_song(matrix2plot, partition, 15000, 0, 800)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(7, 5)) \n",
    "    im1 = librosa.display.specshow(matrix2plot, x_axis='time', y_axis=y_axis, ax = ax)\n",
    "    plt.colorbar(im1, format='%+2.0f dB')\n",
    "    \n",
    "  \n",
    "    #Librosa frequency functions \n",
    "    f_dict = { \"stft\": librosa.fft_frequencies,\n",
    "               \"cqt\" : librosa.cqt_frequencies,\n",
    "               \"hybrid_cqt\": None,\n",
    "               \"pseudo_cqt\": None,\n",
    "               \"vqt\" : None}\n",
    "    \n",
    "    \n",
    "    f_default_args_stft = {\"stft\": {\"sr\" : sr, \"n_fft\" : 512}}            \n",
    "    \n",
    "    try:\n",
    "        f_default_args_cqt = {\"cqt\" : {\"fmin\" : fmin, \"n_bins\" : Xpow.shape[0]}}\n",
    "        f_default_args     = Merge(f_default_args_stft, f_default_args_cqt)\n",
    "        \n",
    "    except NameError:   \n",
    "        f_default_args = f_default_args_stft\n",
    "        \n",
    "    freq_fun = f_dict[trans_type]\n",
    "    freqs    = freq_fun(**f_default_args[trans_type])\n",
    "    \n",
    "    #\"type\": \n",
    "    #consider putting all transform types in this position.\n",
    "    transform = {\"signal\": x,\n",
    "                 \"sr\" : sr,\n",
    "                 \"transform\": {\n",
    "                               \"Xdb\"  : Xdb,\n",
    "                               \"Xpow\" : Xpow,\n",
    "                               \"f\"  :  freqs\n",
    "                               }\n",
    "                }\n",
    "    \n",
    "    #assertion to avoid incorrect frequency length.\n",
    "    err_msg = str(len(freqs.tolist())) + \"   \" + str(Xpow.shape[0])\n",
    "    assert len(freqs) == Xpow.shape[0], err_msg\n",
    "    \n",
    "    if save_path:\n",
    "        save_path = save_path + \"_\" + trans_type\n",
    "        print(\"saving at: \" + str(save_path))\n",
    "        save_pickle(save_path, transform)\n",
    "    \n",
    "    audio = Audio(transform)\n",
    "    plt.title(label)\n",
    "    \n",
    "    print_msg =  \"\\x1b[31m\\\"\"+ 'pickle_load(' + save_path + ')' + \"\\\"\\x1b[0m\"\n",
    "    print(\" to load this transform type \" + print_msg)\n",
    "    \n",
    "    \n",
    "    #print(plt.get_xydata() )\n",
    "    #print(labels)\n",
    "    #if trans_type == \"cqt\":\n",
    "    #    print(\"C1 = \" + str(librosa.note_to_hz('C0')))\n",
    "    return(audio) #transform, \n",
    "\n",
    "\n",
    "def save_pickle(path, transform):\n",
    "    save_path = \"./pickle_files/spectrogram_files/\" + path +\".pickle\"\n",
    "    with open(save_path, 'wb') as handle:\n",
    "        pickle.dump(transform, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(path):\n",
    "    path = \"./pickle_files/spectrogram_files/\" + path +\".pickle\"\n",
    "    with open(path, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return(b)\n",
    "#self.librosa_outfile = librosa_outfile\n",
    "#self.spectogram_path = spectogram_path\n",
    "\n",
    "#'./pickle_files/cqt_low_pitch.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"stft\", label = \"19th century male voice\",\n",
    "              pth = \"/Users/hayden/Desktop/wav_files/computer_male.mp3\",\n",
    "              save_path = \"19th_century_male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"stft\", label = \"19th century female voice\", pth = \"/Users/hayden/Desktop/computer_female.mp3\", save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"19th century male cqt transform\", pth = \"/Users/hayden/Desktop/computer_male.mp3\", save = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"18th_cqt_f\", pth = \"/Users/hayden/Desktop/computer_female.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"18th_cqt_f\", pth = \"/Users/hayden/Desktop/computer_female.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"18th_cqt_high\",  pth = pth_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_free = \"/Users/hayden/Desktop/get_free.wav\"\n",
    "get_transform(\"cqt\", label = \"CQT: Get Free\",  pth = pth_free, partition = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_free = \"/Users/hayden/Desktop/get_free.wav\"\n",
    "#get_transform(\"stft\", label = \"FT: Get Free\",  pth = pth_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cqt = get_transform(\"cqt\", n_bins_mult = 150, y_axis = \"hz\", norm = 1, method = \"pow\" ) #filter_scale = 0.2\n",
    "NBINS = 100\n",
    "\n",
    "#plt.subplot(1,2,1)\n",
    "\n",
    "                              #hop_length_mult = 1)\n",
    "#plt.title(\"dense\")\n",
    "#save_pickle(\"18th_cqt_low\", cqt_low_pitch)\n",
    "\n",
    "#plt.subplot(1,2,2)\n",
    "cqt_high_pitch  = get_transform(\"cqt\", \n",
    "                                pth = pth_high,  \n",
    "                                n_bins_mult = NBINS,\n",
    "                                label = \"18th_cqt_high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the moment log(power) and default db setting appear the same. consider custom functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "fourier = get_transform(\"stft\", n_bins_mult = 150, method = \"pow\") # y_axis = \"log\",)\n",
    "save_pickle(\"fourier_power_low\", fourier_db_low)\n",
    "\n",
    "pth_high = \"/Users/hayden/Desktop/18th_century_high.m4a\"\n",
    "plt.title(\"dense\")\n",
    "fourier = get_transform(\"stft\", n_bins_mult = 150, # y_axis = \"log\",\n",
    "                       pth = pth_high, method = \"pow\")\n",
    "plt.title(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "exper_ = get_experiment(experiment_lst[0], plot_split = False, compare_ = False)\n",
    "print(exper_.A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "librosa.display.specshow(exper_.A.T, y_axis='cqt_note', x_axis='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert dB to Pascals: (Pa)\n",
    "\n",
    "https://www.translatorscafe.com/unit-converter/en-US/sound-pressure-level/2-9/pascal-sound%20pressure%20level%20in%20decibels/#:~:text=Sound%20pressure%20level%20Lp,20%20%CE%BCPa%20or%200.00002%20Pa)\n",
    "\n",
    "\"Sound pressure level (SPL) is a logarithmic (decibel) measure of the sound pressure relative to the reference value of 20 μPa threshold of hearing. The threshold of hearing is the quietest sound that most young healthy people can hear. Sound pressure level Lp is measured in decibels (dB) and is calculated as follows:\"\n",
    "\n",
    "$L_p = 20*log_{10} (p/p_0)$\n",
    "\n",
    "Thus:\n",
    "$(p/p_0) = 10^{L_p/20}$\n",
    "\n",
    "usually $p_0 = 20 \\mu $ Pa or 0.00002 Pa\n",
    "\n",
    "\"The sound pressure level is an absolute value because it is referenced to another absolute value — the threshold of hearing. Therefore, the sound pressure in linear values like pascals can be converted into the sound pressure level in decibels and vice versa if the reference sound pressure is known.\"\n",
    "\n",
    "So p = 0.00002 * 10^{L_p/20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert dB to Intensity of sound: \n",
    "\n",
    "https://www.omnicalculator.com/physics/db#sound-intensity-level-sil\n",
    "\n",
    "Sound intensity is defined as the sound wave power per unit area. It is a special quantity that allows us to measure the energy of sound (or, to be more precise, the energy per second per one squared meter).\n",
    "\n",
    "SIL = $10*log_{10}\\left(\\frac{I}{I_{ref}}\\right)$\n",
    "\n",
    "where:\n",
    "SIL is the sound intensity level in dB;\n",
    "I is the sound intensity in watts per squared meter;\n",
    "Iref is the reference value if sound intensity. Typically, it is assumed to be equal to 1×10⁻¹² W/m².\n",
    "\n",
    "$$I = 10^{\\frac{SIL}{10}-12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The threshold of hearing:\n",
    "The threshold of hearing is generally reported as the RMS sound pressure of 20 micropascals, i.e. 0 dB SPL, corresponding to a sound intensity of 0.98 pW/m2 at 1 atmosphere and 25 °C.[3] It is approximately the quietest sound a young human with undamaged hearing can detect at 1,000 Hz.[4] The threshold of hearing is frequency-dependent and it has been shown that the ear's sensitivity is best at frequencies between 2 kHz and 5 kHz,[5] where the threshold reaches as low as −9 dB SPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = get_experiment(experiment_lst[0])\n",
    "dat = hi.A\n",
    "sns.distplot(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_parameters(array):\n",
    "    log_array    = np.log(array)\n",
    "    sample_mu, sample_sigma   = np.mean(log_array), np.std(log_array)\n",
    "    sample_variance = sample_sigma**2\n",
    "    estimated_mean = np.exp(sample_mu + 0.5*sample_variance)\n",
    "    estimated_variance = estimated_mean**2 * (np.exp(sample_variance) - 1) #np.exp\n",
    "    \n",
    "    est_params = {\"mean\" : estimated_mean,\n",
    "                  \"sd\" :   np.sqrt(estimated_variance)}\n",
    "    return(est_params)\n",
    "\n",
    "def dB2Pa(db_np, normalize = False, drop_silent = False, relative = True):\n",
    "    \"\"\"\n",
    "    converts a decibel level to pascals, for numpy arrays\n",
    "    \n",
    "    if normalize is set to true it normalizes the data assuming a log-normal distribution.\n",
    "    \n",
    "    if drop_silent is true, the function will flatten sounds not hearable by the human ear ( less that 20 * 10-6 pascals)\n",
    "    \"\"\"\n",
    "    p0 = 0.00002\n",
    "    pa_np = 10**(db_np/20)* p0 \n",
    "    \n",
    "    hearing_threshold = 20 * 10 **(-6)\n",
    "    faint_sounds = pa_np < hearing_threshold\n",
    "    \"\"\"\n",
    "    if normalize: #pa_np has a log-normal distribution: https://stats.stackexchange.com/questions/173715/calculate-variance-and-standard-deviation-for-log-normal-distribution\n",
    "        pa_np = np.log(pa_np) # it has a log-normal distribution roughly, so we transform, normalize, then transform back\n",
    "        pa_np = (pa_np - np.mean(pa_np))/np.std(pa_np)\n",
    "        pa_np = np.exp(pa_np)\n",
    "        \n",
    "     \n",
    "    \"\"\"\n",
    "    if normalize: #pa_np has a log-normal distribution: https://stats.stackexchange.com/questions/173715/calculate-variance-and-standard-deviation-for-log-normal-distribution\n",
    "        params = log_normal_parameters(pa_np)\n",
    "        print(params)\n",
    "        \n",
    "        mn, sig = params[\"mean\"], params[\"sd\"]\n",
    "        #hearing_threshold = (hearing_threshold - mn)/sig\n",
    "        pa_np = ((pa_np - mn)/sig)\n",
    "        #pa_np = pa_np  - np.min(pa_np)\n",
    "    \n",
    "    if relative:\n",
    "        pa_np = pa_np/ hearing_threshold\n",
    "    \n",
    "    #Flatten the sounds which the human ear cannot hear. rounding to 5 places preserved the lower bound of human hearing.\n",
    "    if drop_silent:\n",
    "        pa_np[faint_sounds] = np.round(pa_np[faint_sounds], 6)\n",
    "        #new lower bound to avoid 0 values:\n",
    "        pa_np_threshold = 0.000002 #20 * 10 ** (-7)\n",
    "        pa_np[pa_np < pa_np_threshold] = pa_np_threshold\n",
    "    return(pa_np)\n",
    "\n",
    "\n",
    "def dB2Intensity(db_np):\n",
    "    \"\"\"\n",
    "    converts a decibel level to pascals, for numpy arrays\n",
    "    *\n",
    "    \"\"\"\n",
    "    power = db_np/10 -12\n",
    "    Intensity = np.power(10, power)\n",
    "    return(Intensity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xpow =  dB2Pa(dat, normalize =  True)\n",
    "\n",
    "sns.distplot(Xpow)#Log- Normal!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xdb_normal = (Xdb - np.mean(Xdb))/np.std(Xdb)\n",
    "Xpow =  dB2Pa(Xdb, normalize = True, drop_silent = False)\n",
    "sns.distplot(np.log10(Xpow)) \n",
    "min_sound_human_hearing = 20 * 10**(-6)\n",
    "plt.xlabel(\"log(Pascals)\")\n",
    "plt.title(\"ke plot of Pascal values\")\n",
    "plt.ylabel(\"\")\n",
    "plt.axvline(x=np.log10(min_sound_human_hearing) , color = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 micro Pascals\n",
    "The softest sound a normal human ear can detect has a pressure variation of 20 micro Pascals, abbreviated as µPa, which is 20 x 10-6 Pa (\"20 millionth of a Pascal\") and is called the Threshold of Hearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_log_comparison(dataset, \n",
    "                          method = \"propto-dB\", \n",
    "                          propto = True, \n",
    "                          normalized = True,\n",
    "                          log = False,\n",
    "                          drop_silent = True):\n",
    "\n",
    "    # we are only proportional because we have normalized the data.\n",
    "    assert method in [\"propto-dB\", \"propto-SIL\", \"propto-Pa\"], \"choose decibels or sound energy\"\n",
    "    \n",
    "    if method == \"propto-SIL\":\n",
    "        plot_value_label = \"SIL \"\n",
    "        dataset = dB2Intensity(dataset)\n",
    "        if log:\n",
    "            dataset = np.log(dataset)/np.log(10)\n",
    "        ylab = 'SIL (Sound Intensity Level)'  #sound wave power per unit area')\n",
    "            #dataset = (dataset - np.mean(dataset))/np.std(dataset) <-- gets you  what you started with\n",
    "    elif method == \"propto-Pa\":\n",
    "        \n",
    "        plot_value_label = \" (Pa)\" #sound pressure\n",
    "        ylab = \"Pascal\"\n",
    "        \n",
    "        dataset = dB2Pa(dataset, normalize = normalized, drop_silent = False)\n",
    "        #dataset = np.abs(dataset)\n",
    "        #dataset = np.log(dataset) #/np.std(dataset)\n",
    "        #dataset = (dataset - np.mean(dataset))/ np.std(dataset)\n",
    "        \n",
    "    else:\n",
    "        plot_value_label = \" (dB)\" # decibels\n",
    "        ylab = \"Hz\"\n",
    "        #lower_db_limit = -20\n",
    "        \n",
    "        if normalized:\n",
    "            dataset = (dataset - np.mean(dataset))/np.std(dataset)\n",
    "            #lower_db_limit = (lower_db_limit - np.mean(dataset))/np.std(dataset) \n",
    "        #if drop_silent:\n",
    "        #    dataset[dataset < lower_db_limit] = lower_db_limit\n",
    "            \n",
    "    \n",
    "    plot_title = \" spectrogram of '18th century'\" + plot_value_label\n",
    "    \n",
    "    plt.figure(figsize = (12,8))\n",
    "    \n",
    "    # Linear spectogram Plot\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    librosa.display.specshow(dataset, y_axis='linear', x_axis='time')\n",
    "    \n",
    "    #display(quadmesh_.get_axes()) # get the first line, there might be more\n",
    "\n",
    "    #print(ax1.get_axes())#.get_xdata())\n",
    "    #print(plt.get_xdata())\n",
    "    plt.title('Linear ' + plot_title)\n",
    "    add_experiment_regions(ax1)\n",
    "    plt.ylabel(\"Hz\")\n",
    "    \n",
    "    #legend\n",
    "    legend_elements = [Patch(facecolor='pink', edgecolor='red',     label='3500 to 4500 Hz'),\n",
    "                       Patch(facecolor='lightblue', edgecolor='blue',   label='1500 to 2500 Hz'),\n",
    "                       Patch(facecolor='palegreen', edgecolor='green', label='250 to 12250 Hz')]\n",
    "    plt.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "    # Log spectogram Plot\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    librosa.display.specshow(dataset, y_axis='log', x_axis='time')\n",
    "    plt.title('Log ' + plot_title)\n",
    "    add_experiment_regions(ax2)\n",
    "    plt.ylabel(\"Hz\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if method == \"propto-dB\":\n",
    "        propto_str = '%+2.0f' + str(r'$\\propto$') if propto else '%+2.0f'\n",
    "        colorbar_label = propto_str +\" dB\" #+\n",
    "    elif method == \"propto-SIL\":\n",
    "        colorbar_label = \"%+2.0f e-12 SIL\"  # + propto_str + \n",
    "    elif method == \"propto-Pa\":\n",
    "        colorbar_label = \"%.1e Pa\"  # propto_str +\n",
    "\n",
    "    plt.colorbar(format= colorbar_label)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    sns.distplot(dataset)\n",
    "    return(dataset)\n",
    "\n",
    "def add_experiment_regions(ax, plot = True):\n",
    "    def fill_region(lb, ub, color_):\n",
    "        ax.axhline(y=lb, color = color_, linestyle='-')\n",
    "        ax.axhline(y=ub, color = color_, linestyle='-')\n",
    "        x  = np.arange(0.0, 27, 0.1)\n",
    "        y1 = lb + 0 * x\n",
    "        y2 = ub + 0 * x\n",
    "        ax.fill_between(x, y2, y1, alpha = 0.2, color = color_)\n",
    "    trial = 2\n",
    "    if trial == 1:\n",
    "        lb_targ, ub_targ, obs_hz  = 210, 560, 320 / 2\n",
    "\n",
    "    elif trial == 2:\n",
    "        lb_targ, ub_targ, obs_hz  = 340, 640, 280\n",
    "    if plot:\n",
    "        fill_region(lb_targ-obs_hz, lb_targ, \"b\")  #150 hz\n",
    "        fill_region(lb_targ, ub_targ, \"g\") #250 hz\n",
    "        fill_region(ub_targ, ub_targ + obs_hz, \"b\") #150 hz\n",
    "    else:\n",
    "        obs_list = list(range(lb_targ-obs_hz, lb_targ, 10))\n",
    "        obs_list += list(range(ub_targ, ub_targ + obs_hz, 10))\n",
    "        resp_list = list(range(lb_targ, ub_targ, 10))\n",
    "        obs_resp = {\"target\": resp_list, \"obs\": obs_list}\n",
    "        return obs_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decibel spectogram (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exper_ = experiment_lst[0]\n",
    "exper_obj = get_experiment(exper_)\n",
    "dat = exper_obj.A.T\n",
    "#hi = (hi - np.mean(hi))/np.std(hi)\n",
    "dataset_Pow = linear_log_comparison(dat, \n",
    "                      propto = False, \n",
    "                      normalized = False,\n",
    "                      drop_silent = False,\n",
    "                      method = \"propto-Pa\") \n",
    "\n",
    "dataset_db = linear_log_comparison(dat, \n",
    "                                    propto = False, \n",
    "                                    normalized = False,\n",
    "                                    drop_silent = True,\n",
    "                                    method = \"propto-dB\") \n",
    "\n",
    "custom_transform = {\"transform\": {\n",
    "                        \"Xdb\"  : dataset_db,\n",
    "                        \"Xpow\" : dataset_Pow,\n",
    "                        \"f\"  :   exper_obj.f}\n",
    "                   }\n",
    "save_pickle(\"custom\",custom_transform)\n",
    "#load_pickle(\"custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE THIS NORMALIZED Pa DATASET!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decibel spectogram, unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(exper_, \n",
    "                      propto = False, \n",
    "                      normalized = False,\n",
    "                      drop_silent = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pascal spectogram, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(exper_, method = \"propto-Pa\", normalized = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pascal spectogram, unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(exper_, method = \"propto-Pa\", normalized = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at the average sound pressure per log frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dB2Pa(exper_.A_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(inputt = Xdb, method = \"propto-Pa\", drop_silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(exper_.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_spectogram(dataset = exper_.A, f_arr = exper_.f):\n",
    "    \n",
    "    T = exper_.T\n",
    "    plt.imshow(dataset)\n",
    "    \n",
    "    f = np.array(f_arr)[1:] # humans hearing ranges from 20 db to 20k db so lets drop 0 to avoid -infty.\n",
    "    dataset = dataset[:,1:]\n",
    "    \n",
    "    f = np.log(f)/np.log(2) # humans experience sound logarithmically\n",
    "    \n",
    "    sns.distplot(dataset)\n",
    "    plt.show()\n",
    "    n_timesteps, n_frequencies  = dataset.shape\n",
    "\n",
    "    for i, time_step in enumerate(range(n_timesteps)):\n",
    "        if not i:\n",
    "            dictt_lst = []\n",
    "        this_timestep = T[time_step][0]\n",
    "        \n",
    "        #assert len(f) == len(this_timestep), \"error: \" + str(len(f)) + \" != \" + str(len(this_timestep))\n",
    "        for i, frequency_spec in enumerate(f):              \n",
    "            dictt_lst += [{\"frequency\" : frequency_spec , \n",
    "                           \"time\"      : this_timestep,\n",
    "                           \"amplitude\" : dataset[time_step, i]\n",
    "                            }]\n",
    "        #display(pd.DataFrame(dictt_lst))\n",
    "    \n",
    "    log_frequency_df = pd.DataFrame(dictt_lst)\n",
    "    log_frequency_df = log_frequency_df.pivot(\"frequency\", \"time\", \"amplitude\")\n",
    "    \n",
    "    sns.heatmap(log_frequency_df)\n",
    "    \n",
    "create_log_spectogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sns.load_dataset(\"flights\")\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(Xdb, y_axis='log', x_axis='time')\n",
    "plt.title('log Power spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Xdb, aspect = 10)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_ = f_[1], f_[15] #_ denotes temporary variable, for testing or within a function.\n",
    "\n",
    "lb_, ub_ = bounds_\n",
    "\n",
    "def retrieve_freqs_btwn(bounds, f_):\n",
    "    f = np.array(f_)\n",
    "    lb, ub = bounds\n",
    "    display(bounds_)\n",
    "    lb_bool_vec, ub_bool_vec = (f > lb_), (f < ub_)\n",
    "    and_vector = ub_bool_vec* lb_bool_vec\n",
    "\n",
    "    freqs = f[and_vector]            #frequencies between bounds\n",
    "    freq_idxs = np.where(and_vector)[0] #indices between bounds\n",
    "\n",
    "    return(freq_idxs.tolist())\n",
    "    \n",
    "retrieve_freqs_btwn(bounds_, f_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def log_amplitude(exper_):\n",
    "    A = np.array(exper_.A)\n",
    "    print(np.min(A))\n",
    "    print(np.max(A))\n",
    "    orig_shape = A.shape\n",
    "    \n",
    "    signs = A.copy().reshape(-1,) < 0\n",
    "    signs = signs * 2 - 1\n",
    "    print(np.unique(signs))\n",
    "    signs = signs.reshape(orig_shape)\n",
    "    #plt.imshow(signs)\n",
    "    A_new = np.log(np.abs(A)) * signs\n",
    "    \n",
    "    #A_new = (A_new - np.mean(A_new))/ np.std(A_new)\n",
    "    #print(np.min(A_new))\n",
    "    #print(np.max(A_new))\n",
    "    #plt.imshow(A_new)\n",
    "    return(A_new)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_pickle('./pickle_files/results/18th_cqt_high/db/untouched/split_0.5/tf_250__obsHz_0.1__targHz_0.02.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_pickle(\"custom\")\n",
    "\n",
    "sns.heatmap(hi[\"Xpow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_pickle(\"18th_cqt_high\")\n",
    "print(hi[\"transform\"][\"Xdb\"])\n",
    "sns.heatmap(hi[\"transform\"][\"Xdb\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "def get_frequencies(trial = 1):\n",
    "    \"\"\"\n",
    "    get frequency lists\n",
    "    \"\"\"\n",
    "    if trial == 1:\n",
    "        lb_targ, ub_targ, obs_hz  = 210, 560, int(320 / 2)\n",
    "\n",
    "    elif trial == 2:\n",
    "        lb_targ, ub_targ, obs_hz  = 340, 640, 280\n",
    "    elif trial == 3:\n",
    "        lb_targ, ub_targ, obs_hz  = 340, 350, 20\n",
    "\n",
    "\n",
    "    obs_list = list(range(lb_targ-obs_hz, lb_targ, 10))\n",
    "    obs_list += list(range(ub_targ, ub_targ + obs_hz, 10))\n",
    "    resp_list = list(range(lb_targ, ub_targ, 10))\n",
    "    return obs_list, resp_list\n",
    "\n",
    "obs_freqs, resp_freqs = get_frequencies(1)\n",
    "librosa_args = { \"spectrogram_path\": \"custom\",#\"cqt_high_pitch\",\n",
    "                         \"librosa\": True}\n",
    "#inputs = {'obs_freq_lst' :, \"targ_freq_lst\": , \"split\": 0.5}\n",
    "                       \n",
    "additional_Echo_inputs = {\n",
    "            \"obs_freq_lst\":  obs_freqs,\n",
    "            \"targ_freq_lst\" : resp_freqs\n",
    "            }\n",
    "Echo_inputs = {\n",
    "        \"size\" : \"medium\",\n",
    "        \"verbose\" : False,\n",
    "        \"prediction_type\" : \"block\"}\n",
    "Echo_inputs = Merge(Echo_inputs, additional_Echo_inputs)\n",
    "experiment = EchoStateExperiment( **Echo_inputs, **librosa_args)\n",
    "experiment.get_observers(method = \"exact\", split = 0.5, aspect = 0.9, plot_split = False)\n",
    "experiment.obs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree pickle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = load_pickle(\"./pickle_files/spectrogram_files/18th_cqt_high.pickle\")\n",
    "g_truth = dat[\"transform\"][\"Xdb\"]\n",
    "g_truth = (g_truth - np.mean(g_truth))/np.std(g_truth)\n",
    "line = g_truth[35][513:]\n",
    "fig, ax = plt.subplots(1,1,figsize = (10,4))\n",
    "sns.lineplot(x = range(len(bye)), y = bye, label = \"unif\")\n",
    "sns.lineplot(x = range(len(ip)), y = ip, label = \"ip\")\n",
    "sns.lineplot(x = range(len(line)), y =line, label = \"gtruth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_shape_0 = 1000\n",
    "def get_obs_eq(k):\n",
    "    hi = A_shape_0//k\n",
    "    viable_start = np.random.randint(hi)\n",
    "    observers = [k*i + viable_start for i, idx in  enumerate(range(viable_start, A_shape_0, k))]\n",
    "    print(observers)\n",
    "        \n",
    "get_obs_eq(25)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls pickle_files/results/custom/db/untouched/split_0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Zhizhuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = 40\n",
    "end_idx = 942\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(test1.ip_res)\n",
    "#https://stackoverflow.com/questions/35215161/most-efficient-way-to-map-function-over-numpy-array\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "def pure_prediction_ip_generator(missing_data, end_idx):\n",
    "    test_idx = list(range(end_idx))[-missing_data:] #553, 712, 942\n",
    "    print(test_idx)\n",
    "    train_range_input = end_idx - missing_data\n",
    "    train_idx = list(range(train_range_input))\n",
    "    print(train_range_input)\n",
    "\n",
    "    experiment_inputs1 =  {'size': 'medium', \n",
    "                           'target_frequency': None, \n",
    "                           'verbose': False, \n",
    "                           'prediction_type': 'column', \n",
    "                           \"interpolation_method\" : \"griddata-nearest\",\n",
    "                           'train_time_idx': train_idx,\n",
    "                           'test_time_idx' : test_idx}#[514, 515, 516, 517, 518, 519, 520, 521, 522, 523]}#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249], 'test_time_idx': [250, 251, 252, 253, 254, 255, 256, 257, 258, 259]}\n",
    "\n",
    "    test1 = EchoStateExperiment(**experiment_inputs1)\n",
    "    obs_inputs1 =  {'split': 0.5, 'aspect': 0.9, 'plot_split': False, 'method': 'exact'}\n",
    "    test1.get_observers(**obs_inputs1)\n",
    "\n",
    "\n",
    "    import math\n",
    "    def f(x):\n",
    "        \"\"\"\n",
    "        check if x is nan\n",
    "        x = float('nan')\n",
    "        math.isnan(x)\n",
    "        \"\"\"\n",
    "        return math.isnan(x)\n",
    "    def array_map(x, f):\n",
    "        x_shape= x.shape\n",
    "        print(\"x_shape\" + str(x.shape))\n",
    "        x  = x.flatten().tolist()\n",
    "        hi = np.array(list(map(f,x)))\n",
    "        print(hi)\n",
    "\n",
    "        return np.array(hi).reshape(x_shape)\n",
    "\n",
    "    test1_ip_pred = test1.ip_res[\"prediction\"]\n",
    "\n",
    "    plt.imshow(array_map(test1_ip_pred, f))\n",
    "    test1_ip_pred\n",
    "\n",
    "    my_dict = {\n",
    "        \"interpolation_prediction\": test1.ip_res[\"prediction\"],\n",
    "        \"ground_truth_test\"  : test1.xTe,\n",
    "        \"ground_truth_train\" : test1.xTr,\n",
    "        \"interpolation_MSE\"  : test1.ip_res[\"nrmse\"]\n",
    "    }\n",
    "\n",
    "    from scipy.io import savemat\n",
    "\n",
    "    save_path = \"zhizhuo/testindex_\" + str(test_idx[0]) + \"_\" + str(test_idx[-1]) +\".mat\"\n",
    "\n",
    "    print(save_path)\n",
    "    savemat(save_path, my_dict) #\"zhizhuo/testindex_514_523.mat\"\n",
    "    plt.imshow(test1.ip_res[\"prediction\"], aspect = 0.1)\n",
    "    return(test1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = [{\"missing_data\" : 40, \"end_idx\" : 289},\n",
    "            {\"missing_data\" : 40, \"end_idx\" : 553},\n",
    "            {\"missing_data\" : 40, \"end_idx\" : 712},\n",
    "            {\"missing_data\" : 40, \"end_idx\" : 942}]\n",
    "test_results = []\n",
    "for prediction in test_lst:\n",
    "    pred_ = pure_prediction_ip_generator(**prediction)\n",
    "    test_results.append(pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rez = [ test_result.ip_res[\"prediction\"] for test_result in test_results]\n",
    "test_ground = [ test_result.xTe for test_result in test_results]\n",
    "count = 1\n",
    "for i, rez in enumerate(test_rez):\n",
    "    plt.imshow(rez, aspect = 10)\n",
    "    plt.title(\"prediction\" + str(count))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.imshow(test_ground[i], aspect = 10)\n",
    "    plt.title(\"truth\" + str(count))\n",
    "    plt.show()\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_pickle('19th_century_male_stft')\n",
    "plt.imshow(X['transform']['Xdb'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"stft\", label = \"19th century male voice\", pth = \"/Users/hayden/Desktop/computer_male.mp3\", save_path = \"19th_century_male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = \"\"\"                  {'target_freq': 2000, 'split': 0.5, 'obs_hz': 250, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 250, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 250, 'target_hz': 1000},\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 500, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 500, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 500, 'target_hz': 1000},\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 750, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 750, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 750, 'target_hz': 1000},\n",
    "\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 250, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 250, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 250, 'target_hz': 1000},\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 500, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 500, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 500, 'target_hz': 1000},\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 750, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 750, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 750, 'target_hz': 1000},\"\"\"\n",
    "path_lst = [\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.25.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.25.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.25.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.5.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.75.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.75.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.75.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.25.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.25.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.25.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.5.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.75.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.75.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.75.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.25.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.25.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.25.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.5.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.75.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.75.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.75.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.25.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.25.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.25.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.5.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.75.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.75.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.75.txt\",\n",
    "            ]\n",
    "\n",
    "path_lst = ['/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/medium/split_0.5/targetKhz:_0.01__obskHz:_0.02.txt' ]\n",
    "\n",
    "path_lst = ['/Users/hayden/Desktop/DL_LAB/Reservoir/backwards/experiment_results1/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.1.txt',\n",
    "            \"/Users/hayden/Desktop/DL_LAB/Reservoir/backwards/experiment_results1/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.25.txt\",\n",
    "            \"/Users/hayden/Desktop/DL_LAB/Reservoir/backwards/experiment_results1/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "            \"/Users/hayden/Desktop/DL_LAB/Reservoir/backwards/experiment_results1/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt\"\n",
    "           ]\n",
    "complete_experiment_path_lst = [ \n",
    "    #targ 500  kHz COMPLETE\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt',\n",
    "            #targ 1000 kHz COMPLETE\n",
    "            '2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_1.0__obskHz:_1.0.txt',\n",
    "            \n",
    "            #targ 500  Hz COMPLETE\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt', #no exp\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.0.txt', #no exp\n",
    "            #targ 1000 Hz COMPLETE\n",
    "            '2k/medium/split_0.5/targetKhz:_1.0__obskHz:_1.0.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt', #no exp\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "            finished but publish size:\n",
    "            '/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.1.txt',\n",
    "            \"/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.25.txt\",\n",
    "            \n",
    "            ########################################################################### 1k\n",
    "            completed 1k tests\n",
    "            \"/1k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.0.txt\",\n",
    "            \"/1k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def check_for_duplicates(lst, UnqLst = True, verbose = True):\n",
    "    for i, item in enumerate(lst):\n",
    "        if not i:\n",
    "            unique_lst = []\n",
    "            duplicates = []\n",
    "        if item in unique_lst:\n",
    "            duplicates.append(item) \n",
    "        else:\n",
    "            unique_lst.append(item)\n",
    "    if verbose:\n",
    "        print(duplicates)\n",
    "    if UnqLst:\n",
    "        return(unique_lst) \n",
    "\"\"\"     \n",
    "\n",
    "    \n",
    "experiments1k = [\n",
    "            \"/1k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt\"]\n",
    "\n",
    "hi = \"\"\"\n",
    "for i in experiments1k:\n",
    "    experiment_ = load_data(i,\n",
    "                             bp = './experiment_results')\n",
    "    hi = pd.DataFrame(experiment_['nrmse'], index = [0])\n",
    "    hi = pd.melt(hi)\n",
    "    hi.columns = [\"model\", \"nrmse\"]\n",
    "    print(hi)\n",
    "    sns.barplot(x = \"model\", y = \"nrmse\", data = hi)\n",
    "    #experiment_obj = get_experiment(experiment_5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# when it comes time to run a lot of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "def quick_write_path(freq, split, targHz, obsHz, size = \"/medium\"):\n",
    "    if freq == 2000:\n",
    "        freqStr = \"2k\"\n",
    "    elif freq == 4000:\n",
    "        freqStr = \"4k\"\n",
    "    splitStr = \"/split_\" + str(split)\n",
    "    targHz, obsHz = str(targHz/1000) , str(obsHz/1000)\n",
    "    HzStr = \"/targetKhz:_\" + targHz + \"__obskHz:_\" +  obsHz \n",
    "    newPath = freqStr + size + splitStr + HzStr +\".txt\"\n",
    "    return([newPath])\n",
    "\n",
    "def quick_write_dict(freq, split, targHz, obsHz):\n",
    "    dict_tmp = {'target_freq': freq, 'split': split, 'obs_hz': obsHz, 'target_hz': targHz}\n",
    "    return([dict_tmp])\n",
    "\n",
    "\n",
    "path_lst = []\n",
    "dict_lst = []\n",
    "for targ_freq in [2000, 4000]:\n",
    "    for split in [0.5, 0.9]:\n",
    "        for targ in list(range(500, 2001, 250)):\n",
    "            for obs in list(range(500, 2001, 250)):\n",
    "                path_lst += quick_write_path(freq = targ_freq, split = split, targHz = targ, obsHz = obs)\n",
    "                dict_lst += quick_write_dict(freq = targ_freq, split = split, targHz = targ, obsHz = obs)\n",
    "\n",
    "\n",
    "path_lst += [ \n",
    "            # the plan is to run all those tests which will give detail from the LHS. ie increasin\n",
    "            # target Hz.\n",
    "            ########################################################################### 2k\n",
    "            #######################2k, 0.9 \n",
    "            #targ 500  kHz COMPLETE\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.75.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.25.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.75.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_2.0.txt',\n",
    "            \n",
    "    \n",
    "            \n",
    "\n",
    "            #targ 750  H z\n",
    "            '2k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.75__obskHz:_1.0.txt',\n",
    "    \n",
    "            #targ 1000 kHz COMPLETE\n",
    "            '2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_1.0__obskHz:_1.0.txt',\n",
    "            \n",
    "            #targ 1250  Hz\n",
    "            '2k/medium/split_0.9/targetKhz:_1.25__obskHz:_0.5.txt', \n",
    "            '2k/medium/split_0.9/targetKhz:_1.25__obskHz:_1.0.txt', \n",
    "    \n",
    "            #targ 1500  Hz\n",
    "            '2k/medium/split_0.9/targetKhz:_1.5__obskHz:_0.5.txt', \n",
    "            '2k/medium/split_0.9/targetKhz:_1.5__obskHz:_1.0.txt', \n",
    "    \n",
    "            #targ 1750  Hz\n",
    "            '2k/medium/split_0.9/targetKhz:_1.75__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_1.75__obskHz:_1.0.txt',\n",
    "    \n",
    "            #targ 2000  Hz\n",
    "            '2k/medium/split_0.9/targetKhz:_2.0__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_2.0__obskHz:_1.0.txt',\n",
    "    \n",
    "            #######################2k, 0.5\n",
    "            #targ 500  Hz COMPLETE\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.75.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.0.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.25.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.5.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.75.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_2.0.txt',\n",
    "    \n",
    "             #targ 750 Hz\n",
    "             \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_1.0.txt\",\n",
    "             \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.5.txt\", #CHECK LATER\n",
    "    \n",
    "            #targ 1000 Hz\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_1.0.txt\", #\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\", #\n",
    "    \n",
    "            #targ 1250 Hz\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.25__obskHz:_1.0.txt\", # ABOUT TO RUN 600\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.25__obskHz:_0.5.txt\", # ABOUT TO RUN 600\n",
    "    \n",
    "            #targ 1500 Hz\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.5__obskHz:_1.0.txt\", # ABOUT TO RUN 1000\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.5__obskHz:_0.5.txt\", # ABOUT TO RUN 1000\n",
    "    \n",
    "     \n",
    "             #targ 2000 Hz\n",
    "             '2k/medium/split_0.5/targetKhz:_2.0__obskHz:_1.0.txt', #no exp\n",
    "             \n",
    "    \n",
    "           ########################################################################### 4k\n",
    "           #######################4k, 0.9 \n",
    "           #4k, 0.9 500 target Hz COMPLETE\n",
    "           \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "           \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt\",\n",
    "            \n",
    "           #4k, 0.9 1000 target Hz RUNNING\n",
    "           '4k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.5.txt', #RUNNING 200\n",
    "           '4k/medium/split_0.9/targetKhz:_0.75__obskHz:_1.0.txt', #RUNNING 200\n",
    "    \n",
    "           \n",
    "           #4k, 0.9 1000 target Hz COMPLETE\n",
    "           '4k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt',\n",
    "           '4k/medium/split_0.9/targetKhz:_1.0__obskHz:_1.0.txt',\n",
    "    \n",
    "           #4k, 0.9 1250 target Hz COMPLETE\n",
    "           '4k/medium/split_0.9/targetKhz:_1.25__obskHz:_0.5.txt', #RUNNING 400\n",
    "           '4k/medium/split_0.9/targetKhz:_1.25__obskHz:_1.0.txt', #RUNNING 400\n",
    "           \n",
    "           #4k, 0.9 1500 target Hz COMPLETE\n",
    "           '4k/medium/split_0.9/targetKhz:_1.5__obskHz:_0.5.txt', #RUNNING 700\n",
    "           '4k/medium/split_0.9/targetKhz:_1.5__obskHz:_1.0.txt', #RUNNING 700\n",
    "            \n",
    "\n",
    "           #######################4k, 0.5\n",
    "           #4k 0.5 target kHz COMPLETE\n",
    "           '4k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt', #???\n",
    "           \"4k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.0.txt\",  #???\n",
    "    \n",
    "           #4k 0.75 target kHz COMPLETE\n",
    "           \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.5.txt\", #NO EXP\n",
    "           \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_1.0.txt\",  \n",
    "           \n",
    "           #4k 1.0 target kHz \n",
    "           \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\",# ????\n",
    "           \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_1.0.txt\",   # ALREADY HAVE IT\n",
    "    \n",
    "           #4k, 0.5 1250 target Hz NEED TO RUN\n",
    "           '4k/medium/split_0.5/targetKhz:_1.25__obskHz:_0.5.txt', #ABOUT TO RUN 500\n",
    "           '4k/medium/split_0.5/targetKhz:_1.25__obskHz:_1.0.txt', #ABOUT TO RUN 500\n",
    "    \n",
    "            #4k, 0.5 1500 target Hz NEED TO RUN\n",
    "           '4k/medium/split_0.5/targetKhz:_1.5__obskHz:_0.5.txt', #ABOUT TO RUN 900\n",
    "           '4k/medium/split_0.5/targetKhz:_1.5__obskHz:_1.0.txt', #NO EXP\n",
    "\n",
    "           #4k 2.0 target kHz \n",
    "           \"4k/medium/split_0.5/targetKhz:_2.0__obskHz:_0.5.txt\", \n",
    "           \"4k/medium/split_0.5/targetKhz:_2.0__obskHz:_1.0.txt\", \n",
    "           \"4k/medium/split_0.5/targetKhz:_2.0__obskHz:_1.5.txt\", #For now this is deemed unessential.\n",
    "           \"4k/medium/split_0.5/targetKhz:_2.0__obskHz:_2.0.txt\", \n",
    "\n",
    "           #4k 0.5, bigger and better! \n",
    "\n",
    "           #\"2k/medium/split_0.5/targetKhz:_1.5__obskHz:_1.0.txt\", \n",
    "\n",
    "\n",
    "           \"4k/medium/split_0.5/targetKhz:_3.0__obskHz:_0.5.txt\",\n",
    "           \"4k/medium/split_0.5/targetKhz:_3.0__obskHz:_1.0.txt\",\n",
    "           \"4k/medium/split_0.5/targetKhz:_3.0__obskHz:_2.0.txt\",\n",
    "\n",
    "           #\"4k/medium/split_0.5/targetKhz:_4.0__obskHz:_0.5.txt\", #??? broken\n",
    "           #\"4k/medium/split_0.5/targetKhz:_4.0__obskHz:_1.0.txt\", #??? broken\n",
    "           #\"4k/medium/split_0.5/targetKhz:_4.0__obskHz:_2.0.txt\", #??? broken\n",
    "\n",
    "           #\"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_1.0.txt\",\n",
    "\n",
    "           # MORE DETAIL:, given that the others aren't converging. \n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out low frequency results\n",
    "exper_lst = []\n",
    "bp_ = \"/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/pickle_files/results/custom/power/untouched/\"\n",
    "\n",
    "new_exper_path_lsts = [\n",
    "    \"split_0.5/tf_485.0__obsNIdx_56__targNIdx_30.pickle\",\n",
    "    \"split_0.9/tf_380.0__obsNIdx_32__targNIdx_35.pickle\"\n",
    "    #tf_485.0__obsNIdx_56__targNIdx_30.pickle\n",
    "]\n",
    "\n",
    "\n",
    "for i in new_exper_path_lsts:\n",
    "    exper_ = load_p_result(i, bp = bp_)\n",
    "    exper_lst += [exper_]\n",
    "    \n",
    "xpow = load_pickle(\"custom\")[\"transform\"][\"Xpow\"]\n",
    "\n",
    "this_experiment = exper_lst[0]\n",
    "resp_idx_ = this_experiment[\"resp_idx\"]\n",
    "print(resp_idx_)\n",
    "resp_ = xpow[resp_idx_]\n",
    "sns.heatmap(resp_)\n",
    "plt.show()\n",
    "sns.heatmap(resp_[:,512:])\n",
    "plt.show()\n",
    "sns.heatmap(np.array(exper_lst[0][\"prediction\"][\"exponential\"]).T)\n",
    "plt.show()\n",
    "sns.heatmap(np.array(exper_lst[0][\"prediction\"][\"interpolation\"]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_lst(lst, scnd_lst):\n",
    "    lst = np.array(lst)\n",
    "    breaka = np.mean(scnd_lst)\n",
    "    lst1, lst2 = lst[lst>breaka], lst[lst<breaka]\n",
    "    \n",
    "    return list(lst1), list(lst2)\n",
    "split_lst([1,2,3, 7,8,9], [4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
