{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mexperiment_results/medium/split_0.5/\u001b[00m\r\n",
      "├── targetHz_ctr:_1000targetKhz:_0.01__obskHz:_0.02.txt\r\n",
      "├── targetHz_ctr:_1000targetKhz:_0.1__obskHz:_0.1.txt\r\n",
      "├── targetHz_ctr:_1000targetKhz:_0.1__obskHz:_0.25.txt\r\n",
      "├── targetHz_ctr:_1000targetKhz:_0.5__obskHz:_1.0.txt\r\n",
      "└── targetHz_ctr:_1000targetKhz:_1.0__obskHz:_1.0.txt\r\n",
      "\r\n",
      "0 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree experiment_results/medium/split_0.5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_string(message, *values):\n",
    "    if not values:\n",
    "        return message\n",
    "    else:\n",
    "        return message.join(str(x) + sep for x in values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_string(\"bp_\", 5, 6, 7, \"blah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def check_for_duplicates(lst, UnqLst = True, verbose = True):\n",
    "    lst_tmp = []\n",
    "    duplicates = []\n",
    "    for i in lst:\n",
    "        if i in lst_tmp:\n",
    "            \n",
    "            duplicates += [i]\n",
    "        else:\n",
    "            lst_tmp += [i]\n",
    "    if verbose == True:\n",
    "        print(duplicates)\n",
    "    if UnqLst:\n",
    "        return(lst_tmp)\n",
    "\n",
    "\n",
    "def compare(truth, \n",
    "            unif_w_pred = None, \n",
    "            exp_w_pred = None, \n",
    "            ip_pred = None,\n",
    "            columnwise = False,\n",
    "            verbose = False):\n",
    "    \"\"\"\n",
    "    This function provides two things, conditional on the columnwise variable.\n",
    "    columnwise = False: cross-model comparison of nrmse\n",
    "    \n",
    "    columnwise = True: model nrmse correlary for each point.\n",
    "    \"\"\"\n",
    "    nrmse_inputs = {\"truth\" : truth, \"columnwise\" : columnwise}\n",
    "    pred_list = [unif_w_pred, exp_w_pred, ip_pred]\n",
    "    pred_dicts = [{\"pred_\": prediction, **nrmse_inputs} for prediction in pred_list]\n",
    "    \n",
    "    def conditional_nrmse(inputs):\n",
    "        return None if not inputs[\"pred_\"] else nrmse(**inputs) # check if prediction is empty\n",
    "        #if _nrmse:\n",
    "        #    return _nrmse\n",
    "    \n",
    "    \n",
    "    unif_nrmse, exp_nrmse, ip_nrmse = [nrmse(**i) for i in pred_dicts]\n",
    "    \n",
    "    \n",
    "    hi = \"\"\"\n",
    "    if type(unif_w_pred) != type(None):\n",
    "        unif_nrmse = nrmse(pred_ = unif_w_pred, **nrmse_inputs)\n",
    "        \n",
    "    if type(exp_w_pred) != type(None):\n",
    "        exp_nrmse  = nrmse(pred_  = exp_w_pred , **nrmse_inputs)\n",
    "    \n",
    "    if type(ip_pred) != type(None):\n",
    "        ip_nrmse   = nrmse(pred_  = ip_pred , **nrmse_inputs)\n",
    "    \"\"\"\n",
    "    ip_res = {\"nrmse\" : ip_nrmse, \"pred\" : ip_pred}\n",
    "        \n",
    "    \n",
    "    assert type(columnwise) == bool, \"columnwise must be a boolean\"\n",
    "    \n",
    "    if not columnwise:\n",
    "        if verbose:\n",
    "            print(\"cubic spline interpolation nrmse: \" + str(ip_res[\"nrmse\"]))\n",
    "            print(\"uniform weights rc nrmse: \" + str(unif_nrmse))\n",
    "            print(\"exponential weights rc nrmse: \" + str(exp_nrmse))\n",
    "            print(\"creating barplot\")\n",
    "        \n",
    "        unif_and_ip_dict = {\"interpolation\" : ip_res[\"nrmse\"], \"uniform rc\" : unif_nrmse}\n",
    "        exp_dict = {\"exponential rc\" : exp_nrmse}\n",
    "        \n",
    "        if type(exp_w_pred) != type(None):\n",
    "            df = pd.DataFrame({\"interpolation\" : ip_res[\"nrmse\"], \n",
    "                               \"uniform rc\" : unif_nrmse, \n",
    "                               \"exponential rc\" : exp_nrmse}, index = [0])\n",
    "        else:\n",
    "            df = pd.DataFrame({\"interpolation\" : ip_res[\"nrmse\"], \n",
    "                               \"uniform rc\" : unif_nrmse}, index = [0])\n",
    "        display(df)\n",
    "\n",
    "        plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "        sns.catplot(data = df, kind = \"bar\")\n",
    "        plt.title(\"model vs nrmse\")\n",
    "        plt.ylabel(\"nrmse\")\n",
    "        improvement = []\n",
    "        for rc_nrmse in[unif_nrmse, exp_nrmse]:\n",
    "            impr_spec = ((ip_res[\"nrmse\"] - rc_nrmse)/ip_res[\"nrmse\"]) * 100\n",
    "            impr_spec = [round(impr_spec,1)]\n",
    "            improvement += impr_spec\n",
    "\n",
    "        pct_improve_unif, pct_improve_exp = improvement\n",
    "        if pct_improve_unif > 0:\n",
    "            print(\"unif improvement vs interpolation: nrmse \" + str(-pct_improve_unif) + \"%\")\n",
    "        else:\n",
    "            print(\"rc didn't beat interpolation: nrmse +\" + str(-pct_improve_unif) + \"%\")\n",
    "        \n",
    "        if pct_improve_exp > 0:\n",
    "            print(\"exp improvement vs interpolation: nrmse \" + str(-pct_improve_exp) + \"%\")\n",
    "        else:\n",
    "            print(\"rc didn't beat interpolation: nrmse +\" + str(-pct_improve_exp) + \"%\")\n",
    "\n",
    "        impr_rc_compare = round(((unif_nrmse - exp_nrmse)/unif_nrmse) * 100,1)\n",
    "\n",
    "        if impr_rc_compare > 0:\n",
    "            print(\"exp rc improvement vs unif rc: nrmse \" + str(-impr_rc_compare) + \"%\")\n",
    "        else:\n",
    "            print(\"exp weights didn't improve rc: nrmse +\" + str(-impr_rc_compare) + \"%\")\n",
    "    else:\n",
    "        print(\"creating first figure\")\n",
    "        model_names = [\"interpolation\", \"uniform rc\", \"exponential rc\"]\n",
    "        for i, model_rmse_np in enumerate([ip_res[\"nrmse\"], unif_nrmse, exp_nrmse]):\n",
    "            model_rmse_pd = pd.melt(pd.DataFrame(model_rmse_np.T))\n",
    "            model_rmse_pd.columns = [\"t\",\"y\"]\n",
    "            model_rmse_pd[\"model\"] = model_names[i]\n",
    "            if not i: # check if i == 0\n",
    "                models_pd = model_rmse_pd\n",
    "            else:\n",
    "                models_pd = pd.concat([models_pd, model_rmse_pd ], axis = 0)\n",
    "        fig, ax = plt.subplots(1,1, figsize = (11, 6))\n",
    "        sns.lineplot(x = \"t\", y = \"y\", hue = \"model\", data = models_pd, ax = ax)\n",
    "        ax.set_title(\"model vs rmse\")\n",
    "        ax.set_ylabel(\"nrmse\")\n",
    "        ax.set_xlabel(\"Test idx\")\n",
    "        \n",
    "def get_experiment(json_obj, verbose = False, compare_ = False, plot_split = False,\n",
    "                   librosa = False):\n",
    "    \n",
    "    experiment_ = EchoStateExperiment(**json_obj[\"experiment_inputs\"], librosa = librosa)\n",
    "    \n",
    "    obs_inputs = json_obj[\"get_observer_inputs\"]\n",
    "    obs_inputs[\"method\"] = \"exact\"\n",
    "    \n",
    "    experiment_.obs_idx, experiment_.resp_idx  = json_obj[\"obs_idx\"], json_obj[\"resp_idx\"]\n",
    "    \n",
    "    experiment_.get_observers(**obs_inputs, \n",
    "                              plot_split = plot_split)\n",
    "    if verbose == True:\n",
    "        print(\"experiment inputs: \" + str(json_obj[\"experiment_inputs\"]))\n",
    "        print(\"get_obs_inputs: \" + str(obs_inputs))\n",
    "        print(\"Train.shape: \" + str(experiment_.Train.shape))\n",
    "        print(\"Saved_prediction.shape: \" + str(np.array(json_obj[\"prediction\"][\"uniform\"]).shape))\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    experiment_.already_trained(json_obj[\"best arguments\"][\"uniform\"], exponential = False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(experiment_.prediction.shape)\n",
    "    #print(experiment_.Test.shape)\n",
    "    \n",
    "    experiment_.Train, experiment_.Test = recover_test_set(json_obj)\n",
    "    \n",
    "    ### which line is missing?\n",
    "    xx = range(experiment_.prediction.shape[0])\n",
    "    \n",
    "    \n",
    "    #plt.imshow( experiment_.prediction)\n",
    "    #plt.show()\n",
    "    \"\"\"\n",
    "    #plt.imshow( experiment_.Test)\n",
    "    #plt.show()\n",
    "    \n",
    "    sns.lineplot( x = xx, y = experiment_.prediction[ : , 0 ], label = \"prediction from cluster 0 \")\n",
    "    sns.lineplot( x = xx, y = experiment_.prediction[ : , 0 ], label = \"prediction from cluster 1 \")\n",
    "    sns.lineplot( x = xx, y = experiment_.Test[ : , 0 ], label = \"actual data 0\")\n",
    "    sns.lineplot( x = xx, y = experiment_.Test[ : , -1 ], label = \"actual data 1\")\n",
    "    #plt.plot(experiment_.Test[ : , 1 ])\n",
    "    \"\"\"\n",
    "    \n",
    "    #experiment_.plot_timeseries(method = \"avg\")\n",
    "    if compare_:\n",
    "        n_keys = len(list(json_obj[\"prediction\"].keys()))\n",
    "        #if  n_keys == 3:\n",
    "        unif_w_pred, exp_w_pred = json_obj[\"prediction\"][\"uniform\"], json_obj[\"prediction\"][\"exponential\"]\n",
    "        ip_pred = json_obj[\"prediction\"][\"interpolation\"]\n",
    "        unif_w_pred, exp_w_pred, ip_pred = [np.array(i) for i in [unif_w_pred, exp_w_pred, ip_pred]]\n",
    "        \"\"\"\n",
    "        if verbose == True:\n",
    "            for i in [[unif_w_pred, \"unif pred\"],\n",
    "                      [exp_w_pred, \"exp pred\"],\n",
    "                      [ip_pred, \"ip pred\"],\n",
    "                      [np.array(experiment_.Test), \"Test\" ]]:\n",
    "                Shape(i)\n",
    "        \"\"\"\n",
    "            \n",
    "        compare(\n",
    "            truth       = np.array(experiment_.Test), \n",
    "            unif_w_pred = unif_w_pred,\n",
    "            ip_pred     = ip_pred,\n",
    "            exp_w_pred  = exp_w_pred, \n",
    "            columnwise  = False,\n",
    "            verbose = False)\n",
    "        if n_keys == 2:\n",
    "            compare(\n",
    "                truth       = np.array(experiment_.Test), \n",
    "                unif_w_pred = np.array(json_obj[\"prediction\"][\"uniform\"]),\n",
    "                ip_pred = np.array(json_obj[\"prediction\"][\"interpolation\"]),\n",
    "                exp_w_pred  = None,#np.array(json_obj[\"prediction\"][\"exponential\"]), \n",
    "                columnwise  = False,\n",
    "                verbose = False)\n",
    "    return(experiment_)\n",
    "    \n",
    "##CHECK IF INTERPOLATION IS RUNNING CORRECTLY (#NOWAY)\n",
    "\n",
    "    \n",
    "    #print(ip_)\n",
    "    #exp_ = nrmse(pred_ = np.array(hi[\"prediction\"][\"exponential\"]), truth = hiObj.xTe, columnwise = False)\n",
    "    #diff += [ip_ - exp_]\n",
    "    #print(exp_)\n",
    "#sns.distplot(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Step 2: store hyper-parameter-results: Let's get some nice hyper-parameter plots.\n",
    "#TODO: Step 1: check if observers are correct:\n",
    "#TODO: fix\n",
    "\n",
    "\n",
    "def check_shape_obs(file = \"default\"):\n",
    "    if file == \"default\":\n",
    "        nf = get_new_filename(exp = exp, current = True)\n",
    "    else:\n",
    "        nf = file\n",
    "    with open(nf) as json_file: # 'non_exp_w.txt'\n",
    "        datt = json.load(json_file)\n",
    "    #datt = non_exp_best_args[\"dat\"]\n",
    "    #datt[\"obs_tr\"], datt[\"obs_te\"]   = np.array(datt[\"obs_tr\"]), np.array(datt[\"obs_te\"])\n",
    "    #datt[\"resp_tr\"], datt[\"resp_te\"] = np.array(datt[\"resp_tr\"]), np.array(datt[\"resp_te\"])\n",
    "    return(datt)\n",
    "\n",
    "def load_data(file = \"default\", print_lst = [\"nrmse\"], bp = None, verbose = True, enforce_exp = False):\n",
    "    if bp != None:\n",
    "        file = bp + file\n",
    "    if file == \"default\":\n",
    "        nf = get_new_filename(exp = exp, current = True)\n",
    "    else:\n",
    "        nf = file\n",
    "    with open(nf) as json_file: # 'non_exp_w.txt'\n",
    "        datt = json.load(json_file)\n",
    "    \n",
    "    for i in print_lst:\n",
    "        if verbose == True:\n",
    "            if enforce_exp == True:\n",
    "                assert len(list(print_lst.keys())) >= 3, \"exp not found: \" + file\n",
    "            print(datt[i])\n",
    "        \n",
    "    return(datt)\n",
    "\n",
    "\n",
    "\n",
    "#experiment.save_json(exp = False)\n",
    "#fp = bp + 'targetKhz:_0.01__obskHz:_0.01.txt'\n",
    "#fp = bp + 'targetKhz:_0.02__obskHz:_0.01.txt'\n",
    "def topline(spec_path, \n",
    "            base_path = \"/Users/hayden/Desktop/experiment_results/2k/medium/\",\n",
    "            #base_path = #\"./experiment_results/...\"\n",
    "            verbose = False,\n",
    "            print_filestructure = False):\n",
    "    \n",
    "    print(base_path)\n",
    "    fp = base_path + spec_path\n",
    "    \"\"\"\n",
    "    \n",
    "    targetKhz:_0.02__obskHz:_0.01.txt\n",
    "    │   │   │   ├── targetKhz:_0.5__obskHz:_0.5.txt\n",
    "    │   │   │   └── targetKhz:_0.5__obskHz:_1.0.txt\n",
    "    \"\"\"\n",
    "    hi = load_data(file = fp)\n",
    "    if print_filestructure == True:\n",
    "        for i in hi.keys():\n",
    "            print(i + \"/\")\n",
    "\n",
    "            if type(hi[i]) == dict:\n",
    "\n",
    "                for j in hi[i].keys():\n",
    "                    print(\"    \" +j)\n",
    "    if verbose == True:\n",
    "        print(\"DATA STRUCTURE: (it's a dict)\")\n",
    "        print(\"/n inputs:\")\n",
    "        print(hi[\"experiment_inputs\"])\n",
    "        print(hi[\"get_observer_inputs\"])\n",
    "\n",
    "        print(\"/n key saved values:\")\n",
    "        print(hi[\"best arguments\"])\n",
    "        print(hi[\"nrmse\"])\n",
    "    return(hi)\n",
    "\n",
    "def recover_test_set(json_obj):\n",
    "    \"\"\"\n",
    "    This function exists for an annoying reason: there is a shitty bug in my code.\n",
    "    A timeline is dropped and surely this is minor if we can just recover the index by exact indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    experiment_ = EchoStateExperiment(**json_obj[\"experiment_inputs\"])\n",
    "    \n",
    "    obs_inputs = json_obj[\"get_observer_inputs\"]\n",
    "    obs_inputs[\"method\"] = \"exact\"\n",
    "    \n",
    "\n",
    "    \n",
    "    obs_idx, resp_idx = json_obj[\"obs_idx\"], json_obj[\"resp_idx\"]\n",
    "    A_subset = experiment_.A.copy()\n",
    "    \n",
    "    # pred shape\n",
    "    pred_shape = np.array(json_obj[\"prediction\"][\"interpolation\"]); pred_shape = pred_shape.shape[0]                   \n",
    "    \n",
    "    A = experiment_.A\n",
    "    \n",
    "    train_len = (A.shape[0] - pred_shape)\n",
    "    Train_Tmp, Test_Tmp  = A[:train_len, resp_idx], A[train_len:, resp_idx]\n",
    "    \n",
    "    return(Train_Tmp, Test_Tmp)\n",
    "def load_p_result (path, bp = \"\"):\n",
    "    path = bp + path\n",
    "    with open(path, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return(b)\n",
    "def check_splits(complete_experiment_path_lst_):\n",
    "    for i in complete_experiment_path_lst_:\n",
    "        experiment_ = load_data(i, bp = './experiment_results/', verbose = False)\n",
    "        get_experiment(experiment_, compare = True)\n",
    "        #experiment_8_obj = get_experiment(experiment_8)\n",
    "        #get_experiment(i)#E, print_filestructure = False)\n",
    "                #base_path = \"./experiment_results/\")\n",
    "def fix_interpolation(exper_, method = \"linear\"):\n",
    "    hiObj = get_experiment(exper_, verbose = False, plot_split = False, compare_ = False)\n",
    "    if method == \"cubic\":\n",
    "        hiObj.interpolation_method = \"griddata-cubic\"\n",
    "    hiObj.runInterpolation()\n",
    "    exper_[\"prediction\"][\"interpolation\"] = hiObj.ip_res[\"prediction\"]\n",
    "    exper_[\"nrmse\"][\"interpolation\"] = hiObj.ip_res[\"nrmse\"]\n",
    "    print(hiObj.ip_res[\"nrmse\"])\n",
    "    return(exper_)\n",
    "#recover_test_set(hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mexperiment_results/medium\u001b[00m\r\n",
      "├── \u001b[01;34msplit_0.5\u001b[00m\r\n",
      "│   ├── targetHz_ctr:_1000targetKhz:_0.01__obskHz:_0.02.txt\r\n",
      "│   ├── targetHz_ctr:_1000targetKhz:_0.1__obskHz:_0.1.txt\r\n",
      "│   ├── targetHz_ctr:_1000targetKhz:_0.1__obskHz:_0.25.txt\r\n",
      "│   ├── targetHz_ctr:_1000targetKhz:_0.5__obskHz:_1.0.txt\r\n",
      "│   └── targetHz_ctr:_1000targetKhz:_1.0__obskHz:_1.0.txt\r\n",
      "└── \u001b[01;34msplit_0.9\u001b[00m\r\n",
      "    └── targetHz_ctr:_1000targetKhz:_0.5__obskHz:_0.5.txt\r\n",
      "\r\n",
      "2 directories, 6 files\r\n"
     ]
    }
   ],
   "source": [
    "! tree experiment_results/medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "path_lst = [ \n",
    "    #targ 500  kHz COMPLETE\n",
    "            #'split_0.5/targetHz_ctr:_1000targetKhz:_0.01__obskHz:_0.02.txt',\n",
    "            'split_0.5/targetHz_ctr:_1000targetKhz:_0.1__obskHz:_0.1.txt',\n",
    "            'split_0.5/targetHz_ctr:_1000targetKhz:_0.1__obskHz:_0.25.txt',\n",
    "            'split_0.5/targetHz_ctr:_1000targetKhz:_0.5__obskHz:_1.0.txt',\n",
    "            'split_0.5/targetHz_ctr:_1000targetKhz:_1.0__obskHz:_1.0.txt'            \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6cc4baf36d4e3c93fcb16b78b6a267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='experiment list, fixing interpolation...', max=3.0, style…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'interpolation': 0.5432129359381137, 'exponential': 0.4513923617281164, 'uniform': 0.4222064661472769}\n",
      "{'exponential': {'noise': 0.0001, 'llambda': 0.001, 'connectivity': 0.001, 'spectral_radius': 0.01, 'regularization': 11.825891651399001, 'leaking_rate': 1.0, 'n_nodes': 1000, 'random_seed': 123}, 'uniform': None}\n",
      "{'method': 'freq', 'split': 0.5, 'aspect': 0.9}\n",
      "{'size': 'medium', 'target_frequency': 1000, 'obs_hz': 100.0, 'target_hz': 100.0, 'verbose': False}\n",
      "None\n",
      "OBS IDX: [106, 107, 108, 109, 110, 90, 91, 92, 93, 94]\n",
      "Resp IDX: [95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n",
      "EXACT!\n",
      "OUTFILE: experiment_results/medium/split_0.5/N_Targidx_11N_Obsidx_10\n",
      "exp: False\n",
      "None\n",
      "OBS IDX: [106, 107, 108, 109, 110, 90, 91, 92, 93, 94]\n",
      "Resp IDX: [95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EchoStateExperiment' object has no attribute 'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/DL_LAB/Reservoir/MARIOS/PyFiles/experiment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'get_observer_inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'experiment_inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxTr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxTe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL_LAB/Reservoir/MARIOS/PyFiles/experiment.py\u001b[0m in \u001b[0;36mget_experiment\u001b[0;34m(json_obj, verbose, compare_, plot_split, librosa)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m### which line is missing?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EchoStateExperiment' object has no attribute 'prediction'"
     ]
    }
   ],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "for i in trange(len(path_lst), desc='experiment list, fixing interpolation...'): \n",
    "    if not i:\n",
    "        experiment_lst = []\n",
    "    experiment_dict = load_data(path_lst[i], bp = \"experiment_results/medium/\")\n",
    "    print(experiment_dict['best arguments'])\n",
    "    print(experiment_dict['get_observer_inputs'])\n",
    "    print(experiment_dict['experiment_inputs'])\n",
    "    experiment = get_experiment(experiment_dict)\n",
    "    \n",
    "    train_set, test_set = experiment.xTr, experiment.xTe\n",
    "\n",
    "    for model_ in list(experiment_dict[\"prediction\"].keys()):\n",
    "        pred_ = experiment_dict[\"prediction\"][model_]\n",
    "        corrected_nrmse = nrmse(pred_, test_set)\n",
    "        exper_[\"nrmse\"][model_] = corrected_nrmse\n",
    "    \n",
    "    experiment_lst.append(fix_interpolation(experiment_dict, method = \"linear\") )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd_lst = []\n",
    "for exper in experiment_lst:\n",
    "    print()\n",
    "    pd_lst.append(exper[\"nrmse\"]) #, index = [0]))\n",
    "hi = pd.melt(pd.DataFrame(pd_lst))\n",
    "hi.columns = [\"model\" ,\"nrmse\"]\n",
    "hi = hi.iloc[:11,:]\n",
    "sns.catplot(x = \"model\", y = \"nrmse\", data = hi, kind = \"bar\")\n",
    "plt.title(\"R for block prediction: large dataset\")\n",
    "plt.ylabel(\"R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_lst_unq = check_for_duplicates(path_lst, verbose = False)\n",
    "dict_lst_unq = check_for_duplicates(dict_lst, verbose = False)\n",
    "\n",
    "complete_experiment_path_lst = path_lst_unq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete_experiment_path_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(check_for_duplicates(complete_experiment_path_lst) != True), \"duplicates found\"\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "bp_ = \"\"# \"./experiment_results/\"\n",
    "\n",
    "for i, path in enumerate(complete_experiment_path_lst):\n",
    "    if not i:\n",
    "        experiment_lst, NOT_INCLUDED, NOT_YET_RUN  = [], [], []\n",
    "    \n",
    "    try:\n",
    "        spec_json = load_data(path, bp = bp_, verbose = False)\n",
    "        models_spec = list(spec_json[\"prediction\"].keys())\n",
    "        try:\n",
    "            assert len(models_spec) >= 3\n",
    "            assert \"exponential\" in models_spec\n",
    "\n",
    "            experiment_lst.append(spec_json)\n",
    "        except:\n",
    "            NOT_INCLUDED += [i]\n",
    "    except:\n",
    "        NOT_YET_RUN += [i]\n",
    "            \n",
    "\n",
    "# fix nrmse calculation AND interpolation\n",
    "for i in trange(len(experiment_lst), desc='experiment list, fixing interpolation...'): \n",
    "    exper_ = experiment_lst[i]\n",
    "    exper_obj = get_experiment(exper_)\n",
    "    \n",
    "    train_set, test_set = exper_obj.xTr, exper_obj.xTe\n",
    "    models_spec = list(exper_[\"prediction\"].keys())\n",
    "    \n",
    "    for model_ in models_spec:\n",
    "        pred_ = exper_[\"prediction\"][model_]\n",
    "        corrected_nrmse = nrmse(pred_, test_set)\n",
    "        exper_[\"nrmse\"][model_] = corrected_nrmse\n",
    "        \n",
    "    experiment_lst[i] = fix_interpolation(exper_, method = \"linear\") \n",
    "    \n",
    "\n",
    "#print(NOT_YET_RUN)\n",
    "if NOT_YET_RUN:\n",
    "    print(\"the following paths have not yet been run: \")\n",
    "    print(np.array(dict_lst_unq)[NOT_YET_RUN])\n",
    "   \n",
    "        \n",
    "if NOT_INCLUDED:\n",
    "    print(\"the following paths contain incomplete experiments: (only unif finished)\")\n",
    "    #print(np.array(path_lst_unq)[NOT_INCLUDED])\n",
    "    print(np.array(dict_lst_unq)[NOT_INCLUDED])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_YET_RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_INCLUDED = check_for_duplicates(NOT_INCLUDED)\n",
    "NOT_YET_RUN = check_for_duplicates(NOT_YET_RUN)\n",
    "print(\"total experiments completed: \" + str(len(experiment_lst)))\n",
    "print(\"total experiments half complete: \" + str(len(NOT_INCLUDED)))\n",
    "print(\"total experiments not yet run: \" + str(len(NOT_YET_RUN)))\n",
    "pct_complete = (len(experiment_lst))/(len(experiment_lst)+len(NOT_INCLUDED)*0.5 + len(NOT_YET_RUN)) * 100\n",
    "pct_complete  = str(round(pct_complete, 1))\n",
    "print(\"Percentage of tests completed: \" + str(pct_complete) + \"%\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_combination(ip, exp, Test, n = 10, optimize = True):\n",
    "    \n",
    "    if optimize == True:\n",
    "        nrmses = []\n",
    "        predictions = []\n",
    "        #(np.array(Test) + np.array(predictions[\"exponential\"])) / 2\n",
    "        vals =[]\n",
    "        for i in range(n):\n",
    "            a = i / n\n",
    "            b = 1 - a\n",
    "            hybrid_pred = ((1-a) * ip + a * exp)\n",
    "            predictions += [hybrid_pred]\n",
    "            nrmses += [nrmse(hybrid_pred , Test) ]\n",
    "            vals += [a]\n",
    "            #nrmse(predictions[\"hybrid\"], Test) \n",
    "        idx = np.argmin(nrmses) \n",
    "        print(nrmses)\n",
    "        print(\"A!! \" + str(vals[idx]))\n",
    "        best_prediction = predictions[idx]\n",
    "        best_nrmse      = nrmses[idx]\n",
    "        return(best_prediction, best_nrmse)\n",
    "    else:\n",
    "        hybrid_pred  = (0.5 * ip) + (0.5 * exp)\n",
    "        hybrid_nrmse = nrmse(hybrid_pred , Test)\n",
    "        return(hybrid_pred, hybrid_nrmse)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add hybrids\n",
    "for i in list(range(len(experiment_lst))):\n",
    "    experiment_lst[i]\n",
    "    predictions_= experiment_lst[i][\"prediction\"]\n",
    "    Train, Test = recover_test_set(experiment_lst[i])\n",
    "    #hybrid_pred_, hybrid_R = optimize_combination(np.array(predictions_[\"interpolation\"]),\n",
    "    #                                                                  np.array(predictions_[\"exponential\"]),\n",
    "    #                                                                  Test, optimize = False)\n",
    "    experiment_lst[i][\"nrmse\"][\"hybrid\"]      = hybrid_R\n",
    "    experiment_lst[i][\"prediction\"][\"hybrid\"] = hybrid_pred_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_lst[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_IP = False\n",
    "\n",
    "\n",
    "def quick_dirty_convert(lst):\n",
    "    if IGNORE_IP == True:\n",
    "        lst *= 2\n",
    "    else:\n",
    "        lst *= 4\n",
    "    pd_ = pd.DataFrame(np.array(lst).reshape(-1,1))\n",
    "    return(pd_)\n",
    "    \n",
    "\n",
    "idx_lst = list(range(len(experiment_lst)))\n",
    "#idx_lst *= 3\n",
    "#idx_lst = pd.DataFrame(np.array(idx_lst).reshape(-1,1))\n",
    "\n",
    "idx_lst = quick_dirty_convert(idx_lst)\n",
    "\n",
    "obs_hz_lst, targ_hz_lst, targ_freq_lst = [], [], []\n",
    "\n",
    "for i, experiment in enumerate(experiment_lst):\n",
    "    #print(experiment['experiment_inputs'].keys())\n",
    "    targ_hz = experiment[\"experiment_inputs\"][\"target_hz\"]\n",
    "    obs_hz  = experiment[\"experiment_inputs\"][\"obs_hz\"]\n",
    "    targ_freq = experiment[\"experiment_inputs\"]['target_frequency']\n",
    "    \n",
    "    if experiment[\"experiment_inputs\"][\"target_hz\"] < 1:\n",
    "        targ_hz *= 1000*1000\n",
    "        obs_hz  *= 1000*1000\n",
    "    obs_hz_lst  += [obs_hz]\n",
    "    targ_hz_lst += [targ_hz]\n",
    "    targ_freq_lst += [targ_freq]\n",
    "    \n",
    "        \n",
    "    hz_line = {\"target hz\" : targ_hz }\n",
    "    hz_line = Merge(hz_line , {\"obs hz\" : obs_hz })\n",
    "    \n",
    "    #print(hz_line)\n",
    "    df_spec= experiment[\"nrmse\"]\n",
    "    \n",
    "    #df_spec = Merge(experiment[\"nrmse\"], {\"target hz\": targ_hz})\n",
    "    df_spec = pd.DataFrame(df_spec, index = [0])\n",
    "    \n",
    "    df_spec_rel = df_spec.copy()\n",
    "    #/df_spec_diff[\"uniform\"]\n",
    "    #df_spec_diff[\"rc_diff\"]\n",
    "    \n",
    "    if IGNORE_IP == True:\n",
    "        df_spec_rel = df_spec_rel / experiment[\"nrmse\"][\"uniform\"]#\n",
    "    else:\n",
    "        df_spec_rel = df_spec_rel / experiment[\"nrmse\"][\"interpolation\"]\n",
    "\n",
    "   \n",
    "    \n",
    "    #print( df_spec_rel)\n",
    "    #print(experiment[\"experiment_inputs\"].keys())\n",
    "    if i == 0:\n",
    "        df      = df_spec\n",
    "        df_rel  = df_spec_rel\n",
    "\n",
    "        \n",
    "    else:\n",
    "        df = pd.concat([df, df_spec])\n",
    "        df_rel = pd.concat([df_rel, df_spec_rel])\n",
    "\n",
    "\n",
    "df_net = df_rel.copy()\n",
    "        \n",
    "obs_hz_lst, targ_hz_lst = quick_dirty_convert(obs_hz_lst), quick_dirty_convert(targ_hz_lst)\n",
    "targ_freq_lst = quick_dirty_convert(targ_freq_lst)\n",
    "#display(df)\n",
    "if IGNORE_IP == True:\n",
    "    df_rel = df_rel.drop(columns = [\"interpolation\"])\n",
    "    df  = df.drop(columns = [\"interpolation\"])\n",
    "#df_rel  = df_rel.drop(columns = [\"hybrid\"])\n",
    "#df      = df.drop(    columns = [\"hybrid\"])\n",
    "\n",
    "df, df_rel = pd.melt(df), pd.melt(df_rel)\n",
    "df  = pd.concat( [idx_lst, df,  obs_hz_lst, targ_hz_lst, targ_freq_lst] ,axis = 1)\n",
    "\n",
    "df_rel = pd.concat( [idx_lst, df_rel,  obs_hz_lst, targ_hz_lst, targ_freq_lst], axis = 1)\n",
    "\n",
    "#df_diff = pd.concat( [idx_lst, df_diff,  obs_hz_lst, targ_hz_lst, targ_freq_lst], axis = 1)\n",
    "\n",
    "col_names = [\"experiment\", \"model\", \"nrmse\", \"obs hz\", \"target hz\", \"target freq\" ]\n",
    "df.columns, df_rel.columns    = col_names, col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = df[df[\"model\"] == \"uniform\"]\n",
    "df_diff.model = \"diff\"\n",
    "nrmse_ = (df[df[\"model\"] == \"exponential\"][\"nrmse\"].values - df_diff[\"nrmse\"].values) * 100\n",
    "df_diff.nrmse = nrmse_\n",
    "def plot_loss_reduction():\n",
    "\n",
    "    df_diff = df[df[\"model\"] == \"uniform\"]\n",
    "    df_diff.model = \"diff\"\n",
    "    #df_diff[\"nrmse\"] = df_diff[\"nrmse\"] - df[df[\"model\"] == \"exponential\"][\"nrmse\"]\n",
    "\n",
    "    #df[df[\"model\"] == \"exponential\"] \n",
    "\n",
    "    nrmse_ = (df[df[\"model\"] == \"exponential\"][\"nrmse\"].values - df_diff[\"nrmse\"].values) * 100\n",
    "    df_diff.nrmse = nrmse_\n",
    "    pct = round(np.mean(nrmse_ < 0) * 100,2)\n",
    "    print(\"odds of loss reduction with exponential weights vs uniform weights: \" + str(pct) + \"%\")\n",
    "    print(\"mean % loss change: \" + str(round(np.mean(nrmse_))) + \"%\")\n",
    "\n",
    "    #sns.catplot(x = \"model\", y = \"nrmse\", data = df_diff)\n",
    "    fig, ax = plt.subplots(1,1,figsize = (10, 6))\n",
    "    plt.xlabel(\"%change in loss\")\n",
    "    plt.ylabel(\"density\")\n",
    "    sns.kdeplot(df_diff[\"nrmse\"], shade = True)\n",
    "    plt.axvline(x=0, color = \"black\", label = \"zero\")\n",
    "    plt.axvline(x=np.mean(df_diff[\"nrmse\"]), color = \"red\", label = \"mean loss reduction\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_loss_reduction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_reduction2d(xx = \"target hz\"):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (6,6))\n",
    "    sns.kdeplot(df_diff[xx], df_diff[\"nrmse\"],\n",
    "                     cmap=\"Blues\", shade=True, shade_lowest=False, ax = ax)#, alpha = 0.5)\n",
    "    #plt.ayvline(y=0, color = \"black\", label = \"zero\")\n",
    "    sns.scatterplot(x = xx, y = \"nrmse\", data = df_diff, ax = ax,  linewidth=0, color = \"black\", alpha = 0.4)\n",
    "    plt.title(\"2d kde plot: nrmse vs target hz\")\n",
    "    plt.axhline(y=0.5, color='black', linestyle='-')\n",
    "    ax.set_ylabel(\"pct loss exp vs unif RC\")\n",
    "plot_loss_reduction2d()\n",
    "plot_loss_reduction2d(xx = \"obs hz\")\n",
    "plot_loss_reduction2d(xx = \"target freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(experiment_lst[0][\"prediction\"][\"uniform\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_lst[0]['get_observer_inputs'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred_, truth, columnwise = True, typee = \"L1\"):\n",
    "    \"\"\"\n",
    "    inputs should be numpy arrays\n",
    "    variables:\n",
    "    pred_ : the prediction numpy matrix\n",
    "    truth : ground truth numpy matrix\n",
    "    columnwise: bool if set to true takes row-wise numpy array (assumes reader thinks of time as running left to right\n",
    "        while the code actually runs vertically.)\n",
    "        \n",
    "    This is an average of the loss across that point, which we must do if we are to compare different sizes of data.\n",
    "\n",
    "    \"\"\"\n",
    "    pred_ = np.array(pred_)\n",
    "    truth = np.array(truth)\n",
    "    assert pred_.shape == truth.shape, \"pred shape: \" + str(pred_.shape) + \" , \" + str(truth.shape)\n",
    "    def L2_(pred, truth):\n",
    "        resid = pred - truth\n",
    "        return(resid**2)\n",
    "    def L1_(pred, truth):\n",
    "        resid = pred - truth\n",
    "        return(abs(resid))\n",
    "    def R_(pred, truth):\n",
    "        loss = ((pred - truth)**2)/(truth**2)\n",
    "        loss = np.sqrt(loss)\n",
    "        return(loss)\n",
    "        \n",
    "    assert typee in [\"L1\", \"L2\", \"R\"]\n",
    "    if typee == \"L1\":\n",
    "        f = L1_\n",
    "    elif typee == \"L2\":\n",
    "        f = L2_\n",
    "    elif typee == \"R\":\n",
    "        f = R_\n",
    "        \n",
    "    loss_arr = f(pred_, truth )  \n",
    "    if columnwise == True:\n",
    "        \n",
    "        loss_arr = np.mean(loss_arr, axis = 1)\n",
    "\n",
    "    return(loss_arr)\n",
    "\n",
    "\n",
    "def get_prediction(model, json_obj):\n",
    "    \n",
    "    experiment_ = EchoStateExperiment(**json_obj[\"experiment_inputs\"])\n",
    "    \n",
    "    obs_inputs = json_obj[\"get_observer_inputs\"]\n",
    "    obs_inputs[\"method\"] = \"exact\"\n",
    "    \n",
    "    experiment_.obs_idx, experiment_.resp_idx  = json_obj[\"obs_idx\"], json_obj[\"resp_idx\"]\n",
    "    \n",
    "    experiment_.get_observers(**obs_inputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(json_obj.keys())\n",
    "    best_args =  json_obj['best arguments'][model]\n",
    "\n",
    "    esn = EchoStateNetwork(**best_args,\n",
    "        obs_idx  = json_obj['obs_idx'],\n",
    "        resp_idx = json_obj['resp_idx'])\n",
    "    Train, Test = recover_test_set(json_obj)\n",
    "    if model == \"uniform\":\n",
    "        esn.exp_weights = False\n",
    "    else:\n",
    "        esn.exp_weights = True\n",
    "        \n",
    "    \n",
    "    \n",
    "    experiment_.already_trained(json_obj[\"best arguments\"][model], exponential = False)\n",
    "    return(experiment_.prediction)\n",
    "\n",
    "\n",
    "def build_loss_df(  #split = 0.5, \n",
    "              exp_json_lst = experiment_lst, \n",
    "              loss_ = \"L1\", \n",
    "              columnwise = True,\n",
    "              relative = True,\n",
    "              rolling = None,\n",
    "              models = [\"uniform\", \"exponential\", \"interpolation\"],#, \"hybrid\"],\n",
    "              silent = True,\n",
    "              hybrid = True):\n",
    "    #exp stands for experiment here, not exponential\n",
    "    \"\"\"\n",
    "    columnwise == False means don't take the mean.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    for i in trange(len(exp_json_lst), desc='processing path list...'):\n",
    "        exp_json = exp_json_lst[i]\n",
    "        existing_models = exp_json[\"nrmse\"].keys()\n",
    "        \n",
    "        exp_obj = get_experiment(exp_json, compare_ = False, verbose = False, plot_split = False)\n",
    "        split_ = exp_json['get_observer_inputs'][\"split\"]\n",
    "        \n",
    "        #construct the required data frame and caculate the nrmse from the predictions:\n",
    "        train_, test_ = exp_obj.xTr, exp_obj.xTe\n",
    "       \n",
    "            \n",
    "        if \"hybrid\" in exp_json[\"prediction\"]:\n",
    "            del exp_json[\"prediction\"]['hybrid'] \n",
    "            del exp_json[\"nrmse\"]['hybrid'] \n",
    "            \n",
    "\n",
    "\n",
    "        if i == 0:\n",
    "            A = exp_obj.A\n",
    "            \n",
    "        test_len = test_.shape[0]\n",
    "        train_len = A.shape[0] - test_len\n",
    "        time_lst = []\n",
    "        time_lst_one_run = list(exp_obj.T[train_len:].reshape(-1,))\n",
    "        \n",
    "        if columnwise ==  False:\n",
    "            time_lst_one_run *= test_.shape[1]\n",
    "        \n",
    "        for j, model in enumerate(existing_models):\n",
    "            if model in models:\n",
    "                #R is new name for nrmseccccc\n",
    "                shared_args = {\n",
    "                    \"pred_\" : exp_json[\"prediction\"][model],\n",
    "                    \"truth\": test_,\n",
    "                    \"columnwise\" : columnwise\n",
    "                }\n",
    "\n",
    "                L1_spec = loss(**shared_args, typee = \"L1\")\n",
    "                L2_spec = loss(**shared_args, typee = \"L2\")\n",
    "                R_spec = loss(**shared_args, typee = \"R\")\n",
    "\n",
    "                if columnwise == False:\n",
    "                    #what does columnwise = True even mean?\n",
    "                    L1_spec = np.mean(L1_spec.T, axis = 0)\n",
    "                    L2_spec = np.mean(L2_spec.T, axis = 0)\n",
    "                    R_spec  =  np.mean(R_spec.T, axis = 0)\n",
    "                L2_spec = list(L2_spec.reshape(-1,))\n",
    "                R_spec = list(R_spec.reshape(-1,))\n",
    "\n",
    "                #idx_lst = list(range(test_len)\n",
    "\n",
    "                L1_spec = pd.DataFrame({model : L1_spec})\n",
    "                time_lst += time_lst_one_run\n",
    "\n",
    "                if j == 0:\n",
    "                    rDF_spec = L1_spec\n",
    "                    L2_lst = L2_spec \n",
    "                    R_lst = R_spec \n",
    "                else:\n",
    "                    rDF_spec = pd.concat([rDF_spec, L1_spec], axis = 1)\n",
    "                    L2_lst += L2_spec\n",
    "                    R_lst += R_spec\n",
    "            \n",
    "            \n",
    "        time_ = pd.Series(time_lst)\n",
    "        rDF_spec = pd.melt(rDF_spec)\n",
    "\n",
    "        rDF_spec[\"L2_loss\"] = L2_lst\n",
    "        rDF_spec[\"R\"] = R_lst\n",
    "        rDF_spec[\"split\"] = split_\n",
    "        rDF_spec[\"time\"] = time_ \n",
    "        rDF_spec[\"experiment #\"] = count\n",
    "        rDF_spec.columns = [\"model\", \"L1_loss\", \"L2_loss\", \"R\", \"split\",  \"time\",\"experiment #\"]\n",
    "\n",
    "        if i == 0:\n",
    "            rDF = rDF_spec\n",
    "        else:\n",
    "            rDF = pd.concat([rDF, rDF_spec], axis = 0)\n",
    "            #count+=1\n",
    "            \n",
    "    if silent != True:\n",
    "        display(rDF)\n",
    "    return(rDF)\n",
    "\n",
    "def loss_plot(rDF, rolling, split, loss = \"L2\", relative = False, include_ip = False, hybrid = False):\n",
    "    if not hybrid:\n",
    "        rDF = rDF[rDF.model != \"hybrid\"]\n",
    "    if include_ip == False:\n",
    "        rDF = rDF[(rDF.model == \"uniform\") | (rDF.model == \"exponential\")]\n",
    "    \n",
    "    rDF = rDF[rDF.split == split]\n",
    "    if loss == \"L2\":\n",
    "        LOSS = rDF.L2_loss\n",
    "    elif loss ==\"L1\":\n",
    "        LOSS = rDF.L1_loss\n",
    "    elif loss ==\"R\":\n",
    "        LOSS = rDF.L1_loss\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "    if relative == True:\n",
    "        diff = rDF[rDF.model == \"exponential\"][\"loss\"].values.reshape(-1,) - rDF[rDF.model == \"uniform\"][\"loss\"].values.reshape(-1,)\n",
    "\n",
    "        df_diff = rDF[rDF.model == \"uniform\"].copy()\n",
    "        df_diff.model = \"diff\"\n",
    "        df_diff.loss = diff\n",
    "        \n",
    "        sns.lineplot( x = \"time\", y = \"loss\" , hue = \"model\" , data = df_diff)\n",
    "        ax.set_title(loss_ + \" loss vs time relative\")\n",
    "    else:\n",
    "    \n",
    "        display(rDF)\n",
    "        if rolling != None:\n",
    "            sns.lineplot( x = \"time\", y = LOSS.rolling(rolling).mean() , hue = \"model\" , data = rDF)\n",
    "            #sns.scatterplot( x = \"time\", y = \"loss\" , hue = \"model\" , data = rDF, alpha = 0.005, edgecolor= None)\n",
    "        else:\n",
    "            sns.lineplot( x = \"time\", y = loss , hue = \"model\" , data = rDF)\n",
    "        ax.set_title( \"mean \" + loss + \" loss vs time for all RC's, split = \" + str(split))\n",
    "        ax.set_ylabel(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args= {\"relative\": False,\n",
    "       \"columnwise\" : True,\n",
    "       \"rolling\" : 9,\n",
    "        \"models\" : [\"uniform\", \"exponential\", \"interpolation\", \"hybrid\"]\n",
    "}\n",
    "Loss_df = build_loss_df(**args, loss_ = \"L1\")\n",
    "#L2_loss_df = loss_df(**args, loss_ = \"L2\")\n",
    "#L1_loss_df = loss_df(relative = False, columnwise = False, rolling = 10, loss_ = \"L1\", models = [\"uniform\", \"exponential\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_plot(Loss_df, rolling = 7, split = 0.5, loss = \"L1\")\n",
    "loss_plot(Loss_df, rolling = None, split = 0.5, loss = \"R\")\n",
    "loss_plot(Loss_df, rolling = None, split = 0.5, loss = \"R\", include_ip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_plot(Loss_df, rolling = 7, split = 0.9, loss = \"L1\")\n",
    "loss_plot(Loss_df, rolling = 30, split = 0.9, loss = \"R\")\n",
    "loss_plot(Loss_df, rolling = 30, split = 0.9, loss = \"R\", include_ip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_plot(Loss_df, rolling = 1, split = 0.5, loss = \"R\", include_ip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nrmse_kde_2d(xx = \"target hz\", \n",
    "                      log = True, \n",
    "                      alph = 1, \n",
    "                      black_pnts = True, \n",
    "                      models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"},\n",
    "                      enforce_bounds = False,\n",
    "                      target_freq = None):\n",
    "    \"\"\"\n",
    "    #todo description\n",
    "    \"\"\"\n",
    "    if target_freq != None:\n",
    "        df_spec = df[df[\"target freq\"] == target_freq]\n",
    "    else:\n",
    "        df_spec = df.copy()\n",
    "            \n",
    "    \n",
    "    def plot_(model_, colorr, alph = alph,  black_pnts =  black_pnts):\n",
    "        if colorr == \"Blues\":\n",
    "            color_ = \"blue\"\n",
    "        elif colorr == \"Reds\":\n",
    "            color_ = \"red\"\n",
    "        elif colorr == \"Greens\":\n",
    "            color_ = \"green\"\n",
    "            \n",
    "        df_ = df_spec[df_spec.model == model_] #df_ip  = df[df.model == \"interpolation\"]\n",
    "        \n",
    "        #display(df_)\n",
    "            \n",
    "        \n",
    "        hi = df_[\"nrmse\"]\n",
    "        cap = 1\n",
    "        if log == True:\n",
    "            hi = np.log(hi)/ np.log(10)\n",
    "            cap = np.log(cap) / np.log(10)\n",
    "        \n",
    "        \n",
    "        sns.kdeplot(df_[xx], hi, cmap= colorr, \n",
    "                    shade=True, shade_lowest=False, ax = ax, label = model_, alpha = alph)#, alpha = 0.5)\n",
    "        \n",
    "        if  black_pnts == True:\n",
    "            col_scatter = \"black\"\n",
    "        else:\n",
    "            col_scatter = color_\n",
    "        \n",
    "        sns.scatterplot(x = xx, y = hi, data = df_,  linewidth=0, \n",
    "                        color = col_scatter, alpha = 0.4, ax = ax)\n",
    "        \n",
    "        plt.title(\"2d kde plot: nrmse vs \" + xx)\n",
    "        \n",
    "        plt.axhline(y=cap, color=color_, linestyle='-', label = \"mean \" + str(model_), alpha = 0.5)\n",
    "        sns.lineplot(y = hi, x = xx, data = df_ , color = color_)#, alpha = 0.2)\n",
    "        if enforce_bounds == True:\n",
    "            ax.set_ylim(0,1)\n",
    "        if log == True:\n",
    "            ax.set_ylabel(\"log( NRMSE) \")\n",
    "        else: \n",
    "            ax.set_ylabel(\"NRMSE\")\n",
    "            \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (12,6))\n",
    "    for model in list(models.keys()):\n",
    "        print(model)\n",
    "        plot_(model, models[model], alph = alph)\n",
    "    #plot_(\"interpolation\", \"Blues\")\n",
    "    #plot_(\"exponential\", \"Reds\", alph = alph)\n",
    "    \n",
    "def kde_plots( target_freq = None, \n",
    "               log = False, \n",
    "               model = \"uniform\", \n",
    "               models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"},\n",
    "               enforce_bounds = True,\n",
    "               split = None):\n",
    "    \"\"\"\n",
    "    HEATMAP EXAMPLE:\n",
    "                     enforce_bounds = True)\n",
    "    flights = flights.pivot(\"month\", \"year\", \"passengers\") #y, x, z\n",
    "    ax = sns.heatmap(flights)\n",
    "    plot_nrmse_kde_2d(**additional_arguments, \n",
    "                      models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"})\n",
    "    \n",
    "    plot_nrmse_kde_2d(xx = \"obs hz\", **additional_arguments, \n",
    "                      models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Reds\", \"uniform\" : \"Blues\"})\n",
    "    \"\"\"\n",
    "    \n",
    "    additional_arguments ={ \"black_pnts\" : False, \n",
    "                           \"alph\" : 0.3, \n",
    "                           \"target_freq\" : target_freq}    \n",
    "    \n",
    "    cmap = \"coolwarm\"\n",
    "    \n",
    "   \n",
    "    def add_noise(np_array, log = log):\n",
    "        sizee = len(np_array)\n",
    "        x =  np.random.randint(100, size = sizee) + np_array \n",
    "        \n",
    "        return(x)\n",
    "    \n",
    "    nrmse_dict = {}\n",
    "    \n",
    "    for i, model in enumerate([\"uniform\", \"exponential\", \"interpolation\"]):\n",
    "        df_ = df[df.model == model ]\n",
    "        \n",
    "        xx, yy = add_noise(df_[\"target hz\"]), add_noise(df_[\"obs hz\"])\n",
    "\n",
    "        nrmse= df_[\"nrmse\"]\n",
    "        if log == True:\n",
    "            print(\"hawabunga\")\n",
    "            nrmse = np.log(nrmse)\n",
    "        nrmse_dict[model] = nrmse\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    nrmse_diff = nrmse_dict[\"exponential\"].values.reshape(-1,)  - nrmse_dict[\"uniform\"].values.reshape(-1,) \n",
    "    print(\"(+): \" + str(np.sum((nrmse_diff > 0)*1)))\n",
    "    \n",
    "    print(\"(-): \" + str(np.sum((nrmse_diff < 0)*1)))\n",
    "    \n",
    "    \n",
    "    display(nrmse_diff)\n",
    "    xx, yy = add_noise(df_[\"target hz\"]), add_noise(df_[\"obs hz\"])\n",
    "    #sns.distplot(nrmse_diff, ax = ax[2])\n",
    "    sns.scatterplot(x = xx, y = yy, data = df_, ax = ax[2], palette=cmap, alpha = 0.9, s = 50, hue = nrmse_diff) #size = nrmse,\n",
    "    ax[2].set_title(\" diff: exponential - uniform\" )\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_nrmse_kde_2d(**additional_arguments, log = False, \n",
    "                      models = models, #{\"exponential\" : \"Reds\", \"uniform\" : \"Blues\", \"interpolation\" : \"Greens\"},\n",
    "                     enforce_bounds = True)\n",
    "    \n",
    "    \n",
    "    plot_nrmse_kde_2d(xx = \"obs hz\", **additional_arguments, log = False, \n",
    "                       models = models, #{\"exponential\" : \"Reds\", \"uniform\" : \"Blues\", \"interpolation\" : \"Greens\"},\n",
    "                       enforce_bounds = True)\n",
    "    \n",
    "               \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "kde_plots(models = {\"interpolation\" : \"Greens\", \"hybrid\" : \"Reds\"})#, \"uniform\" : \"Blues\"},)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_plots(models = {\"interpolation\" : \"Greens\", \"exponential\" : \"Blues\", \"uniform\" : \"Reds\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: move the legend to the lower right corner\n",
    "kde_plots(target_freq = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exper_ in experiment_lst:\n",
    "    get_experiment(exper_,  compare_ = True, plot_split = False  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_plots(target_freq = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    " \n",
    "def optimize_combination(ip, exp, Test, n = 10, optimize = True):\n",
    "    \n",
    "    if optimize == True:\n",
    "        nrmses = []\n",
    "        predictions = []\n",
    "        #(np.array(Test) + np.array(predictions[\"exponential\"])) / 2\n",
    "        vals =[]\n",
    "        for i in range(n):\n",
    "            a = i / n\n",
    "            b = 1 - a\n",
    "            hybrid_pred = ((1-a) * ip + a * exp)\n",
    "            predictions += [hybrid_pred]\n",
    "            nrmses += [nrmse(hybrid_pred , Test) ]\n",
    "            vals += [a]\n",
    "            #nrmse(predictions[\"hybrid\"], Test) \n",
    "        idx = np.argmin(nrmses) \n",
    "        print(nrmses)\n",
    "        print(\"A!! \" + str(vals[idx]))\n",
    "        best_prediction = predictions[idx]\n",
    "        best_nrmse      = nrmses[idx]\n",
    "        return(best_prediction, best_nrmse)\n",
    "    else:\n",
    "        hybrid_pred  = (0.5 * ip) + (0.5 * exp)\n",
    "        hybrid_nrmse = nrmse(hybrid_pred , Test)\n",
    "        return(hybrid_pred, hybrid_nrmse)\n",
    "    \n",
    "\n",
    "def show_images(exper_, aspect = 10, sigma = 1, method = \"heatmap\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Train, Test = recover_test_set(exper_)\n",
    "    \n",
    "    exper_ = fix_interpolation(exper_)\n",
    "    \n",
    "    predictions = exper_[\"prediction\"]\n",
    "    #print(predictions)\n",
    "    for i in predictions.values():\n",
    "        a = np.array(i)\n",
    "        #print(a.shape)\n",
    "        \n",
    "    nrmses      = exper_[\"nrmse\"]\n",
    "    predictions[\"ground_Truth\"]  = Test\n",
    "    nrmses[\"ground_Truth\"]       = 0\n",
    "    #predictions[\"ground_Truth_smooth\"]  = gaussian_filter(Test, sigma=sigma)\n",
    "    #nrmses[\"ground_Truth_smooth\"]       = 0\n",
    "    \n",
    "    fig, ax = plt.subplots(2,3, figsize = (16, 10))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, (key, value) in enumerate(predictions.items()):\n",
    "        print(i)\n",
    "        print(key)\n",
    "        arr = np.array(value)\n",
    "        full_arr = np.concatenate((Train, arr), axis = 0)\n",
    "        #arr = full_arr\n",
    "        \n",
    "        if method != \"heatmap\":\n",
    "            plt.imshow(arr.T, aspect = aspect)\n",
    "            plt.title(key +\" R: \" + str(nrmses[key]))\n",
    "            plt.subplot(2,3,i)\n",
    "            plt.show()       \n",
    "        else:\n",
    "            sns.heatmap(full_arr.T, ax = ax[i])\n",
    "            ax[i].set_title(key +\" R: \" + str(nrmses[key]))\n",
    "            #plt.show()\n",
    "    #plt.show()\n",
    "show_images(experiment_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(experiment_lst[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[1])\n",
    "ax[0].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "ax[1].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "ax[0].set_ylabel(\"NRMSE\"); ax[1].set_ylabel(\"NRMSE\")\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[1])\n",
    "ax[0].set_title(\"Relative NRMSE vs Interpolation model across different RC's\")\n",
    "ax[1].set_title(\"Relavite NRMSE vs Interpolation model across different RC's\")\n",
    "ax[0].set_ylabel(\"Relative NRMSE\"); ax[1].set_ylabel(\"Relative NRMSE\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (7,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df_diff, ax = ax)\n",
    "#sns.boxplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[1])\n",
    "ax.set_title(\"Relative NRMSE: ([exp nrmse] -  [unif nrmse])/[unif_nrmse] * 100\")\n",
    "ax.set_ylabel(\"Relative NRMSE\"); ax.set_ylabel(\"Relative NRMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = df[\"nrmse\"] #df[\"nrmse\"]np.log(df[\"nrmse\"])\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5, legend = False)\n",
    "sns.lineplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "cap = np.log(1.0)/np.log(10)\n",
    "plt.axhline(y=cap, color=\"black\", linestyle='-', label = \"mean \" + str(model_), alpha = 0.5)\n",
    "plt.ylabel(\"log( NRMSE)\")\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5, legend = False)\n",
    "sns.lineplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "cap = np.log(1.0)/np.log(10)\n",
    "plt.axhline(y=cap, color=\"black\", linestyle='-', label = \"mean \" + str(model_), alpha = 0.5)\n",
    "plt.ylabel(\"log( NRMSE)\")\n",
    "plt.ylim((-1.5,cap))\n",
    "\n",
    "\n",
    "\n",
    "hi = df[\"nrmse\"]\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5, legend = False)\n",
    "sns.lineplot( y = hi, x = \"target hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "plt.ylabel(\"NRMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = np.log(df[\"nrmse\"])\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "sns.scatterplot( y = hi, x = \"obs hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "sns.lineplot( y = hi, x = \"obs hz\", data = df, hue = \"model\", alpha = 0.5)\n",
    "ax.set_ylabel(\"Log ( NRMSE )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target freq\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "plt.ylim((0,1.5))\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target freq\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_plot(experiment_lst):\n",
    "    \"\"\"\n",
    "    Let's visualize the hyper-parameter plots.\n",
    "    \"\"\"\n",
    "    log_vars = [\"noise\", \"connectivity\", \"regularization\", \"llambda\"]\n",
    "    \n",
    "    for i, experiment in enumerate(experiment_lst):\n",
    "        df_spec_unif = pd.DataFrame(experiment[\"best arguments\"][\"uniform\"], index = [0])\n",
    "        df_spec_exp  = pd.DataFrame(experiment[\"best arguments\"][\"exponential\"], index = [0])\n",
    "        \n",
    "        if not i:\n",
    "            df_unif = df_spec_unif\n",
    "            df_exp = df_spec_exp\n",
    "        else:\n",
    "            df_unif = pd.concat([df_unif, df_spec_unif])\n",
    "            df_exp = pd.concat([df_exp, df_spec_exp])\n",
    "\n",
    "    unif_vars = [\"connectivity\", \"regularization\", \"leaking_rate\", \"spectral_radius\"]\n",
    "    exp_vars  = [\"llambda\", \"noise\"]\n",
    "    df_unif = df_unif[unif_vars]\n",
    "    df_exp = df_exp[unif_vars + exp_vars]\n",
    "    \n",
    "    for i in list(df_unif.columns):\n",
    "        if i in log_vars:\n",
    "            df_unif[i] = np.log(df_unif[i])/np.log(10)\n",
    "            \n",
    "    for i in list(df_exp.columns):\n",
    "        if i in log_vars:\n",
    "            df_exp[i] = np.log(df_exp[i])/np.log(10)\n",
    "    \n",
    "    \n",
    "    #display(df_unif)\n",
    "    \n",
    "    sns.catplot(data = df_unif)\n",
    "    plt.title(\"uniform RC hyper-parameters\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    sns.catplot(data = df_exp)\n",
    "    plt.title(\"exponential RC hyper-parameters\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    #display(df_exp)\n",
    "   \n",
    "hyper_parameter_plot(experiment_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, experiment in enumerate(experiment_lst):\n",
    "    #print(experiment['get_observer_inputs'].keys())\n",
    "    split = experiment['get_observer_inputs'][\"split\"]\n",
    "    targ_hz = experiment['experiment_inputs'][\"target_hz\"]\n",
    "    targ_idx_LB, targ_idx_UB = experiment[\"resp_idx\"][0], experiment[\"resp_idx\"][-1]\n",
    "    obs_hz = experiment['experiment_inputs'][\"obs_hz\"]\n",
    "    f = np.array(experiment_8_obj.f)\n",
    "    obs_idx = experiment[\"obs_idx\"] \n",
    "\n",
    "    obs_idx  = [int(j) for j in experiment[\"obs_idx\"] ]\n",
    "    obs_freq = [max(f) - f[j] for j in obs_idx]\n",
    "    \n",
    "    \n",
    "    print(\"\\nexperiment: \" + str(i) + \", target hz: \" + str(targ_hz) + \", obs hz: \" + str(obs_hz) +\n",
    "         \", split: \" + str(split))\n",
    "\n",
    "    \n",
    "    print(\"target idx: [\" + str(targ_idx_LB) + \", \" + str(targ_idx_UB) + \"]\")\n",
    "    print(\"target freq: [\" + str(max(f) - f[targ_idx_LB]) + \", \" + str(max(f) - f[targ_idx_UB]) + \"]\")\n",
    "    print(\"obs idx: \" + str(obs_idx))\n",
    "    print(\"obs freq: \" + str(obs_freq))\n",
    "    print(experiment_8_obj.A.shape[0] - np.array(experiment[\"prediction\"][\"interpolation\"]).shape[0])\n",
    "    print(experiment_8_obj.A.shape[0])\n",
    "    #print(experiment[\"resp_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_exp_weights(json_obj, llambda = None):\n",
    "    print(json_obj.keys())\n",
    "    esn_ = EchoStateNetwork(**json_obj[\"best arguments\"][\"exponential\"], plot = True)\n",
    "    esn_.obs_idx  = json_obj[\"obs_idx\"]\n",
    "    esn_.resp_idx = json_obj[\"resp_idx\"]\n",
    "    if llambda != None:\n",
    "        esn_.llambda = llambda\n",
    "    esn_.get_exp_weights()\n",
    "\n",
    "\n",
    "for i in experiment_lst:\n",
    "    show_exp_weights(i)  \n",
    "#show_exp_weights(experiment_2)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_exp_weights(i, llambda = 10**-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**-2 \n",
    "np.log(10**-4)/np.log(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment_lst = [experiment_1, experiment_2, experiment_3, experiment_4, \n",
    "                  experiment_5, experiment_6, experiment_7, experiment_8]\n",
    "\n",
    "def quick_dirty_convert(lst):\n",
    "    lst *= 3\n",
    "    print(lst)\n",
    "    pd_ = pd.DataFrame(np.array(lst).reshape(-1,1))\n",
    "    return(pd_)\n",
    "    \n",
    "\n",
    "idx_lst = list(range(len(experiment_lst)))\n",
    "#idx_lst *= 3\n",
    "#idx_lst = pd.DataFrame(np.array(idx_lst).reshape(-1,1))\n",
    "\n",
    "idx_lst = quick_dirty_convert(idx_lst)\n",
    "\n",
    "obs_hz_lst, targ_hz_lst = [], []\n",
    "\n",
    "for i, experiment in enumerate(experiment_lst):\n",
    "    targ_hz = experiment[\"experiment_inputs\"][\"target_hz\"]\n",
    "    obs_hz  = experiment[\"experiment_inputs\"][\"obs_hz\"]\n",
    "    \n",
    "    \n",
    "    if experiment[\"experiment_inputs\"][\"target_hz\"] < 1:\n",
    "        targ_hz *= 1000*1000\n",
    "        obs_hz  *= 1000*1000\n",
    "    obs_hz_lst  += [obs_hz]\n",
    "    targ_hz_lst += [targ_hz]\n",
    "    \n",
    "        \n",
    "    hz_line = {\"target hz\" : targ_hz }\n",
    "    hz_line = Merge(hz_line , {\"obs hz\" : obs_hz })\n",
    "    \n",
    "    #print(hz_line)\n",
    "    df_spec = experiment[\"nrmse\"]\n",
    "    #df_spec = Merge(experiment[\"nrmse\"], {\"target hz\": targ_hz})\n",
    "    df_spec = pd.DataFrame(df_spec, index = [0])\n",
    "    \n",
    "    df_spec_rel = df_spec.copy()\n",
    "    df_spec_rel = df_spec_rel / experiment[\"nrmse\"][\"interpolation\"]\n",
    "    \n",
    "    #print( df_spec_rel)\n",
    "    #print(experiment[\"experiment_inputs\"].keys())\n",
    "    if i == 0:\n",
    "        df = df_spec\n",
    "        df_rel = df_spec_rel\n",
    "        \n",
    "    else:\n",
    "        df = pd.concat([df, df_spec])\n",
    "        df_rel = pd.concat([df_rel, df_spec_rel])\n",
    "\n",
    "#obs_hz_lst  *= 3\n",
    "#targ_hz_lst *= 3\n",
    "\n",
    "obs_hz_lst, targ_hz_lst = quick_dirty_convert(obs_hz_lst), quick_dirty_convert(targ_hz_lst)\n",
    "        \n",
    "df, df_rel = pd.melt(df), pd.melt(df_rel)\n",
    "df  = pd.concat( [idx_lst, df,  obs_hz_lst, targ_hz_lst] ,axis = 1)\n",
    "\n",
    "df_rel = pd.concat( [idx_lst, df_rel,  obs_hz_lst, targ_hz_lst], axis = 1)\n",
    "\n",
    "df.columns     = [\"experiment\", \"model\", \"nrmse\", \"obs hz\", \"target hz\" ]\n",
    "df_rel.columns = [\"experiment\", \"model\", \"nrmse\", \"obs hz\", \"target hz\"] \n",
    "display(df)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df, ax = ax[1])\n",
    "ax[0].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "ax[1].set_title(\"General NRMSE vs MODEL across different RC's\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14,4))\n",
    "sns.violinplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[0])\n",
    "sns.boxplot( y = \"nrmse\" , x = \"model\", data = df_rel, ax = ax[1])\n",
    "ax[0].set_title(\"Relative NRMSE vs Interpolation model across different RC's\")\n",
    "ax[1].set_title(\"Relavite NRMSE vs Interpolation model across different RC's\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"target hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df)\n",
    "plt.title(\"General NRMSE vs MODEL across different RC's\")\n",
    "sns.catplot(y = \"nrmse\" , x = \"model\", hue =\"obs hz\", data = df_rel)\n",
    "plt.title(\"Relavite NRMSE vs Interpolation model across different RC's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def merge_unif_exp(fp_unif, fp_exp):\n",
    "    exp_dat = load_data(fp_exp)\n",
    "    unif_dat = load_data(fp_unif)\n",
    "    assert exp_dat[\"prediction\"][\"interpolation\"] == unif_dat[\"prediction\"][\"interpolation\"], \"something is wrong!\"\n",
    "    joint_dat = unif_dat.copy()\n",
    "    for i in [\"prediction\", \"nrmse\", \"best arguments\"]:\n",
    "        exp_dict = {\"exponential\" : exp_dat[i][\"exponential\"]}\n",
    "        joint_dat[i] = Merge(joint_dat[i], exp_dict)\n",
    "    print(joint_dat[\"best arguments\"])\n",
    "        \n",
    "     \n",
    "    return(joint_dat)\n",
    "#0.5_1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.1.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_data('experiment_results/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.25.txt')\n",
    "hi = fix_interpolation(hi)\n",
    "\n",
    "get_experiment(hi) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_5 = load_data('/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt')\n",
    "                         #bp = '/Users/hayden/Desktop/')\n",
    "experiment_5_obj = get_experiment(experiment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "uniform_ = df_rel[df_rel.model == \"uniform\"]\n",
    "exp_ = df_rel[df_rel.model == \"exponential\"]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,6))\n",
    "sns.kdeplot(uniform_[\"target hz\"], uniform_[\"nrmse\"],\n",
    "                 cmap=\"Reds\", shade=True, shade_lowest=False, ax = ax[0])#, alpha = 0.5)\n",
    "sns.kdeplot(exp_[\"target hz\"], exp_[\"nrmse\"],\n",
    "                 cmap=\"Blues\", shade=True, shade_lowest=False, ax = ax[1])#, alpha = 0.5)\n",
    "ax[0].set_ylim(0,0.3)\n",
    "ax[1].set_ylim(0,0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOOTH\n",
    "#experiment_5 = load_data('/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/2k/medium/split_0.5/targetKhz:_0.02__obskHz:_0.01.txt')\n",
    "#                         #bp = '/Users/hayden/Desktop/')\n",
    "#experiment_5_obj = get_experiment(experiment_5)\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "experiment_ = experiment_5.copy()\n",
    "for key, prediction in experiment_[\"prediction\"].items():\n",
    "    prediction = np.array(prediction)\n",
    "    Train, Test = recover_test_set(experiment_)\n",
    "    \n",
    "    experiment_[\"nrmse\"][key] = nrmse(prediction, Test)\n",
    "get_experiment(experiment_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOOTH\n",
    "experiment_5 = load_data('/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/2k/medium/split_0.5/targetKhz:_0.02__obskHz:_0.01.txt')\n",
    "                         #bp = '/Users/hayden/Desktop/')\n",
    "experiment_ = experiment_5.copy()\n",
    "for key, prediction in experiment_[\"prediction\"].items():\n",
    "    prediction = np.array(prediction)\n",
    "    Train, Test = recover_test_set(experiment_)\n",
    "    \n",
    "    experiment_[\"nrmse\"][key] = nrmse(prediction, Test)\n",
    "experiment_5_obj = get_experiment(experiment_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liang_idx_convert(250, 259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "def liang_idx_convert(lb, ub):\n",
    "    idx_list = list(range(lb, ub + 1))\n",
    "    return idx_list\n",
    "\n",
    "\n",
    "\n",
    "experiment_ = EchoStateExperiment( size = \"medium\", \n",
    "                                   test_time_idx = liang_idx_convert(250, 259), \n",
    "                                   target_frequency = 2000,\n",
    "                                   train_time_idx = liang_idx_convert(220, 249),\n",
    "                                   prediction_type = \"column\")\n",
    "experiment_.get_observers(method = \"exact\", plot_split = True, aspect = 1)\n",
    "\n",
    "bounds = {\n",
    "          #'noise' : (-2, -4),\n",
    "          'llambda' : (-3, -1), \n",
    "          'connectivity': (-3, 0), # 0.5888436553555889, \n",
    "          'n_nodes': 1000,#(100, 1500),\n",
    "          'spectral_radius': (0.05, 0.99),\n",
    "          'regularization': (-10,-2)}\n",
    "\n",
    "\n",
    "default_presets = {\n",
    "                  'scoring_method' : 'tanh',\n",
    "                  \"cv_samples\" : 5,\n",
    "                  \"max_iterations\" : 4000,\n",
    "                  \"eps\" : 1e-5,\n",
    "                  'subsequence_length' : 250,\n",
    "                  \"initial_samples\" : 100}\n",
    "\n",
    "cv_args = {\n",
    "  'bounds' : bounds,\n",
    "  \"n_jobs\" : 4,\n",
    "  \"verbose\" : True,\n",
    "  \"plot\" : False, \n",
    "  **default_presets\n",
    "}\n",
    "\n",
    "experiment_.RC_CV(cv_args = cv_args, model = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(experiment_.dat[\"resp_tr\"].T, aspect = 0.1)\n",
    "plt.show()\n",
    "plt.imshow(experiment_.dat[\"resp_te\"].T, aspect = 0.1)\n",
    "plt.show()\n",
    "plt.imshow(experiment_.dat[\"obs_tr\"].T, aspect = 10)\n",
    "plt.show()\n",
    "plt.imshow(experiment_.dat[\"obs_te\"].T, aspect = 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwave_data = np.load(\"/Users/hayden/Desktop/GW.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwave_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwave_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3,figsize = (16,8))\n",
    "ax = ax.flatten()\n",
    "n = 5\n",
    "for i, j in enumerate(range(n, n+6)): \n",
    "    ax[i].imshow(gwave_data[j,:,:,1], aspect = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3,figsize = (16,8))\n",
    "ax = ax.flatten()\n",
    "for i, j in enumerate([5, 6, 7, 8, 12,13]): #[5, 6, 7, 8, 12,13]\n",
    "    ax[i].imshow(gwave_data[j,:,:,1], aspect = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/msripooja/steps-to-convert-audio-clip-to-spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import pickle\n",
    "import IPython.display as ipd\n",
    "def Audio(transform):\n",
    "    obj = ipd.Audio(data = transform[\"signal\"], rate = transform[\"sr\"])\n",
    "    return(obj)\n",
    "\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "\n",
    "pth_high = \"/Users/hayden/Desktop/18th_century_high.m4a\"\n",
    "\n",
    "#pth = \"/Users/hayden/Desktop/19th_century_high.wav\"#\"/Users/hayden/Desktop/get_free.wav\"\n",
    "\n",
    "def partition_song(array, x_start, x_len,  y_start, y_stop):\n",
    "    print(array.shape)\n",
    "    x_stop = x_start + 3000\n",
    "    partitioned_array = array[y_start:y_stop, x_start:x_stop].copy()\n",
    "    print(partitioned_array.shape)\n",
    "    return(partitioned_array)\n",
    "\n",
    "def get_transform(trans_type,\n",
    "                  hop_length_mult = 1, pth = \"/Users/hayden/Desktop/18th_century_dense.m4a\",\n",
    "                  n_bins_mult = 2, bins_per_octave =12, sr = None, y_axis = \"hz\", filter_scale = 1,\n",
    "                  norm = 1, method = \"db\", label = None, partition = False, save_path = None\n",
    "                  ):\n",
    "    fmin   = librosa.note_to_hz('C0')\n",
    "    if trans_type == 'cqt':\n",
    "        \n",
    "        y_axis = \"cqt_note\"\n",
    "    \n",
    "    assert label, \"enter a label so that you can save a file\"\n",
    "    assert trans_type in [\"stft\", \"cqt\", \"hybrid_cqt\", \"pseudo_cqt\", \"vqt\"]\n",
    "    \n",
    "    x, sr = librosa.load(pth, sr=None)\n",
    "    #C = np.abs(librosa.cqt(x, sr=None))\n",
    "    \n",
    "    #print(\"transform_type: \" + trans_type)\n",
    "    # Librosa transform functions dictionary:\n",
    "    trans_dict = { \"stft\" : librosa.stft,\n",
    "                   \"cqt\" : librosa.cqt,\n",
    "                   \"hybrid_cqt\": librosa.hybrid_cqt,\n",
    "                   \"pseudo_cqt\": librosa.pseudo_cqt,\n",
    "                   \"vqt\" : librosa.vqt\n",
    "    }\n",
    "    #n_bins = int(84*n_bins_mult)\n",
    "    n_bins = n_bins_mult\n",
    "    default_args = {\n",
    "         \"stft\" : {\n",
    "              \"n_fft\" : 512\n",
    "              #\"sr\" : sr\n",
    "         },\n",
    "         \"cqt\" : {\n",
    "                  \"fmin\" : fmin,\n",
    "                  'pad_mode': 'wrap',\n",
    "                  #\"fmin\": FMIN, , \n",
    "                  \"sr\" : sr,\n",
    "                  #\"n_bins\": 84,\n",
    "                  #\"hop_length\" : 2**6 * hop_length_mult\n",
    "                  #\"n_bins\" : n_bins #\"norm\": norm,\n",
    "                  }, #, \"hop_length\" : 64**hop_length_exp \"fmin\"  #\"filter_scale\": filter_scale : 1 #\"res_type\": \"fft\", #\"sparsity\": 0.1\n",
    "         \"hybrid_cqt\": {},\n",
    "         \"pseudo_cqt\": {},\n",
    "         \"vqt\" : {}\n",
    "        \n",
    "    }\n",
    "    \n",
    "    #x = librosa.resample(x, orig_sr = sr, target_sr = sr*5) #NOPE\n",
    "\n",
    "    N_FFT = len(x)\n",
    "    N_FFT_exp = 4\n",
    "    f = trans_dict[trans_type]\n",
    "    X = f(x, **default_args[trans_type]) #stft #, n_fft =int(N_FFT/np.exp(N_FFT_exp)\n",
    "\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    Xpow = np.log10( librosa.db_to_power(Xdb))\n",
    "    matrix2plot = Xpow if method == \"pow\" else Xdb\n",
    "    \n",
    "    if partition:\n",
    "        matrix2plot = partition_song(matrix2plot, partition, 15000, 0, 800)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(7, 5)) \n",
    "    im1 = librosa.display.specshow(matrix2plot, x_axis='time', y_axis=y_axis, ax = ax)\n",
    "    plt.colorbar(im1, format='%+2.0f dB')\n",
    "    \n",
    "  \n",
    "    #Librosa frequency functions \n",
    "    f_dict = { \"stft\": librosa.fft_frequencies,\n",
    "               \"cqt\" : librosa.cqt_frequencies,\n",
    "               \"hybrid_cqt\": None,\n",
    "               \"pseudo_cqt\": None,\n",
    "               \"vqt\" : None}\n",
    "    \n",
    "    \n",
    "    f_default_args_stft = {\"stft\": {\"sr\" : sr, \"n_fft\" : 512}}            \n",
    "    \n",
    "    try:\n",
    "        f_default_args_cqt = {\"cqt\" : {\"fmin\" : fmin, \"n_bins\" : Xpow.shape[0]}}\n",
    "        f_default_args     = Merge(f_default_args_stft, f_default_args_cqt)\n",
    "        \n",
    "    except NameError:   \n",
    "        f_default_args = f_default_args_stft\n",
    "        \n",
    "    freq_fun = f_dict[trans_type]\n",
    "    freqs    = freq_fun(**f_default_args[trans_type])\n",
    "    \n",
    "    #\"type\": \n",
    "    #consider putting all transform types in this position.\n",
    "    transform = {\"signal\": x,\n",
    "                 \"sr\" : sr,\n",
    "                 \"transform\": {\n",
    "                               \"Xdb\"  : Xdb,\n",
    "                               \"Xpow\" : Xpow,\n",
    "                               \"f\"  :  freqs\n",
    "                               }\n",
    "                }\n",
    "    \n",
    "    #assertion to avoid incorrect frequency length.\n",
    "    err_msg = str(len(freqs.tolist())) + \"   \" + str(Xpow.shape[0])\n",
    "    assert len(freqs) == Xpow.shape[0], err_msg\n",
    "    \n",
    "    if save_path:\n",
    "        save_path = save_path + \"_\" + trans_type\n",
    "        print(\"saving at: \" + str(save_path))\n",
    "        save_pickle(save_path, transform)\n",
    "    \n",
    "    audio = Audio(transform)\n",
    "    plt.title(label)\n",
    "    \n",
    "    print_msg =  \"\\x1b[31m\\\"\"+ 'pickle_load(' + save_path + ')' + \"\\\"\\x1b[0m\"\n",
    "    print(\" to load this transform type \" + print_msg)\n",
    "    \n",
    "    \n",
    "    #print(plt.get_xydata() )\n",
    "    #print(labels)\n",
    "    #if trans_type == \"cqt\":\n",
    "    #    print(\"C1 = \" + str(librosa.note_to_hz('C0')))\n",
    "    return(audio) #transform, \n",
    "\n",
    "\n",
    "def save_pickle(path, transform):\n",
    "    save_path = \"./pickle_files/spectrogram_files/\" + path +\".pickle\"\n",
    "    with open(save_path, 'wb') as handle:\n",
    "        pickle.dump(transform, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(path):\n",
    "    path = \"./pickle_files/spectrogram_files/\" + path +\".pickle\"\n",
    "    with open(path, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return(b)\n",
    "#self.librosa_outfile = librosa_outfile\n",
    "#self.spectogram_path = spectogram_path\n",
    "\n",
    "#'./pickle_files/cqt_low_pitch.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"stft\", label = \"19th century male voice\",\n",
    "              pth = \"/Users/hayden/Desktop/computer_male.mp3\",\n",
    "              save_path = \"19th_century_male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"stft\", label = \"19th century female voice\", pth = \"/Users/hayden/Desktop/computer_female.mp3\", save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"19th century male cqt transform\", pth = \"/Users/hayden/Desktop/computer_male.mp3\", save = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"18th_cqt_f\", pth = \"/Users/hayden/Desktop/computer_female.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"18th_cqt_f\", pth = \"/Users/hayden/Desktop/computer_female.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"cqt\", label = \"18th_cqt_high\",  pth = pth_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_free = \"/Users/hayden/Desktop/get_free.wav\"\n",
    "get_transform(\"cqt\", label = \"CQT: Get Free\",  pth = pth_free, partition = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_free = \"/Users/hayden/Desktop/get_free.wav\"\n",
    "#get_transform(\"stft\", label = \"FT: Get Free\",  pth = pth_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cqt = get_transform(\"cqt\", n_bins_mult = 150, y_axis = \"hz\", norm = 1, method = \"pow\" ) #filter_scale = 0.2\n",
    "NBINS = 100\n",
    "\n",
    "#plt.subplot(1,2,1)\n",
    "\n",
    "                              #hop_length_mult = 1)\n",
    "#plt.title(\"dense\")\n",
    "#save_pickle(\"18th_cqt_low\", cqt_low_pitch)\n",
    "\n",
    "#plt.subplot(1,2,2)\n",
    "cqt_high_pitch  = get_transform(\"cqt\", \n",
    "                                pth = pth_high,  \n",
    "                                n_bins_mult = NBINS,\n",
    "                                label = \"18th_cqt_high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the moment log(power) and default db setting appear the same. consider custom functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "fourier = get_transform(\"stft\", n_bins_mult = 150, method = \"pow\") # y_axis = \"log\",)\n",
    "save_pickle(\"fourier_power_low\", fourier_db_low)\n",
    "\n",
    "pth_high = \"/Users/hayden/Desktop/18th_century_high.m4a\"\n",
    "plt.title(\"dense\")\n",
    "fourier = get_transform(\"stft\", n_bins_mult = 150, # y_axis = \"log\",\n",
    "                       pth = pth_high, method = \"pow\")\n",
    "plt.title(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "exper_ = get_experiment(experiment_lst[0], plot_split = False, compare_ = False)\n",
    "print(exper_.A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "librosa.display.specshow(exper_.A.T, y_axis='cqt_note', x_axis='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert dB to Pascals: (Pa)\n",
    "\n",
    "https://www.translatorscafe.com/unit-converter/en-US/sound-pressure-level/2-9/pascal-sound%20pressure%20level%20in%20decibels/#:~:text=Sound%20pressure%20level%20Lp,20%20%CE%BCPa%20or%200.00002%20Pa)\n",
    "\n",
    "\"Sound pressure level (SPL) is a logarithmic (decibel) measure of the sound pressure relative to the reference value of 20 μPa threshold of hearing. The threshold of hearing is the quietest sound that most young healthy people can hear. Sound pressure level Lp is measured in decibels (dB) and is calculated as follows:\"\n",
    "\n",
    "$L_p = 20*log_{10} (p/p_0)$\n",
    "\n",
    "Thus:\n",
    "$(p/p_0) = 10^{L_p/20}$\n",
    "\n",
    "usually $p_0 = 20 \\mu $ Pa or 0.00002 Pa\n",
    "\n",
    "\"The sound pressure level is an absolute value because it is referenced to another absolute value — the threshold of hearing. Therefore, the sound pressure in linear values like pascals can be converted into the sound pressure level in decibels and vice versa if the reference sound pressure is known.\"\n",
    "\n",
    "So p = 0.00002 * 10^{L_p/20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert dB to Intensity of sound: \n",
    "\n",
    "https://www.omnicalculator.com/physics/db#sound-intensity-level-sil\n",
    "\n",
    "Sound intensity is defined as the sound wave power per unit area. It is a special quantity that allows us to measure the energy of sound (or, to be more precise, the energy per second per one squared meter).\n",
    "\n",
    "SIL = $10*log_{10}\\left(\\frac{I}{I_{ref}}\\right)$\n",
    "\n",
    "where:\n",
    "SIL is the sound intensity level in dB;\n",
    "I is the sound intensity in watts per squared meter;\n",
    "Iref is the reference value if sound intensity. Typically, it is assumed to be equal to 1×10⁻¹² W/m².\n",
    "\n",
    "$$I = 10^{\\frac{SIL}{10}-12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The threshold of hearing:\n",
    "The threshold of hearing is generally reported as the RMS sound pressure of 20 micropascals, i.e. 0 dB SPL, corresponding to a sound intensity of 0.98 pW/m2 at 1 atmosphere and 25 °C.[3] It is approximately the quietest sound a young human with undamaged hearing can detect at 1,000 Hz.[4] The threshold of hearing is frequency-dependent and it has been shown that the ear's sensitivity is best at frequencies between 2 kHz and 5 kHz,[5] where the threshold reaches as low as −9 dB SPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = get_experiment(experiment_lst[0])\n",
    "dat = hi.A\n",
    "sns.distplot(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_parameters(array):\n",
    "    log_array    = np.log(array)\n",
    "    sample_mu, sample_sigma   = np.mean(log_array), np.std(log_array)\n",
    "    sample_variance = sample_sigma**2\n",
    "    estimated_mean = np.exp(sample_mu + 0.5*sample_variance)\n",
    "    estimated_variance = estimated_mean**2 * (np.exp(sample_variance) - 1) #np.exp\n",
    "    \n",
    "    est_params = {\"mean\" : estimated_mean,\n",
    "                  \"sd\" :   np.sqrt(estimated_variance)}\n",
    "    return(est_params)\n",
    "\n",
    "def dB2Pa(db_np, normalize = False, drop_silent = False, relative = True):\n",
    "    \"\"\"\n",
    "    converts a decibel level to pascals, for numpy arrays\n",
    "    \n",
    "    if normalize is set to true it normalizes the data assuming a log-normal distribution.\n",
    "    \n",
    "    if drop_silent is true, the function will flatten sounds not hearable by the human ear ( less that 20 * 10-6 pascals)\n",
    "    \"\"\"\n",
    "    p0 = 0.00002\n",
    "    pa_np = 10**(db_np/20)* p0 \n",
    "    \n",
    "    hearing_threshold = 20 * 10 **(-6)\n",
    "    faint_sounds = pa_np < hearing_threshold\n",
    "    \"\"\"\n",
    "    if normalize: #pa_np has a log-normal distribution: https://stats.stackexchange.com/questions/173715/calculate-variance-and-standard-deviation-for-log-normal-distribution\n",
    "        pa_np = np.log(pa_np) # it has a log-normal distribution roughly, so we transform, normalize, then transform back\n",
    "        pa_np = (pa_np - np.mean(pa_np))/np.std(pa_np)\n",
    "        pa_np = np.exp(pa_np)\n",
    "        \n",
    "     \n",
    "    \"\"\"\n",
    "    if normalize: #pa_np has a log-normal distribution: https://stats.stackexchange.com/questions/173715/calculate-variance-and-standard-deviation-for-log-normal-distribution\n",
    "        params = log_normal_parameters(pa_np)\n",
    "        print(params)\n",
    "        \n",
    "        mn, sig = params[\"mean\"], params[\"sd\"]\n",
    "        #hearing_threshold = (hearing_threshold - mn)/sig\n",
    "        pa_np = ((pa_np - mn)/sig)\n",
    "        #pa_np = pa_np  - np.min(pa_np)\n",
    "    \n",
    "    if relative:\n",
    "        pa_np = pa_np/ hearing_threshold\n",
    "    \n",
    "    #Flatten the sounds which the human ear cannot hear. rounding to 5 places preserved the lower bound of human hearing.\n",
    "    if drop_silent:\n",
    "        pa_np[faint_sounds] = np.round(pa_np[faint_sounds], 6)\n",
    "        #new lower bound to avoid 0 values:\n",
    "        pa_np_threshold = 0.000002 #20 * 10 ** (-7)\n",
    "        pa_np[pa_np < pa_np_threshold] = pa_np_threshold\n",
    "    return(pa_np)\n",
    "\n",
    "\n",
    "def dB2Intensity(db_np):\n",
    "    \"\"\"\n",
    "    converts a decibel level to pascals, for numpy arrays\n",
    "    *\n",
    "    \"\"\"\n",
    "    power = db_np/10 -12\n",
    "    Intensity = np.power(10, power)\n",
    "    return(Intensity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xpow =  dB2Pa(dat, normalize =  True)\n",
    "\n",
    "sns.distplot(Xpow)#Log- Normal!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xdb_normal = (Xdb - np.mean(Xdb))/np.std(Xdb)\n",
    "Xpow =  dB2Pa(Xdb, normalize = True, drop_silent = False)\n",
    "sns.distplot(np.log10(Xpow)) \n",
    "min_sound_human_hearing = 20 * 10**(-6)\n",
    "plt.xlabel(\"log(Pascals)\")\n",
    "plt.title(\"ke plot of Pascal values\")\n",
    "plt.ylabel(\"\")\n",
    "plt.axvline(x=np.log10(min_sound_human_hearing) , color = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 micro Pascals\n",
    "The softest sound a normal human ear can detect has a pressure variation of 20 micro Pascals, abbreviated as µPa, which is 20 x 10-6 Pa (\"20 millionth of a Pascal\") and is called the Threshold of Hearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_log_comparison(dataset, \n",
    "                          method = \"propto-dB\", \n",
    "                          propto = True, \n",
    "                          normalized = True,\n",
    "                          log = False,\n",
    "                          drop_silent = True):\n",
    "\n",
    "    # we are only proportional because we have normalized the data.\n",
    "    assert method in [\"propto-dB\", \"propto-SIL\", \"propto-Pa\"], \"choose decibels or sound energy\"\n",
    "    \n",
    "    if method == \"propto-SIL\":\n",
    "        plot_value_label = \"SIL \"\n",
    "        dataset = dB2Intensity(dataset)\n",
    "        if log:\n",
    "            dataset = np.log(dataset)/np.log(10)\n",
    "        ylab = 'SIL (Sound Intensity Level)'  #sound wave power per unit area')\n",
    "            #dataset = (dataset - np.mean(dataset))/np.std(dataset) <-- gets you  what you started with\n",
    "    elif method == \"propto-Pa\":\n",
    "        \n",
    "        plot_value_label = \" (Pa)\" #sound pressure\n",
    "        ylab = \"Pascal\"\n",
    "        \n",
    "        dataset = dB2Pa(dataset, normalize = normalized, drop_silent = False)\n",
    "        #dataset = np.abs(dataset)\n",
    "        #dataset = np.log(dataset) #/np.std(dataset)\n",
    "        #dataset = (dataset - np.mean(dataset))/ np.std(dataset)\n",
    "        \n",
    "    else:\n",
    "        plot_value_label = \" (dB)\" # decibels\n",
    "        ylab = \"Hz\"\n",
    "        #lower_db_limit = -20\n",
    "        \n",
    "        if normalized:\n",
    "            dataset = (dataset - np.mean(dataset))/np.std(dataset)\n",
    "            #lower_db_limit = (lower_db_limit - np.mean(dataset))/np.std(dataset) \n",
    "        #if drop_silent:\n",
    "        #    dataset[dataset < lower_db_limit] = lower_db_limit\n",
    "            \n",
    "    \n",
    "    plot_title = \" spectrogram of '18th century'\" + plot_value_label\n",
    "    \n",
    "    plt.figure(figsize = (12,8))\n",
    "    \n",
    "    # Linear spectogram Plot\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    librosa.display.specshow(dataset, y_axis='linear', x_axis='time')\n",
    "    \n",
    "    #display(quadmesh_.get_axes()) # get the first line, there might be more\n",
    "\n",
    "    #print(ax1.get_axes())#.get_xdata())\n",
    "    #print(plt.get_xdata())\n",
    "    plt.title('Linear ' + plot_title)\n",
    "    add_experiment_regions(ax1)\n",
    "    plt.ylabel(\"Hz\")\n",
    "    \n",
    "    #legend\n",
    "    legend_elements = [Patch(facecolor='pink', edgecolor='red',     label='3500 to 4500 Hz'),\n",
    "                       Patch(facecolor='lightblue', edgecolor='blue',   label='1500 to 2500 Hz'),\n",
    "                       Patch(facecolor='palegreen', edgecolor='green', label='250 to 12250 Hz')]\n",
    "    plt.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "    # Log spectogram Plot\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    librosa.display.specshow(dataset, y_axis='log', x_axis='time')\n",
    "    plt.title('Log ' + plot_title)\n",
    "    add_experiment_regions(ax2)\n",
    "    plt.ylabel(\"Hz\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if method == \"propto-dB\":\n",
    "        propto_str = '%+2.0f' + str(r'$\\propto$') if propto else '%+2.0f'\n",
    "        colorbar_label = propto_str +\" dB\" #+\n",
    "    elif method == \"propto-SIL\":\n",
    "        colorbar_label = \"%+2.0f e-12 SIL\"  # + propto_str + \n",
    "    elif method == \"propto-Pa\":\n",
    "        colorbar_label = \"%.1e Pa\"  # propto_str +\n",
    "\n",
    "    plt.colorbar(format= colorbar_label)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    sns.distplot(dataset)\n",
    "    return(dataset)\n",
    "\n",
    "def add_experiment_regions(ax, plot = True):\n",
    "    def fill_region(lb, ub, color_):\n",
    "        ax.axhline(y=lb, color = color_, linestyle='-')\n",
    "        ax.axhline(y=ub, color = color_, linestyle='-')\n",
    "        x  = np.arange(0.0, 27, 0.1)\n",
    "        y1 = lb + 0 * x\n",
    "        y2 = ub + 0 * x\n",
    "        ax.fill_between(x, y2, y1, alpha = 0.2, color = color_)\n",
    "    trial = 2\n",
    "    if trial == 1:\n",
    "        lb_targ, ub_targ, obs_hz  = 210, 560, 320 / 2\n",
    "\n",
    "    elif trial == 2:\n",
    "        lb_targ, ub_targ, obs_hz  = 340, 640, 280\n",
    "    if plot:\n",
    "        fill_region(lb_targ-obs_hz, lb_targ, \"b\")  #150 hz\n",
    "        fill_region(lb_targ, ub_targ, \"g\") #250 hz\n",
    "        fill_region(ub_targ, ub_targ + obs_hz, \"b\") #150 hz\n",
    "    else:\n",
    "        obs_list = list(range(lb_targ-obs_hz, lb_targ, 10))\n",
    "        obs_list += list(range(ub_targ, ub_targ + obs_hz, 10))\n",
    "        resp_list = list(range(lb_targ, ub_targ, 10))\n",
    "        obs_resp = {\"target\": resp_list, \"obs\": obs_list}\n",
    "        return obs_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decibel spectogram (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exper_ = experiment_lst[0]\n",
    "exper_obj = get_experiment(exper_)\n",
    "dat = exper_obj.A.T\n",
    "#hi = (hi - np.mean(hi))/np.std(hi)\n",
    "dataset_Pow = linear_log_comparison(dat, \n",
    "                      propto = False, \n",
    "                      normalized = False,\n",
    "                      drop_silent = False,\n",
    "                      method = \"propto-Pa\") \n",
    "\n",
    "dataset_db = linear_log_comparison(dat, \n",
    "                                    propto = False, \n",
    "                                    normalized = False,\n",
    "                                    drop_silent = True,\n",
    "                                    method = \"propto-dB\") \n",
    "\n",
    "custom_transform = {\"transform\": {\n",
    "                        \"Xdb\"  : dataset_db,\n",
    "                        \"Xpow\" : dataset_Pow,\n",
    "                        \"f\"  :   exper_obj.f}\n",
    "                   }\n",
    "save_pickle(\"custom\",custom_transform)\n",
    "#load_pickle(\"custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE THIS NORMALIZED Pa DATASET!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decibel spectogram, unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(exper_, \n",
    "                      propto = False, \n",
    "                      normalized = False,\n",
    "                      drop_silent = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pascal spectogram, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(exper_, method = \"propto-Pa\", normalized = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pascal spectogram, unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(exper_, method = \"propto-Pa\", normalized = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at the average sound pressure per log frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dB2Pa(exper_.A_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_comparison(inputt = Xdb, method = \"propto-Pa\", drop_silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(exper_.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_spectogram(dataset = exper_.A, f_arr = exper_.f):\n",
    "    \n",
    "    T = exper_.T\n",
    "    plt.imshow(dataset)\n",
    "    \n",
    "    f = np.array(f_arr)[1:] # humans hearing ranges from 20 db to 20k db so lets drop 0 to avoid -infty.\n",
    "    dataset = dataset[:,1:]\n",
    "    \n",
    "    f = np.log(f)/np.log(2) # humans experience sound logarithmically\n",
    "    \n",
    "    sns.distplot(dataset)\n",
    "    plt.show()\n",
    "    n_timesteps, n_frequencies  = dataset.shape\n",
    "\n",
    "    for i, time_step in enumerate(range(n_timesteps)):\n",
    "        if not i:\n",
    "            dictt_lst = []\n",
    "        this_timestep = T[time_step][0]\n",
    "        \n",
    "        #assert len(f) == len(this_timestep), \"error: \" + str(len(f)) + \" != \" + str(len(this_timestep))\n",
    "        for i, frequency_spec in enumerate(f):              \n",
    "            dictt_lst += [{\"frequency\" : frequency_spec , \n",
    "                           \"time\"      : this_timestep,\n",
    "                           \"amplitude\" : dataset[time_step, i]\n",
    "                            }]\n",
    "        #display(pd.DataFrame(dictt_lst))\n",
    "    \n",
    "    log_frequency_df = pd.DataFrame(dictt_lst)\n",
    "    log_frequency_df = log_frequency_df.pivot(\"frequency\", \"time\", \"amplitude\")\n",
    "    \n",
    "    sns.heatmap(log_frequency_df)\n",
    "    \n",
    "create_log_spectogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sns.load_dataset(\"flights\")\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(Xdb, y_axis='log', x_axis='time')\n",
    "plt.title('log Power spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Xdb, aspect = 10)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_ = f_[1], f_[15] #_ denotes temporary variable, for testing or within a function.\n",
    "\n",
    "lb_, ub_ = bounds_\n",
    "\n",
    "def retrieve_freqs_btwn(bounds, f_):\n",
    "    f = np.array(f_)\n",
    "    lb, ub = bounds\n",
    "    display(bounds_)\n",
    "    lb_bool_vec, ub_bool_vec = (f > lb_), (f < ub_)\n",
    "    and_vector = ub_bool_vec* lb_bool_vec\n",
    "\n",
    "    freqs = f[and_vector]            #frequencies between bounds\n",
    "    freq_idxs = np.where(and_vector)[0] #indices between bounds\n",
    "\n",
    "    return(freq_idxs.tolist())\n",
    "    \n",
    "retrieve_freqs_btwn(bounds_, f_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def log_amplitude(exper_):\n",
    "    A = np.array(exper_.A)\n",
    "    print(np.min(A))\n",
    "    print(np.max(A))\n",
    "    orig_shape = A.shape\n",
    "    \n",
    "    signs = A.copy().reshape(-1,) < 0\n",
    "    signs = signs * 2 - 1\n",
    "    print(np.unique(signs))\n",
    "    signs = signs.reshape(orig_shape)\n",
    "    #plt.imshow(signs)\n",
    "    A_new = np.log(np.abs(A)) * signs\n",
    "    \n",
    "    #A_new = (A_new - np.mean(A_new))/ np.std(A_new)\n",
    "    #print(np.min(A_new))\n",
    "    #print(np.max(A_new))\n",
    "    #plt.imshow(A_new)\n",
    "    return(A_new)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_pickle('./pickle_files/results/18th_cqt_high/db/untouched/split_0.5/tf_250__obsHz_0.1__targHz_0.02.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_pickle(\"custom\")\n",
    "\n",
    "sns.heatmap(hi[\"Xpow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = load_pickle(\"18th_cqt_high\")\n",
    "print(hi[\"transform\"][\"Xdb\"])\n",
    "sns.heatmap(hi[\"transform\"][\"Xdb\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "def get_frequencies(trial = 1):\n",
    "    \"\"\"\n",
    "    get frequency lists\n",
    "    \"\"\"\n",
    "    if trial == 1:\n",
    "        lb_targ, ub_targ, obs_hz  = 210, 560, int(320 / 2)\n",
    "\n",
    "    elif trial == 2:\n",
    "        lb_targ, ub_targ, obs_hz  = 340, 640, 280\n",
    "    elif trial == 3:\n",
    "        lb_targ, ub_targ, obs_hz  = 340, 350, 20\n",
    "\n",
    "\n",
    "    obs_list = list(range(lb_targ-obs_hz, lb_targ, 10))\n",
    "    obs_list += list(range(ub_targ, ub_targ + obs_hz, 10))\n",
    "    resp_list = list(range(lb_targ, ub_targ, 10))\n",
    "    return obs_list, resp_list\n",
    "\n",
    "obs_freqs, resp_freqs = get_frequencies(1)\n",
    "librosa_args = { \"spectrogram_path\": \"custom\",#\"cqt_high_pitch\",\n",
    "                         \"librosa\": True}\n",
    "#inputs = {'obs_freq_lst' :, \"targ_freq_lst\": , \"split\": 0.5}\n",
    "                       \n",
    "additional_Echo_inputs = {\n",
    "            \"obs_freq_lst\":  obs_freqs,\n",
    "            \"targ_freq_lst\" : resp_freqs\n",
    "            }\n",
    "Echo_inputs = {\n",
    "        \"size\" : \"medium\",\n",
    "        \"verbose\" : False,\n",
    "        \"prediction_type\" : \"block\"}\n",
    "Echo_inputs = Merge(Echo_inputs, additional_Echo_inputs)\n",
    "experiment = EchoStateExperiment( **Echo_inputs, **librosa_args)\n",
    "experiment.get_observers(method = \"exact\", split = 0.5, aspect = 0.9, plot_split = False)\n",
    "experiment.obs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree pickle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = load_pickle(\"./pickle_files/spectrogram_files/18th_cqt_high.pickle\")\n",
    "g_truth = dat[\"transform\"][\"Xdb\"]\n",
    "g_truth = (g_truth - np.mean(g_truth))/np.std(g_truth)\n",
    "line = g_truth[35][513:]\n",
    "fig, ax = plt.subplots(1,1,figsize = (10,4))\n",
    "sns.lineplot(x = range(len(bye)), y = bye, label = \"unif\")\n",
    "sns.lineplot(x = range(len(ip)), y = ip, label = \"ip\")\n",
    "sns.lineplot(x = range(len(line)), y =line, label = \"gtruth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_shape_0 = 1000\n",
    "def get_obs_eq(k):\n",
    "    hi = A_shape_0//k\n",
    "    viable_start = np.random.randint(hi)\n",
    "    observers = [k*i + viable_start for i, idx in  enumerate(range(viable_start, A_shape_0, k))]\n",
    "    print(observers)\n",
    "        \n",
    "get_obs_eq(25)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls pickle_files/results/custom/db/untouched/split_0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Zhizhuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = 40\n",
    "end_idx = 942\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(test1.ip_res)\n",
    "#https://stackoverflow.com/questions/35215161/most-efficient-way-to-map-function-over-numpy-array\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "\n",
    "def pure_prediction_ip_generator(missing_data, end_idx):\n",
    "    test_idx = list(range(end_idx))[-missing_data:] #553, 712, 942\n",
    "    print(test_idx)\n",
    "    train_range_input = end_idx - missing_data\n",
    "    train_idx = list(range(train_range_input))\n",
    "    print(train_range_input)\n",
    "\n",
    "    experiment_inputs1 =  {'size': 'medium', \n",
    "                           'target_frequency': None, \n",
    "                           'verbose': False, \n",
    "                           'prediction_type': 'column', \n",
    "                           \"interpolation_method\" : \"griddata-cubic\",\n",
    "                           'train_time_idx': train_idx,\n",
    "                           'test_time_idx' : test_idx}#[514, 515, 516, 517, 518, 519, 520, 521, 522, 523]}#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249], 'test_time_idx': [250, 251, 252, 253, 254, 255, 256, 257, 258, 259]}\n",
    "\n",
    "    test1 = EchoStateExperiment(**experiment_inputs1)\n",
    "    obs_inputs1 =  {'split': 0.5, 'aspect': 0.9, 'plot_split': True, 'method': 'exact'}\n",
    "    test1.get_observers(**obs_inputs1)\n",
    "\n",
    "\n",
    "    import math\n",
    "    def f(x):\n",
    "        \"\"\"\n",
    "        check if x is nan\n",
    "        x = float('nan')\n",
    "        math.isnan(x)\n",
    "        \"\"\"\n",
    "        return math.isnan(x)\n",
    "    def array_map(x, f):\n",
    "        x_shape= x.shape\n",
    "        print(\"x_shape\" + str(x.shape))\n",
    "        x  = x.flatten().tolist()\n",
    "        hi = np.array(list(map(f,x)))\n",
    "        print(hi)\n",
    "\n",
    "        return np.array(hi).reshape(x_shape)\n",
    "\n",
    "    test1_ip_pred = test1.ip_res[\"prediction\"]\n",
    "\n",
    "    plt.imshow(array_map(test1_ip_pred, f))\n",
    "    test1_ip_pred\n",
    "\n",
    "    my_dict = {\n",
    "        \"interpolation_prediction\": test1.ip_res[\"prediction\"],\n",
    "        \"ground_truth_test\"  : test1.xTe,\n",
    "        \"ground_truth_train\" : test1.xTr,\n",
    "        \"interpolation_MSE\"  : test1.ip_res[\"nrmse\"]\n",
    "    }\n",
    "\n",
    "    from scipy.io import savemat\n",
    "\n",
    "    save_path = \"zhizhuo/testindex_\" + str(test_idx[0]) + \"_\" + str(test_idx[-1]) +\".mat\"\n",
    "\n",
    "    print(save_path)\n",
    "    savemat(save_path, my_dict) #\"zhizhuo/testindex_514_523.mat\"\n",
    "    plt.imshow(test1.ip_res[\"prediction\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = [{\"missing_data\" : 40, \"end_idx\" : 289},\n",
    "            {\"missing_data\" : 40, \"end_idx\" : 553},\n",
    "            {\"missing_data\" : 40, \"end_idx\" : 712},\n",
    "            {\"missing_data\" : 40, \"end_idx\" : 942}]\n",
    "for prediction in test_lst:\n",
    "    pure_prediction_ip_generator(**prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test1.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_pickle('19th_century_male_stft')\n",
    "plt.imshow(X['transform']['Xdb'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_transform(\"stft\", label = \"19th century male voice\", pth = \"/Users/hayden/Desktop/computer_male.mp3\", save_path = \"19th_century_male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = \"\"\"                  {'target_freq': 2000, 'split': 0.5, 'obs_hz': 250, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 250, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 250, 'target_hz': 1000},\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 500, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 500, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 500, 'target_hz': 1000},\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 750, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 750, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.5, 'obs_hz': 750, 'target_hz': 1000},\n",
    "\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 250, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 250, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 250, 'target_hz': 1000},\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 500, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 500, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 500, 'target_hz': 1000},\n",
    "\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 750, 'target_hz':  500},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 750, 'target_hz':  750},\n",
    "                          {'target_freq': 2000, 'split': 0.9, 'obs_hz': 750, 'target_hz': 1000},\"\"\"\n",
    "path_lst = [\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.25.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.25.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.25.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.5.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.75.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.75.txt\",\n",
    "          \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.75.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.25.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.25.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.25.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.5.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.75.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.75.txt\",\n",
    "          \"2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.75.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.25.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.25.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.25.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.5.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.75.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.75.txt\",\n",
    "          \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.75.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.25.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.25.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.25.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.5.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "    \n",
    "          \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.75.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.75.txt\",\n",
    "          \"4k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.75.txt\",\n",
    "            ]\n",
    "\n",
    "path_lst = ['/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/experiment_results/medium/split_0.5/targetKhz:_0.01__obskHz:_0.02.txt' ]\n",
    "\n",
    "path_lst = ['/Users/hayden/Desktop/DL_LAB/Reservoir/backwards/experiment_results1/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.1.txt',\n",
    "            \"/Users/hayden/Desktop/DL_LAB/Reservoir/backwards/experiment_results1/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.25.txt\",\n",
    "            \"/Users/hayden/Desktop/DL_LAB/Reservoir/backwards/experiment_results1/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "            \"/Users/hayden/Desktop/DL_LAB/Reservoir/backwards/experiment_results1/1k/publish/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt\"\n",
    "           ]\n",
    "complete_experiment_path_lst = [ \n",
    "    #targ 500  kHz COMPLETE\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt',\n",
    "            #targ 1000 kHz COMPLETE\n",
    "            '2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_1.0__obskHz:_1.0.txt',\n",
    "            \n",
    "            #targ 500  Hz COMPLETE\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt', #no exp\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.0.txt', #no exp\n",
    "            #targ 1000 Hz COMPLETE\n",
    "            '2k/medium/split_0.5/targetKhz:_1.0__obskHz:_1.0.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt', #no exp\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "            finished but publish size:\n",
    "            '/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.1.txt',\n",
    "            \"/1k/publish/split_0.5/targetKhz:_0.1__obskHz:_0.25.txt\",\n",
    "            \n",
    "            ########################################################################### 1k\n",
    "            completed 1k tests\n",
    "            \"/1k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.0.txt\",\n",
    "            \"/1k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt\",\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def check_for_duplicates(lst, UnqLst = True, verbose = True):\n",
    "    for i, item in enumerate(lst):\n",
    "        if not i:\n",
    "            unique_lst = []\n",
    "            duplicates = []\n",
    "        if item in unique_lst:\n",
    "            duplicates.append(item) \n",
    "        else:\n",
    "            unique_lst.append(item)\n",
    "    if verbose:\n",
    "        print(duplicates)\n",
    "    if UnqLst:\n",
    "        return(unique_lst) \n",
    "\"\"\"     \n",
    "\n",
    "    \n",
    "experiments1k = [\n",
    "            \"/1k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt\",\n",
    "            \"/1k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt\"]\n",
    "\n",
    "hi = \"\"\"\n",
    "for i in experiments1k:\n",
    "    experiment_ = load_data(i,\n",
    "                             bp = './experiment_results')\n",
    "    hi = pd.DataFrame(experiment_['nrmse'], index = [0])\n",
    "    hi = pd.melt(hi)\n",
    "    hi.columns = [\"model\", \"nrmse\"]\n",
    "    print(hi)\n",
    "    sns.barplot(x = \"model\", y = \"nrmse\", data = hi)\n",
    "    #experiment_obj = get_experiment(experiment_5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# when it comes time to run a lot of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run -i '../MARIOS/PyFiles/imports.py'\n",
    "%run -i '../MARIOS/PyFiles/helpers.py'\n",
    "%run -i \"../MARIOS/PyFiles/experiment.py\"\n",
    "def quick_write_path(freq, split, targHz, obsHz, size = \"/medium\"):\n",
    "    if freq == 2000:\n",
    "        freqStr = \"2k\"\n",
    "    elif freq == 4000:\n",
    "        freqStr = \"4k\"\n",
    "    splitStr = \"/split_\" + str(split)\n",
    "    targHz, obsHz = str(targHz/1000) , str(obsHz/1000)\n",
    "    HzStr = \"/targetKhz:_\" + targHz + \"__obskHz:_\" +  obsHz \n",
    "    newPath = freqStr + size + splitStr + HzStr +\".txt\"\n",
    "    return([newPath])\n",
    "\n",
    "def quick_write_dict(freq, split, targHz, obsHz):\n",
    "    dict_tmp = {'target_freq': freq, 'split': split, 'obs_hz': obsHz, 'target_hz': targHz}\n",
    "    return([dict_tmp])\n",
    "\n",
    "\n",
    "path_lst = []\n",
    "dict_lst = []\n",
    "for targ_freq in [2000, 4000]:\n",
    "    for split in [0.5, 0.9]:\n",
    "        for targ in list(range(500, 2001, 250)):\n",
    "            for obs in list(range(500, 2001, 250)):\n",
    "                path_lst += quick_write_path(freq = targ_freq, split = split, targHz = targ, obsHz = obs)\n",
    "                dict_lst += quick_write_dict(freq = targ_freq, split = split, targHz = targ, obsHz = obs)\n",
    "\n",
    "\n",
    "path_lst += [ \n",
    "            # the plan is to run all those tests which will give detail from the LHS. ie increasin\n",
    "            # target Hz.\n",
    "            ########################################################################### 2k\n",
    "            #######################2k, 0.9 \n",
    "            #targ 500  kHz COMPLETE\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.75.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.25.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.75.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.5__obskHz:_2.0.txt',\n",
    "            \n",
    "    \n",
    "            \n",
    "\n",
    "            #targ 750  H z\n",
    "            '2k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_0.75__obskHz:_1.0.txt',\n",
    "    \n",
    "            #targ 1000 kHz COMPLETE\n",
    "            '2k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_1.0__obskHz:_1.0.txt',\n",
    "            \n",
    "            #targ 1250  Hz\n",
    "            '2k/medium/split_0.9/targetKhz:_1.25__obskHz:_0.5.txt', \n",
    "            '2k/medium/split_0.9/targetKhz:_1.25__obskHz:_1.0.txt', \n",
    "    \n",
    "            #targ 1500  Hz\n",
    "            '2k/medium/split_0.9/targetKhz:_1.5__obskHz:_0.5.txt', \n",
    "            '2k/medium/split_0.9/targetKhz:_1.5__obskHz:_1.0.txt', \n",
    "    \n",
    "            #targ 1750  Hz\n",
    "            '2k/medium/split_0.9/targetKhz:_1.75__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_1.75__obskHz:_1.0.txt',\n",
    "    \n",
    "            #targ 2000  Hz\n",
    "            '2k/medium/split_0.9/targetKhz:_2.0__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.9/targetKhz:_2.0__obskHz:_1.0.txt',\n",
    "    \n",
    "            #######################2k, 0.5\n",
    "            #targ 500  Hz COMPLETE\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.75.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.0.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.25.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.5.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.75.txt',\n",
    "            '2k/medium/split_0.5/targetKhz:_0.5__obskHz:_2.0.txt',\n",
    "    \n",
    "             #targ 750 Hz\n",
    "             \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_1.0.txt\",\n",
    "             \"2k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.5.txt\", #CHECK LATER\n",
    "    \n",
    "            #targ 1000 Hz\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_1.0.txt\", #\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\", #\n",
    "    \n",
    "            #targ 1250 Hz\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.25__obskHz:_1.0.txt\", # ABOUT TO RUN 600\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.25__obskHz:_0.5.txt\", # ABOUT TO RUN 600\n",
    "    \n",
    "            #targ 1500 Hz\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.5__obskHz:_1.0.txt\", # ABOUT TO RUN 1000\n",
    "             \"2k/medium/split_0.5/targetKhz:_1.5__obskHz:_0.5.txt\", # ABOUT TO RUN 1000\n",
    "    \n",
    "     \n",
    "             #targ 2000 Hz\n",
    "             '2k/medium/split_0.5/targetKhz:_2.0__obskHz:_1.0.txt', #no exp\n",
    "             \n",
    "    \n",
    "           ########################################################################### 4k\n",
    "           #######################4k, 0.9 \n",
    "           #4k, 0.9 500 target Hz COMPLETE\n",
    "           \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_0.5.txt\",\n",
    "           \"4k/medium/split_0.9/targetKhz:_0.5__obskHz:_1.0.txt\",\n",
    "            \n",
    "           #4k, 0.9 1000 target Hz RUNNING\n",
    "           '4k/medium/split_0.9/targetKhz:_0.75__obskHz:_0.5.txt', #RUNNING 200\n",
    "           '4k/medium/split_0.9/targetKhz:_0.75__obskHz:_1.0.txt', #RUNNING 200\n",
    "    \n",
    "           \n",
    "           #4k, 0.9 1000 target Hz COMPLETE\n",
    "           '4k/medium/split_0.9/targetKhz:_1.0__obskHz:_0.5.txt',\n",
    "           '4k/medium/split_0.9/targetKhz:_1.0__obskHz:_1.0.txt',\n",
    "    \n",
    "           #4k, 0.9 1250 target Hz COMPLETE\n",
    "           '4k/medium/split_0.9/targetKhz:_1.25__obskHz:_0.5.txt', #RUNNING 400\n",
    "           '4k/medium/split_0.9/targetKhz:_1.25__obskHz:_1.0.txt', #RUNNING 400\n",
    "           \n",
    "           #4k, 0.9 1500 target Hz COMPLETE\n",
    "           '4k/medium/split_0.9/targetKhz:_1.5__obskHz:_0.5.txt', #RUNNING 700\n",
    "           '4k/medium/split_0.9/targetKhz:_1.5__obskHz:_1.0.txt', #RUNNING 700\n",
    "            \n",
    "\n",
    "           #######################4k, 0.5\n",
    "           #4k 0.5 target kHz COMPLETE\n",
    "           '4k/medium/split_0.5/targetKhz:_0.5__obskHz:_0.5.txt', #???\n",
    "           \"4k/medium/split_0.5/targetKhz:_0.5__obskHz:_1.0.txt\",  #???\n",
    "    \n",
    "           #4k 0.75 target kHz COMPLETE\n",
    "           \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_0.5.txt\", #NO EXP\n",
    "           \"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_1.0.txt\",  \n",
    "           \n",
    "           #4k 1.0 target kHz \n",
    "           \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_0.5.txt\",# ????\n",
    "           \"4k/medium/split_0.5/targetKhz:_1.0__obskHz:_1.0.txt\",   # ALREADY HAVE IT\n",
    "    \n",
    "           #4k, 0.5 1250 target Hz NEED TO RUN\n",
    "           '4k/medium/split_0.5/targetKhz:_1.25__obskHz:_0.5.txt', #ABOUT TO RUN 500\n",
    "           '4k/medium/split_0.5/targetKhz:_1.25__obskHz:_1.0.txt', #ABOUT TO RUN 500\n",
    "    \n",
    "            #4k, 0.5 1500 target Hz NEED TO RUN\n",
    "           '4k/medium/split_0.5/targetKhz:_1.5__obskHz:_0.5.txt', #ABOUT TO RUN 900\n",
    "           '4k/medium/split_0.5/targetKhz:_1.5__obskHz:_1.0.txt', #NO EXP\n",
    "\n",
    "           #4k 2.0 target kHz \n",
    "           \"4k/medium/split_0.5/targetKhz:_2.0__obskHz:_0.5.txt\", \n",
    "           \"4k/medium/split_0.5/targetKhz:_2.0__obskHz:_1.0.txt\", \n",
    "           \"4k/medium/split_0.5/targetKhz:_2.0__obskHz:_1.5.txt\", #For now this is deemed unessential.\n",
    "           \"4k/medium/split_0.5/targetKhz:_2.0__obskHz:_2.0.txt\", \n",
    "\n",
    "           #4k 0.5, bigger and better! \n",
    "\n",
    "           #\"2k/medium/split_0.5/targetKhz:_1.5__obskHz:_1.0.txt\", \n",
    "\n",
    "\n",
    "           \"4k/medium/split_0.5/targetKhz:_3.0__obskHz:_0.5.txt\",\n",
    "           \"4k/medium/split_0.5/targetKhz:_3.0__obskHz:_1.0.txt\",\n",
    "           \"4k/medium/split_0.5/targetKhz:_3.0__obskHz:_2.0.txt\",\n",
    "\n",
    "           #\"4k/medium/split_0.5/targetKhz:_4.0__obskHz:_0.5.txt\", #??? broken\n",
    "           #\"4k/medium/split_0.5/targetKhz:_4.0__obskHz:_1.0.txt\", #??? broken\n",
    "           #\"4k/medium/split_0.5/targetKhz:_4.0__obskHz:_2.0.txt\", #??? broken\n",
    "\n",
    "           #\"4k/medium/split_0.5/targetKhz:_0.75__obskHz:_1.0.txt\",\n",
    "\n",
    "           # MORE DETAIL:, given that the others aren't converging. \n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out low frequency results\n",
    "exper_lst = []\n",
    "bp_ = \"/Users/hayden/Desktop/DL_LAB/Reservoir/MARIOS/pickle_files/results/custom/power/untouched/\"\n",
    "\n",
    "new_exper_path_lsts = [\n",
    "    \"split_0.5/tf_485.0__obsNIdx_56__targNIdx_30.pickle\",\n",
    "    \"split_0.9/tf_380.0__obsNIdx_32__targNIdx_35.pickle\"\n",
    "    #tf_485.0__obsNIdx_56__targNIdx_30.pickle\n",
    "]\n",
    "\n",
    "\n",
    "for i in new_exper_path_lsts:\n",
    "    exper_ = load_p_result(i, bp = bp_)\n",
    "    exper_lst += [exper_]\n",
    "    \n",
    "xpow = load_pickle(\"custom\")[\"transform\"][\"Xpow\"]\n",
    "\n",
    "this_experiment = exper_lst[0]\n",
    "resp_idx_ = this_experiment[\"resp_idx\"]\n",
    "print(resp_idx_)\n",
    "resp_ = xpow[resp_idx_]\n",
    "sns.heatmap(resp_)\n",
    "plt.show()\n",
    "sns.heatmap(resp_[:,512:])\n",
    "plt.show()\n",
    "sns.heatmap(np.array(exper_lst[0][\"prediction\"][\"exponential\"]).T)\n",
    "plt.show()\n",
    "sns.heatmap(np.array(exper_lst[0][\"prediction\"][\"interpolation\"]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
